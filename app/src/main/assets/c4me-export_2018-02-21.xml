<c4me>
 <main>
  <conferenceShortName>DHd-Tagung 2018</conferenceShortName>
  <conferenceName>DHd 2018</conferenceName>
  <conferenceSubTitle>Kritik der digitalen Vernunft | 26. Februar -2. März 2018 in Köln</conferenceSubTitle>
  <conferenceSubTitle2>Kritik der digitalen Vernunft | 26 February - 2 March 2018 in Cologne</conferenceSubTitle2>
  <conferenceURL>http://dhd2018.uni-koeln.de/</conferenceURL>
  <conftoolURL>https://www.conftool.com/dhd2018/</conftoolURL>
  <conftoolAgendaURL>https://www.conftool.com/dhd2018/sessions.php</conftoolAgendaURL>
 </main>
<sessions>
 <session>
  <session_ID>186</session_ID>
  <session_short>Treffen</session_short>
  <session_title>Meet the TEI Council</session_title>
  <session_start>2018-02-26 10:00</session_start>
  <session_end>2018-02-26 12:00</session_end>
  <session_room_ID>16</session_room_ID>
  <session_room>S 23, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>94</session_ID>
  <session_title>Öffnungszeiten Konferenzsekretariat</session_title>
  <session_start>2018-02-26 13:00</session_start>
  <session_end>2018-02-26 17:30</session_end>
  <session_room_ID>5</session_room_ID>
  <session_room>Hörsaal F, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>6</session_ID>
  <session_short>Workshop_1a</session_short>
  <session_title>Workshop_1a</session_title>
  <session_start>2018-02-26 14:00</session_start>
  <session_end>2018-02-26 15:30</session_end>
  <session_room_ID>8</session_room_ID>
  <session_room>S 11, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Dank dem Einsatz neuer Ansätze aus der Computer Vision sowie neuronaler Netze ist eine ungemein verbessert Layout- und Texterkennung von alten Drucken und gar Handschriften möglich.
Der Workshop führt in die Tools und Anwendungen (u.a. Transkribus) ein, die im Rahmen von READ (Recognition and Enrichment of Archival Documents) entwickelt werden.
Die Veranstaltung bietet sowohl hands-on Einführungen als auch theoretische Vertiefungen durch Spezialisten. Vorkenntnisse sind keine notwendig.
</session_abstract>
  <attendee_count>3</attendee_count>
  <sessionID>6</sessionID>
  <presentations>1</presentations>
  <p1_paperID>103</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Hodel, Tobias
Strauß, Tobias
Diem, Markus</p1_authors>
  <p1_organisations>Staatsarchiv des Kantons Zürich, Schweiz
Universität Rostock CITLab
Vienna University of Technology, Computer Vision Lab</p1_organisations>
  <p1_emails>tobias.hodel@hist.uzh.ch
tobias.strauss@uni-rostock.de
diem@caa.tuwien.ac.at</p1_emails>
  <p1_presenting_author>Hodel, Tobias
Strauß, Tobias
Diem, Markus</p1_presenting_author>
  <p1_title>Automatic Text Recognition: Mit Transkribus Texterkennung trainieren und anwenden</p1_title>
  <p1_abstract>&lt;p&gt;Dank dem Einsatz neuer Ansätze aus der Computer Vision sowie neuronaler Netze ist eine ungemein verbessert Layout- und Texterkennung von alten Drucken und gar Handschriften möglich.&lt;/p&gt;
&lt;p&gt;Der Workshop führt in die Tools und Anwendungen (u.a. Transkribus) ein, die im Rahmen von READ (Recognition and Enrichment of Archival Documents) entwickelt werden.&lt;/p&gt;
&lt;p&gt;Die Veranstaltung bietet sowohl &lt;em&gt;hands-on&lt;/em&gt; Einführungen als auch theoretische Vertiefungen durch Spezialisten. Vorkenntnisse sind keine notwendig.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>7</session_ID>
  <session_short>Workshop_2a</session_short>
  <session_title>Workshop_2a</session_title>
  <session_start>2018-02-26 14:00</session_start>
  <session_end>2018-02-26 15:30</session_end>
  <session_room_ID>9</session_room_ID>
  <session_room>S 12, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
  <sessionID>7</sessionID>
  <presentations>1</presentations>
  <p1_paperID>126</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Rüdiger, Jan Oliver</p1_authors>
  <p1_organisations>Universität Kassel, Deutschland</p1_organisations>
  <p1_emails>jan.ruediger@uni-kassel.de</p1_emails>
  <p1_presenting_author>Rüdiger, Jan Oliver</p1_presenting_author>
  <p1_title>CorpusExplorer v2.0 - Seminartauglich in einem halben Tag</p1_title>
  <p1_abstract>&lt;p&gt;-&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>19</session_ID>
  <session_short>Workshop_3a</session_short>
  <session_title>Workshop_3a</session_title>
  <session_start>2018-02-26 14:00</session_start>
  <session_end>2018-02-26 15:30</session_end>
  <session_room_ID>10</session_room_ID>
  <session_room>S 13, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten.</session_abstract>
  <attendee_count>4</attendee_count>
  <sessionID>19</sessionID>
  <presentations>1</presentations>
  <p1_paperID>121</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Reiter, Nils
Ketschik, Nora
Kremer, Gerhard
Schulz, Sarah</p1_authors>
  <p1_organisations>Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland
Institut für Literaturwissenschaft, Universtität Stuttgart, Deutschland
Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland
Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland</p1_organisations>
  <p1_emails>nils.reiter@ims.uni-stuttgart.de
nora.ketschik@ilw.uni-stuttgart.de
gerhard.kremer@ims.uni-stuttgart.de
sarah.schulz@ims.uni-stuttgart.de</p1_emails>
  <p1_presenting_author>Reiter, Nils
Ketschik, Nora
Kremer, Gerhard
Schulz, Sarah</p1_presenting_author>
  <p1_title>Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse</p1_title>
  <p1_abstract>&lt;p&gt;Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>21</session_ID>
  <session_short>Workshop_4a</session_short>
  <session_title>Workshop_4a</session_title>
  <session_start>2018-02-26 14:00</session_start>
  <session_end>2018-02-26 15:30</session_end>
  <session_room_ID>11</session_room_ID>
  <session_room>S 14, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Im Rahmen des an das Format des Hackathons angelehnten "Modellathons" soll praktisches Handwerkszeug digitaler 3D-Rekonstruktion in wissenschaftlichen Kontexten vermittelt und erprobt werden. Der Modellathon beinhaltet einen ½ tägigen Workshop zur Vermittlung von Grundlagen der 3D-Rekonstruktionen und deren filmischer oder bildlicher Inszenierung. Daran anschliessend sollen im Rahmen eines die DHd-Jahrestagung begleitenden Wettbewerbs durch studentische Arbeitsteams 3D-Rekonstruktionen eines noch zu benennenden Objekts erstellt werden. Die Ergebnisse werden im abschliessenden DHd-Plenum vorgestellt und durch das Publikum sowie eine Expertenjury bewertet sowie prämiert.</session_abstract>
  <attendee_count>3</attendee_count>
  <sessionID>21</sessionID>
  <presentations>1</presentations>
  <p1_paperID>135</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Münster, Sander
Christen, Jonas
Pfarr-Harfst, Mieke</p1_authors>
  <p1_organisations>Medienzentrum/TU Dresden, Deutschland
Zürcher Hochschule der Künste, Schweiz
Technische Universität Darmstadt, Deutschland</p1_organisations>
  <p1_emails>sander.muenster@tu-dresden.de
jonas.christen@zhdk.ch
pfarr@dg.tu-darmstadt.de</p1_emails>
  <p1_presenting_author>Münster, Sander
Christen, Jonas
Pfarr-Harfst, Mieke</p1_presenting_author>
  <p1_title>Modellathon „Digitale 3D-Rekonstruktion" </p1_title>
  <p1_abstract>&lt;p&gt;Im Rahmen des an das Format des Hackathons angelehnten "Modellathons" soll praktisches Handwerkszeug digitaler 3D-Rekonstruktion in wissenschaftlichen Kontexten vermittelt und erprobt werden. Der Modellathon beinhaltet einen ½ tägigen Workshop zur Vermittlung von Grundlagen der 3D-Rekonstruktionen und deren filmischer oder bildlicher Inszenierung. Daran anschliessend sollen im Rahmen eines die DHd-Jahrestagung begleitenden Wettbewerbs durch studentische Arbeitsteams 3D-Rekonstruktionen eines noch zu benennenden Objekts erstellt werden. Die Ergebnisse werden im abschliessenden DHd-Plenum vorgestellt und durch das Publikum sowie eine Expertenjury bewertet sowie prämiert.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>8</session_ID>
  <session_short>Workshop_5a</session_short>
  <session_title>Workshop_5a</session_title>
  <session_start>2018-02-26 14:00</session_start>
  <session_end>2018-02-26 15:30</session_end>
  <session_room_ID>6</session_room_ID>
  <session_room>Tagungsraum, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Mit dem Eintritt der Geisteswissenschaften in den digitalen Raum öffnet sich für Forschende auch ein neuer Rechtsraum mit Anforderungen und Problemstellungen, die uns bislang nicht oder nur marginal betroffen haben. 

Dieser Workshop soll die gängigsten Fragen beantworten, die sich aus unterschiedlichen Rechtsbereichen bei der Realisierung von Digitalisierungsvorhaben und digitalen Forschungsprojekten ergeben. 
Besonders eingegangen wird auf jüngste Entwicklungen und Neuerungen in den Legislaturen speziell der deutschsprachigen Länder, die für die DH von besonderer Relevanz sind (etwa die UhrWissG-Novelle in Deutschland oder die EU-Datenschutz-Grundverordnung, die jeweils 2018 in Kraft treten).

Ziel dieses Workshops ist es, die Teilnehmenden für die rechtlichen Aspekte des digitalen Arbeitens zu sensibilisieren und ihnen im Speziellen einen Überblick über jene Rechtsbereiche zu verschaffen, mit denen wir im Rahmen unser Forschungstätigkeiten konfrontiert werden.
Dabei wird ein interaktives Format gewählt, bei dem die TeilnehmerInnen die Gelegenheit finden, konkrete Fragen aus ihrer Anwendungspraxis im Rahmen der thematischen Blöcke sowie am Ende des Workshops in einer offenen Diskussion einzubringen und mit den anwesenden ExpertInnen und KollegInnen zu erörtern.</session_abstract>
  <attendee_count>5</attendee_count>
  <sessionID>8</sessionID>
  <presentations>1</presentations>
  <p1_paperID>137</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Hannesschläger, Vanessa
Kamocki, Pawel
Scholger, Walter</p1_authors>
  <p1_organisations>ACDH-ÖAW, Österreichische Akademie der Wissenschaften, Österreich
L'Université Paris Descartes, Frankreich
ZIM-ACDH, Universität Graz, Österreich</p1_organisations>
  <p1_emails>Vanessa.Hannesschlaeger@oeaw.ac.at
pawel.kamocki@gmail.com
walter.scholger@uni-graz.at</p1_emails>
  <p1_presenting_author>Hannesschläger, Vanessa
Kamocki, Pawel
Scholger, Walter</p1_presenting_author>
  <p1_title>Rechtsfragen in DH-Projekten: Alles, was man wissen muss</p1_title>
  <p1_abstract>&lt;p dir="ltr"&gt;Mit dem Eintritt der Geisteswissenschaften in den digitalen Raum öffnet sich für Forschende auch ein neuer Rechtsraum mit Anforderungen und Problemstellungen, die uns bislang nicht oder nur marginal betroffen haben. &lt;/p&gt;
&lt;p dir="ltr"&gt;Dieser Workshop soll die gängigsten Fragen beantworten, die sich aus unterschiedlichen Rechtsbereichen bei der Realisierung von Digitalisierungsvorhaben und digitalen Forschungsprojekten ergeben. &lt;br /&gt;Besonders eingegangen wird auf jüngste Entwicklungen und Neuerungen in den Legislaturen speziell der deutschsprachigen Länder, die für die DH von besonderer Relevanz sind (etwa die UhrWissG-Novelle in Deutschland oder die EU-Datenschutz-Grundverordnung, die jeweils 2018 in Kraft treten).&lt;/p&gt;
&lt;p dir="ltr"&gt;Ziel dieses Workshops ist es, die Teilnehmenden für die rechtlichen Aspekte des digitalen Arbeitens zu sensibilisieren und ihnen im Speziellen einen Überblick über jene Rechtsbereiche zu verschaffen, mit denen wir im Rahmen unser Forschungstätigkeiten konfrontiert werden.&lt;br /&gt;Dabei wird ein interaktives Format gewählt, bei dem die TeilnehmerInnen die Gelegenheit finden, konkrete Fragen aus ihrer Anwendungspraxis im Rahmen der thematischen Blöcke sowie am Ende des Workshops in einer offenen Diskussion einzubringen und mit den anwesenden ExpertInnen und KollegInnen zu erörtern.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>20</session_ID>
  <session_short>Workshop_6a</session_short>
  <session_title>Workshop_6a</session_title>
  <session_start>2018-02-26 14:00</session_start>
  <session_end>2018-02-26 15:30</session_end>
  <session_room_ID>7</session_room_ID>
  <session_room>S 01, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Dieser Workshop möchte mit den Teilnehmerinnen und Teilnehmern folgende Fragen diskutieren: Wie können Analysewerkzeuge den Forscherinnen und Forschern vielfältige Analysemethoden und Visualisierungsmethoden für verschiedene historische Korpora ermöglichen? Wie kann ANNIS die verschiedenen Analysemethoden bislang unterstützen? Wie kann es gelingen, auch die Vielfältigkeit der Forschungsdaten als solche zu berücksichtigen und deren Wiederverwendung zu ermöglichen? Wie können Werkzeuge spezifisch genug entwickelt werden, um genaue und für den Forschungskontext und die Forschungsdaten angepasste Analysen zu ermöglichen? Der Workshop hat das Ziel, anhand mehrerer historischer Korpora des Deutschen das generische Such- und Visualisierungstool ANNIS (Krause und Zeldes 2016) für den Einsatz in den Digital Humanities zu diskutieren und anzuwenden.

Um diese Fragen adressieren zu können, wird der Workshop zwei Schwerpunkte enthalten. Der erste Schwerpunkt wird die Einführung in die Funktionen und Suchanfragesprache von ANNIS sowie die damit verbundene Vorstellung der zwei historischen Beispielkorpora umfassen. Wir wollen den Teilnehmerinnen und Teilnehmern die verschiedenen Analyse- und Visualisierungsmöglichkeiten online und hands-on vorstellen. Über die Vorstellung zweier historischer Korpora mit dem generischen ANNIS können bereits die Herausforderungen der heterogenen Datengrundlage in den digitalen Geisteswissenschaften für Analysetools herausgearbeitet werden.

Der zweite Schwerpunkt soll Raum für eine Diskussion mit den Teilnehmerinnen und Teilnehmern sowie auch die Möglichkeit geben, weitere Korpora in ANNIS – geleitet von den Forschungsinteressen der Teilnehmerinnen und Teilnehmern – zu durchsuchen. Mit diesem Workshop wollen wir uns gemeinsam mit den Teilnehmerinnen und Teilnehmern kritisch mit den Anforderungen an ein Analysetool für verschiedene Methoden zur Analyse und Visualisierung von historischen Korpora auseinandersetzen und prüfen, in wie weit ANNIS bereits einige dieser Anforderungen erfüllen kann. So wollen wir ANNIS in einen neuen Forschungskontext der Digital Humanities diskutieren und dabei neue Nutzerszenarien für die weitere Entwicklung erarbeiten.</session_abstract>
  <attendee_count>3</attendee_count>
  <sessionID>20</sessionID>
  <presentations>1</presentations>
  <p1_paperID>127</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Odebrecht, Carolin
Krause, Thomas
Guescini, Rolf
Kühnlenz, Frank
Lüdeling, Anke
Dreyer, Malte</p1_authors>
  <p1_organisations>Humboldt-Universität zu Berlin, Deutschland
Humboldt-Universität zu Berlin, Deutschland
Humboldt-Universität zu Berlin, Deutschland
Humboldt-Universität zu Berlin, Deutschland
Humboldt-Universität zu Berlin, Deutschland
Humboldt-Universität zu Berlin, Deutschland</p1_organisations>
  <p1_emails>carolin.odebrecht@hu-berlin.de
krauseto@hu-berlin.de
rolf.guescini@cms.hu-berlin.de
frank.kuehnlenz@cms.hu-berlin.de
anke.luedeling@hu-berlin.de
malte.dreyer@cms.hu-berlin.de</p1_emails>
  <p1_presenting_author>Odebrecht, Carolin
Krause, Thomas
Guescini, Rolf
Kühnlenz, Frank</p1_presenting_author>
  <p1_title>Suche und Visualisierung von Annotationen historischer Korpora mit ANNIS. Kritik der korpuslinguistischen Analysemethoden in einem erweiterten Nutzungskontext</p1_title>
  <p1_abstract>&lt;p&gt;Dieser Workshop möchte mit den Teilnehmerinnen und Teilnehmern folgende Fragen diskutieren: Wie können Analysewerkzeuge den Forscherinnen und Forschern vielfältige Analysemethoden und Visualisierungsmethoden für verschiedene historische Korpora ermöglichen? Wie kann ANNIS die verschiedenen Analysemethoden bislang unterstützen? Wie kann es gelingen, auch die Vielfältigkeit der Forschungsdaten als solche zu berücksichtigen und deren Wiederverwendung zu ermöglichen? Wie können Werkzeuge spezifisch genug entwickelt werden, um genaue und für den Forschungskontext und die Forschungsdaten angepasste Analysen zu ermöglichen? Der Workshop hat das Ziel, anhand mehrerer historischer Korpora des Deutschen das generische Such- und Visualisierungstool ANNIS (Krause und Zeldes 2016) für den Einsatz in den Digital Humanities zu diskutieren und anzuwenden.&lt;/p&gt;
&lt;p&gt;Um diese Fragen adressieren zu können, wird der Workshop zwei Schwerpunkte enthalten. Der erste Schwerpunkt wird die Einführung in die Funktionen und Suchanfragesprache von ANNIS sowie die damit verbundene Vorstellung der zwei historischen Beispielkorpora umfassen. Wir wollen den Teilnehmerinnen und Teilnehmern die verschiedenen Analyse- und Visualisierungsmöglichkeiten online und hands-on vorstellen. Über die Vorstellung zweier historischer Korpora mit dem generischen ANNIS können bereits die Herausforderungen der heterogenen Datengrundlage in den digitalen Geisteswissenschaften für Analysetools herausgearbeitet werden.&lt;/p&gt;
&lt;p&gt;Der zweite Schwerpunkt soll Raum für eine Diskussion mit den Teilnehmerinnen und Teilnehmern sowie auch die Möglichkeit geben, weitere Korpora in ANNIS – geleitet von den Forschungsinteressen der Teilnehmerinnen und Teilnehmern – zu durchsuchen. Mit diesem Workshop wollen wir uns gemeinsam mit den Teilnehmerinnen und Teilnehmern kritisch mit den Anforderungen an ein Analysetool für verschiedene Methoden zur Analyse und Visualisierung von historischen Korpora auseinandersetzen und prüfen, in wie weit ANNIS bereits einige dieser Anforderungen erfüllen kann. So wollen wir ANNIS in einen neuen Forschungskontext der Digital Humanities diskutieren und dabei neue Nutzerszenarien für die weitere Entwicklung erarbeiten.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>12</session_ID>
  <session_short>Workshop_7a</session_short>
  <session_title>Workshop_7a</session_title>
  <session_start>2018-02-26 14:00</session_start>
  <session_end>2018-02-26 15:30</session_end>
  <session_room_ID>12</session_room_ID>
  <session_room>S 15, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>eComparatio ist ein einfach und ohne Vorkenntnisse zu bedienendes Tool für den Vergleich prinzipiell beliebig vieler und beliebig langer digitalisierter Editionen vorrangig griechischer und lateinischer, technisch gesehen aber auch sämtlicher anderer in einem UNICODE-Format zugänglicher Texte. eComparatio ist sowohl browserbasiert als auch offline verwendbar und ermöglicht auf der Basis des Textvergleichs die Erstellung eines digitalen apparatus criticus sowie eine Visualisierung des Textvergleichs als Diagrammdarstellung. Ziel des Workshops ist die Einführung in die Anwendung auf der Grundlage der Textkritik. Die Teilnehmer können eigene Texte (griechische, lateinische, deutsche, englische Texte) mitbringen, anhand derer praktisch gearbeitet werden soll.</session_abstract>
  <attendee_count>3</attendee_count>
  <sessionID>12</sessionID>
  <presentations>1</presentations>
  <p1_paperID>142</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Schubert, Charlotte
Kahl, Hannes
Meins, Friedrich
Bräckel, Oliver</p1_authors>
  <p1_organisations>Universität Leipzig\Historisches Seminar, Deutschland
Universität Leipzig\Historisches Seminar, Deutschland
Universität Leipzig\Historisches Seminar, Deutschland
Universität Leipzig\Historisches Seminar, Deutschland</p1_organisations>
  <p1_emails>schubert@uni-leipzig.de
hannes.kahl@uni-leipzig.de
friedrich_meins@uni-leipzig.de
oliver.braeckel@uni-leipzig.de</p1_emails>
  <p1_presenting_author>Schubert, Charlotte
Kahl, Hannes
Meins, Friedrich
Bräckel, Oliver</p1_presenting_author>
  <p1_title>Workshop eComparatio: Textvergleich und digitaler Apparat</p1_title>
  <p1_abstract>&lt;p&gt;eComparatio ist ein einfach und ohne Vorkenntnisse zu bedienendes Tool für den Vergleich prinzipiell beliebig vieler und beliebig langer digitalisierter Editionen vorrangig griechischer und lateinischer, technisch gesehen aber auch sämtlicher anderer in einem UNICODE-Format zugänglicher Texte. eComparatio ist sowohl browserbasiert als auch offline verwendbar und ermöglicht auf der Basis des Textvergleichs die Erstellung eines digitalen apparatus criticus sowie eine Visualisierung des Textvergleichs als Diagrammdarstellung. Ziel des Workshops ist die Einführung in die Anwendung auf der Grundlage der Textkritik. Die Teilnehmer können eigene Texte (griechische, lateinische, deutsche, englische Texte) mitbringen, anhand derer praktisch gearbeitet werden soll.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>67</session_ID>
  <session_title>Kaffeepause</session_title>
  <session_start>2018-02-26 15:30</session_start>
  <session_end>2018-02-26 16:00</session_end>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>132</session_ID>
  <session_short>Workshop_1b</session_short>
  <session_title>Automatic Text Recognition: Mit Transkribus Texterkennung trainieren und anwenden</session_title>
  <session_start>2018-02-26 16:00</session_start>
  <session_end>2018-02-26 17:30</session_end>
  <session_room_ID>8</session_room_ID>
  <session_room>S 11, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>133</session_ID>
  <session_short>Workshop_2b</session_short>
  <session_title>CorpusExplorer v2.0: Seminartauglich in einem halben Tag</session_title>
  <session_start>2018-02-26 16:00</session_start>
  <session_end>2018-02-26 17:30</session_end>
  <session_room_ID>9</session_room_ID>
  <session_room>S 12, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>134</session_ID>
  <session_short>Workshop_3b</session_short>
  <session_title>Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse</session_title>
  <session_start>2018-02-26 16:00</session_start>
  <session_end>2018-02-26 17:30</session_end>
  <session_room_ID>10</session_room_ID>
  <session_room>S 13, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>135</session_ID>
  <session_short>Workshop_4b</session_short>
  <session_title>Modellathon „Digitale 3D-Rekonstruktionen“</session_title>
  <session_start>2018-02-26 16:00</session_start>
  <session_end>2018-02-26 17:30</session_end>
  <session_room_ID>11</session_room_ID>
  <session_room>S 14, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>136</session_ID>
  <session_short>Workshop_5b</session_short>
  <session_title>Rechtsfragen in DH-Projekten: Alles, was man wissen muss</session_title>
  <session_start>2018-02-26 16:00</session_start>
  <session_end>2018-02-26 17:30</session_end>
  <session_room_ID>6</session_room_ID>
  <session_room>Tagungsraum, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>137</session_ID>
  <session_short>Workshop_6b</session_short>
  <session_title>Suche und Visualisierung von Annotationen historischer Korpora mit ANNIS. Kritik der korpuslinguistischen Analysemethoden in einem erweiterten Nutzungskontext</session_title>
  <session_start>2018-02-26 16:00</session_start>
  <session_end>2018-02-26 17:30</session_end>
  <session_room_ID>7</session_room_ID>
  <session_room>S 01, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>25</session_ID>
  <session_short>Workshop_7b</session_short>
  <session_title>Workshop eComparatio: Textvergleich und digitaler Apparat</session_title>
  <session_start>2018-02-26 16:00</session_start>
  <session_end>2018-02-26 17:30</session_end>
  <session_room_ID>12</session_room_ID>
  <session_room>S 15, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>182</session_ID>
  <session_short>DARIAH-DE</session_short>
  <session_title>Teamtreffen</session_title>
  <session_start>2018-02-26 17:00</session_start>
  <session_end>2018-02-26 18:00</session_end>
  <session_room_ID>15</session_room_ID>
  <session_room>S 22, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>168</session_ID>
  <session_title>Öffnungszeiten Konferenzsekretariat</session_title>
  <session_start>2018-02-27 08:30</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>5</session_room_ID>
  <session_room>Hörsaal F, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>11</session_ID>
  <session_short>Workshop_10a</session_short>
  <session_title>Workshop_10a</session_title>
  <session_start>2018-02-27 09:00</session_start>
  <session_end>2018-02-27 10:30</session_end>
  <session_room_ID>9</session_room_ID>
  <session_room>S 12, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>The workshop presents ATHEN(Annotation and Text Highlighting Environment), an extensible desktop-based annotation environment which supports more than just regular annotation. Besides being a general purpose annotation environment, ATHEN supports indexing and querying support of your data as well as the ability to automatically preprocess your data with Meta information. It is especially suited for those who want to extend existing general- purpose annotation tools by implementing their own custom features, which cannot be fulfilled by other available annotation environments.</session_abstract>
  <attendee_count>3</attendee_count>
  <sessionID>11</sessionID>
  <presentations>1</presentations>
  <p1_paperID>168</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Krug, Markus
Tu, Ngoc Duyen Tanja
Weimer, Lukas
Reger, Isabella
Konle, Leonard
Jannidis, Fotis
Puppe, Frank</p1_authors>
  <p1_organisations>Universität Würzburg, Deutschland
Institut für Deutsche Sprache, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland</p1_organisations>
  <p1_emails>markus.krug@uni-wuerzburg.de
tu@ids-mannheim.de
lukas.weimer@uni-wuerzburg.de
isabella.reger@uni-wuerzburg.de
leonard.konle@uni-wuerzburg.de
fotis.jannidis@uni-wuerzburg.de
puppe@informatik.uni-wuerzburg.de</p1_emails>
  <p1_presenting_author>Krug, Markus
Tu, Ngoc Duyen Tanja</p1_presenting_author>
  <p1_title>Annotation and beyond – Using ATHEN</p1_title>
  <p1_abstract>&lt;p&gt;The workshop presents ATHEN&lt;sup&gt;&lt;/sup&gt;(&lt;strong&gt;A&lt;/strong&gt;nnotation and &lt;strong&gt;T&lt;/strong&gt;ext &lt;strong&gt;H&lt;/strong&gt;ighlighting &lt;strong&gt;En&lt;/strong&gt;vironment), an extensible desktop-based annotation environment which supports more than just regular annotation. Besides being a general purpose annotation environment, ATHEN supports indexing and querying support of your data as well as the ability to automatically preprocess your data with Meta information. It is especially suited for those who want to extend existing general- purpose annotation tools by implementing their own custom features, which cannot be fulfilled by other available annotation environments.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>10</session_ID>
  <session_short>Workshop_11a</session_short>
  <session_title>Workshop_11a</session_title>
  <session_start>2018-02-27 09:00</session_start>
  <session_end>2018-02-27 10:30</session_end>
  <session_room_ID>6</session_room_ID>
  <session_room>Tagungsraum, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Software ist ein integraler Bestandteil jeder Forschungsaktivität, so auch in den Digital Humanities zur Bearbeitung von geistes- und kulturwissenschaftlichen Forschungsfragen. Im allgemeinen Sinn verstanden reicht das Anwendungsfeld für geisteswissenschaftliche Forschungssoftware von der täglichen Arbeit mit Webbrowsern und Textverarbeitungsprogrammen bis hin zum Einsatz spezialisierter Softwarelösungen, virtueller Forschungsumgebungen oder auch webbasierter Publikations- und Analyseinstrumente für geisteswissenschaftliche Forschungsdaten. Im speziellen Sinn und insbesondere im projektspezifischen Forschungskontext verstanden ist Forschungssoftware in den Digital Humanities als Summe spezifischer Komponenten aufzufassen, die unter entwicklerischer Durchdringung der jeweiligen Wissens- und Anwendungsdomäne konzipiert und implementiert werden.

Zum aktuellen Zeitpunkt finden die Entwicklungsprozesse für Forschungssoftware in den Digital Humanities häufig noch isoliert, unreflektiert, undokumentiert, nicht an gängigen Industriestandards ausgerichtet und insbesondere nicht innerhalb eines organisierten, disziplinspezifischen Rahmens statt. Aus dieser Situation heraus ergibt sich ein eminentes Nachhaltigkeitsproblem für geisteswissenschaftliche Forschungssoftware.

Der hier vorgeschlagene Workshop versteht sich als eine erste Maßnahme, auch in den Digital Humanities im deutschsprachigen Raum einen kritischen Reflexionsprozess zum Thema ‘Nachhaltige Softwareentwicklung’ anzustoßen und durch die Etablierung einer gemeinsamen Diskussionsplattform ein stärkeres Bewusstsein für diesen zentralen, aber vernachlässigten Baustein guter Digital Humanities Forschung zu wecken.</session_abstract>
  <attendee_count>3</attendee_count>
  <sessionID>10</sessionID>
  <presentations>1</presentations>
  <p1_paperID>207</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Schrade, Torsten
Czmiel, Alexander
Druskat, Stephan</p1_authors>
  <p1_organisations>Akademie der Wissenschaften und der Literatur Mainz, Deutschland
Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland
Humboldt-Universität zu Berlin, Deutschland</p1_organisations>
  <p1_emails>Torsten.Schrade@adwmainz.de
czmiel@bbaw.de
stephan.druskat@hu-berlin.de</p1_emails>
  <p1_presenting_author>Schrade, Torsten
Czmiel, Alexander
Druskat, Stephan</p1_presenting_author>
  <p1_title>Research Software Engineering und Digital Humanities. Reflexion, Kartierung, Organisation.</p1_title>
  <p1_abstract>&lt;p&gt;&lt;strong&gt;Software ist ein integraler Bestandteil jeder Forschungsaktivität, so auch in den Digital Humanities zur Bearbeitung von geistes- und kulturwissenschaftlichen Forschungsfragen. Im allgemeinen Sinn verstanden reicht das Anwendungsfeld für geisteswissenschaftliche Forschungssoftware von der täglichen Arbeit mit Webbrowsern und Textverarbeitungsprogrammen bis hin zum Einsatz spezialisierter Softwarelösungen, virtueller Forschungsumgebungen oder auch webbasierter Publikations- und Analyseinstrumente für geisteswissenschaftliche Forschungsdaten. Im speziellen Sinn und insbesondere im projektspezifischen Forschungskontext verstanden ist Forschungssoftware in den Digital Humanities als Summe spezifischer Komponenten aufzufassen, die unter entwicklerischer Durchdringung der jeweiligen Wissens- und Anwendungsdomäne konzipiert und implementiert werden.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Zum aktuellen Zeitpunkt finden die Entwicklungsprozesse für Forschungssoftware in den Digital Humanities häufig noch isoliert, unreflektiert, undokumentiert, nicht an gängigen Industriestandards ausgerichtet und insbesondere nicht innerhalb eines organisierten, disziplinspezifischen Rahmens statt. Aus dieser Situation heraus ergibt sich ein eminentes Nachhaltigkeitsproblem für geisteswissenschaftliche Forschungssoftware.&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Der hier vorgeschlagene Workshop versteht sich als eine erste Maßnahme, auch in den Digital Humanities im deutschsprachigen Raum einen kritischen Reflexionsprozess zum Thema ‘Nachhaltige Softwareentwicklung’ anzustoßen und durch die Etablierung einer gemeinsamen Diskussionsplattform ein stärkeres Bewusstsein für diesen zentralen, aber vernachlässigten Baustein guter Digital Humanities Forschung zu wecken.&lt;/strong&gt;&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>22</session_ID>
  <session_short>Workshop_12a</session_short>
  <session_title>Workshop_12a</session_title>
  <session_start>2018-02-27 09:00</session_start>
  <session_end>2018-02-27 10:30</session_end>
  <session_room_ID>12</session_room_ID>
  <session_room>S 15, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Digitale Spracharchive sind ein integraler Bestandteil der Forschungsdateninfrastruktur und haben den spezifische Auftrag, audiovisuelle Sprachdaten und Dokumente zu sichern und auf deren Basis Wissensgenerierung zu ermöglichen und zu unterstützen. Ein Spracharchiv ist in diesem Sinn eine Plattform, die zwischen Produzenten und Konsumenten von Primärdaten vermittelt, so dass diese direkt oder indirekt interagieren können. Den datenproduzierenden Forschern ermöglicht das Archiv, Audio- und Videoaufnahmen menschlicher Kommunikation zu archivieren und idealerweise web-basiert zugänglich zu machen. Auf der anderen Seite werden Forscher, Sprachgemeinschaften und die weitere Öffentlichkeit in die Lage versetzt, diese Daten aufzufinden, zu betrachten, herunterzuladen und weiterzuverwenden und auf dieser Grundlage neues Wissen zu generieren. Um diesen Austausch zu unterstützen, haben die verschiedenen Spracharchive komplexe Webplattformen entwickelt.

Wie alle Forschungsdatenarchive profitieren Repositorien für audiovisuelle Daten von einem voranschreitenden Standardisierungsprozess. Infrastrukturinitiativen wirken als starke integrative Kraft und haben mit der Etablierung von Standards, die in allen Aspekten des Datenlebenszyklus zur Anwendung kommen, gemeinsame Lösungen geschaffen. Forschungsdatenrepositorien für Sprachdaten sind herausgefordert, gültige Standards zu implementieren und gleichzeitig attraktive Dienste anzubieten, die die Spezifika der Datentypen und die Bedürfnisse der jeweiligen Nutzergruppen berücksichtigen, um eine erfolgreiche Nachnutzung von Forschungsdaten zu befördern.

Der Workshop soll Archivbetreibern, Datenkuratoren, Datenproduzenten und Datenkonsumenten die Möglichkeit zum Austausch über Angebote, Bedarfe und zentrale Weiterentwicklungen geben. Der Workshop richtet sich an Betreiber von Forschungsdatenrepositorien mit audiovisuellen (Sprach-)Daten sowie Mitarbeiter von Institutionen, die Forschungsdatenmanagement für Forschung mit audiovisuellen Daten anbieten. Ebenso sind auch Wissenschaftler als aktive oder potentielle Nutzer von Forschungsdatenrepositoren angesprochen.</session_abstract>
  <attendee_count>4</attendee_count>
  <sessionID>22</sessionID>
  <presentations>1</presentations>
  <p1_paperID>257</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Blumtritt, Jonathan
Rau, Felix</p1_authors>
  <p1_organisations>Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p1_organisations>
  <p1_emails>jonathan.blumtritt@uni-koeln.de
f.rau@uni-koeln.de</p1_emails>
  <p1_presenting_author>Blumtritt, Jonathan
Rau, Felix</p1_presenting_author>
  <p1_title>Nutzerunterstützung und neueste Entwicklungen in Forschungsdatenrepositorien für audiovisuelle (Sprach-)Daten</p1_title>
  <p1_abstract>&lt;p dir="ltr"&gt;Digitale Spracharchive sind ein integraler Bestandteil der Forschungsdateninfrastruktur und haben den spezifische Auftrag, audiovisuelle Sprachdaten und Dokumente zu sichern und auf deren Basis Wissensgenerierung zu ermöglichen und zu unterstützen. Ein Spracharchiv ist in diesem Sinn eine Plattform, die zwischen Produzenten und Konsumenten von Primärdaten vermittelt, so dass diese direkt oder indirekt interagieren können. Den datenproduzierenden Forschern ermöglicht das Archiv, Audio- und Videoaufnahmen menschlicher Kommunikation zu archivieren und idealerweise web-basiert zugänglich zu machen. Auf der anderen Seite werden Forscher, Sprachgemeinschaften und die weitere Öffentlichkeit in die Lage versetzt, diese Daten aufzufinden, zu betrachten, herunterzuladen und weiterzuverwenden und auf dieser Grundlage neues Wissen zu generieren. Um diesen Austausch zu unterstützen, haben die verschiedenen Spracharchive komplexe Webplattformen entwickelt.&lt;/p&gt;
&lt;p dir="ltr"&gt;Wie alle Forschungsdatenarchive profitieren Repositorien für audiovisuelle Daten von einem voranschreitenden Standardisierungsprozess. Infrastrukturinitiativen wirken als starke integrative Kraft und haben mit der Etablierung von Standards, die in allen Aspekten des Datenlebenszyklus zur Anwendung kommen, gemeinsame Lösungen geschaffen. Forschungsdatenrepositorien für Sprachdaten sind herausgefordert, gültige Standards zu implementieren und gleichzeitig attraktive Dienste anzubieten, die die Spezifika der Datentypen und die Bedürfnisse der jeweiligen Nutzergruppen berücksichtigen, um eine erfolgreiche Nachnutzung von Forschungsdaten zu befördern.&lt;/p&gt;
&lt;p&gt;Der Workshop soll Archivbetreibern, Datenkuratoren, Datenproduzenten und Datenkonsumenten die Möglichkeit zum Austausch über Angebote, Bedarfe und zentrale Weiterentwicklungen geben. Der Workshop richtet sich an Betreiber von Forschungsdatenrepositorien mit audiovisuellen (Sprach-)Daten sowie Mitarbeiter von Institutionen, die Forschungsdatenmanagement für Forschung mit audiovisuellen Daten anbieten. Ebenso sind auch Wissenschaftler als aktive oder potentielle Nutzer von Forschungsdatenrepositoren angesprochen.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>13</session_ID>
  <session_short>Workshop_8a</session_short>
  <session_title>Workshop_8a</session_title>
  <session_start>2018-02-27 09:00</session_start>
  <session_end>2018-02-27 10:30</session_end>
  <session_room_ID>7</session_room_ID>
  <session_room>S 01, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Die systematische Erfassung und wissenschaftliche Erschließung einer Sammlung sind grundlegende Voraussetzungen, um ihr wissenschaftliches Potential sichtbar zu machen. Häufig aber fehlen Software-Lösungen und Know-How für eine flächendeckende Digitalisierung und Online-Präsenz. 
Dieser Workshop führt anhand praktischer Beispiele in die digitale Sammlungsarbeit mit WissKI und in die Modellierung mit dem CIDOC Conceptual Reference Model (CRM) ein. Durch die praktische Arbeit lernen die Teilnehmer die im Projekt „Objekte im Netz" bereitgestellte Modellierung sowie die Konfiguration der Virtuellen Forschungsumgebung (VFU) für universitäre Sammlungen kennen.</session_abstract>
  <attendee_count>5</attendee_count>
  <sessionID>13</sessionID>
  <presentations>1</presentations>
  <p1_paperID>148</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Scholz, Martin
Wagner, Sarah</p1_authors>
  <p1_organisations>Friedrich-Alexander-Universität Erlangen-Nürnberg, Deutschland
Germanisches Nationalmuseum, Deutschland</p1_organisations>
  <p1_emails>martin.scholz@fau.de
s.wagner@gnm.de</p1_emails>
  <p1_presenting_author>Scholz, Martin
Wagner, Sarah</p1_presenting_author>
  <p1_title>Digitale Sammlungserschließung mit WissKI und CIDOC CRM</p1_title>
  <p1_abstract>&lt;p dir="ltr"&gt;Die systematische Erfassung und wissenschaftliche Erschließung einer Sammlung sind grundlegende Voraussetzungen, um ihr wissenschaftliches Potential sichtbar zu machen. Häufig aber fehlen Software-Lösungen und Know-How für eine flächendeckende Digitalisierung und Online-Präsenz.&lt;/p&gt;
&lt;p dir="ltr"&gt;Dieser Workshop führt anhand praktischer Beispiele in die digitale Sammlungsarbeit mit WissKI und in die Modellierung mit dem CIDOC Conceptual Reference Model (CRM) ein. Durch die praktische Arbeit lernen die Teilnehmer die im Projekt „Objekte im Netz" bereitgestellte Modellierung sowie die Konfiguration der Virtuellen Forschungsumgebung (VFU) für universitäre Sammlungen kennen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Erschließung und Digitalisierung von Sammlungen&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Neben Museen beherbergen auch Universitäten einen großen Schatz an Sammlungen, die der Wissenschaftrat 2011 „als wertvolle Infrastruktur für [...] Forschung” mit „beachtliche[m] wissenschaftliche[n] Potential” identifiziert hat. Allein in Deutschland existieren rund 1000 Sammlungen an über 80 Universitäten. Zwar sind darunter auch renommierte Sammlungen, doch leidet das Gros an unzureichender Erschließung, Sichtbarkeit, Betreuung, Pflege oder Unterbringung. Auch bei der Digitalisierung gibt es enormen Aufholbedarf: Lediglich ein Drittel der Sammlungen sind digital zugänglich. Grund dafür sind u.a. auch das Fehlen von Software-Lösungen und Know-How für eine flächendeckende Digitalisierung und Online-Präsenz.&lt;/p&gt;
&lt;p dir="ltr"&gt;Seit einigen Jahren gibt es verstärkte Anstrengungen, universitäre Sammlungen aus ihrem Dornröschenschlaf zu wecken und sie zu einer wichtigen Ergänzung objektgebundener Forschung und Lehre weiter zu entwickeln. Dies drückt sich unter anderem in deutschlandweiten Förderprogrammen aus, wie etwa der „Allianz für universitäre Sammlungen” des Bundesministeriums für Bildung und Forschung. Das darin geförderte Projekt „Objekte im Netz” konzentriert sich auf die Digitalisierung universitärer Sammlungen und entwickelt in einer Kooperation zwischen der Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) und dem Germanischen Nationalmuseum Nürnberg (GNM) eine gemeinsame Erschließungs- und Digitalisierungsstrategie für die Sammlungen der FAU, um die wissenschaftliche Nutzbarkeit der reichhaltigen Bestände zu verbessern.&lt;/p&gt;
&lt;p dir="ltr"&gt;Im Fokus stehen jedoch nicht einzelne Sammlungen oder Fachbereiche, sondern die Bereitstellung von Software-Werkzeugen und Lösungswegen, um die digitale Erschließung und Verfügbarkeit an breiter Front voranzutreiben. Die über 20 Sammlungen der FAU bilden dabei eine äußerst heterogene Entwicklungs- und Testlandschaft, um Lösungen zu erarbeiten, die über die FAU hinaus anwendbar sind. Die nötige Generizität der Ansätze und die Nachhaltigkeit sind daher zentrale Herausforderungen, wobei bei letzterem die langfristige Interpretierbarkeit der Daten im Blickpunkt des Projekts steht. Daneben müssen die meist knappen personellen und finanziellen Mittel berücksichtigt werden.&lt;/p&gt;
&lt;p&gt;Als besonders geeignet zur Umsetzung der Ziele erscheinen auf technischer Seite Lösungen, die unter freien Lizenzen (Open Source) zur Verfügung stehen und die Ideen des Semantic Web implementieren: Flexible Wissensnetze mit klar definierter Semantik, die weltweit – und damit auch sammlungsübergreifend – verknüpft werden können. Das Projekt erweitert daher die virtuelle Forschungs- und Dokumentationsumgebung WissKI zu einem Werkzeug für die digitale Sammlungserschließung und stellt auf verschiedene Sammlungsbereiche abgestimmte Konfigurationen der Software sowie Leitfäden der Community zur Verfügung. Für die standardisierte semantische Auszeichnung der Daten kommt das Conceptual Reference Model (CIDOC CRM) zum Einsatz.&lt;/p&gt;
&lt;p&gt;Aufgrund einer erfolgreichen Pilotstudie kann das Projekt bereits auf erste Ergebnisse verweisen. Im Rahmen des Workshops wird die bereits publizierte generische Konfiguration vorgestellt und von den Teilnehmern angewandt.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Die Werkzeuge - WissKI und CIDOC CRM&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Semantische Technologien - im Speziellen Semantic Web und Linked Open Data - erfreuen sich zunehmender Beliebtheit in den Digital Humanities. Für objektbasierte Forschung bieten die flexible, netzwerkartige Grundstruktur des Resource Description Framework (RDF) und darauf aufbauende Formate ein adäquates Mittel zur Repräsentation, Verwaltung und Publikation von (Meta-)Daten. Zahlreiche VFUs unterstützen das Erstellen von komplexen Wissensnetzen und deren Export in Tripelformaten. Wichtige Normdateien und Thesauri stehen als Linked Open Data zur Verfügung. Ontologien wie das CIDOC CRM bilden das semantische Rückgrat dieses Ansatzes und garantieren ein Mindestmaß an Interoperabilität und Datenaustausch, das über das klassische Verlinken von Web-Dokumenten hinausgeht.&lt;/p&gt;
&lt;p dir="ltr"&gt;Wenngleich die Nutzung semantischer Technologien zunimmt, stellt der praktische Einsatz unerfahrene (digitale) Geisteswissenschaftler meist vor große Herausforderungen. Dies gilt weniger für die Beherrschung bestimmter Formate und Werkzeuge als vielmehr für die semantische Modellierung der Daten, d.h. die Erstellung von und den richtigen Umgang mit Ontologien. Da hierbei die Bedeutung der Daten formalisiert niedergelegt wird, ist teils ein gehöriges Maß an Wissen über einen Anwendungs-/Fachbereich erforderlich, um Modellierungsfehler zu vermeiden und so eine spätere korrekte Interpretation zu gewährleisten. Insbesondere das CIDOC CRM, das eine Top-Level-Ontologie für die Dokumentation kulturellen Erbes darstellt, steht immer wieder in der Kritik, für Einsteiger zu komplex zu sein.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die virtuelle Forschungs- und Dokumentationsumgebung WissKI nimmt sich dieser Herausforderung an. Die browserbasierte WissKI-Software ist das Produkt aus zwei DFG-geförderten Projekten und entstand aus Anforderungen an die kooperative Forschung in Museen bzw. im Bereich des Kulturerbes und seiner Dokumentation im digitalen Medium. Zentraler Fokus von WissKI ist das vernetzte Arbeiten auf Basis semantischer Tiefenerschließung von Forschungsdaten. Eine Schlüsselrolle kommt hierbei dem CIDOC CRM zu, das um projektspezifische Anwendungsontologien erweitert werden kann.&lt;/p&gt;
&lt;p dir="ltr"&gt;Aus Nutzersicht ist das System an die tradierten Formen der Datenakquise und -präsentation angelehnt. Die Daten werden jedoch semantisch aufbereitet und nativ als RDF mitsamt Ontologie-Konstrukten gespeichert. Dem Nutzer werden so die Vorteile von Linked Open Data und Semantic Web zugänglich, ohne dass dieser sich mit technischen und ontologischen Details auseinandersetzen muss. Kern dieses Ansatzes ist eine Abbildung zwischen den tradierten, meist datensatz-basierten, tabellarischen Darstellungen und der graphbasierten Wissensrepräsentation, die die ontologiegestützte, formale Semantik der verwendeten Datenfelder beinhaltet. Diese Abbildung wird von einem inhaltlichen Administrator festgelegt und ist für die Nutzer standardmäßig nicht sichtbar. Die formale Semantik muss also nicht verstanden werden, um das System effektiv zu nutzen. Abbildungen oder Teile davon können zwischen verschiedenen Systemen wiederverwendet und erweitert werden, so dass sich Best-Practice-Modellierungen herausbilden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Open-Source-Lizenzierung aller in diesem Workshop verwendeten Werkzeuge und Standards ist ein wichtiger Aspekt. Die kostenfreie Nutzung trägt zum einen der häufig angespannten finanziellen Situation universitärer Sammlungen Rechnung und ist zum anderen Bestandteil des partizipativen Konzepts: Anwender können die Materialien nutzen, an ihre Bedürfnisse anpassen und wiederum der Community zur Verfügung stellen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zielgruppe sowie Inhalt und Ziele des Workshops&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Der Workshop richtet sich an alle, die mit Sammlungsobjekten oder mit Objekten des kulturellen Erbes im Allgemeinen arbeiten und diese digital dokumentieren oder erschließen. Auch spricht der Workshop interessierte Wissenschaftler an, die Objekte standardisiert dokumentieren und ihre Metadaten semantisch aufzubereiten möchten. Es werden von den Teilnehmern keine Vorkenntnisse für die VFU WissKI oder das CIDOC CRM vorausgesetzt.&lt;/p&gt;
&lt;p&gt;Der Workshop zeigt anhand praktischer Beispiele, wie Erfassungsschemata und -modi aus der universitären Sammlungslandschaft mithilfe der Referenzontologie CIDOC CRM und der VFU WissKI auf Objekte universitärer Sammlungen bzw. des kulturellen Erbes im Allgemeinen umgesetzt werden können.&lt;/p&gt;
&lt;p dir="ltr"&gt;Während des Workshops arbeiten die Teilnehmer mit ihrem eigenen WissKI-System, wahlweise einzeln oder in Kleingruppen. Dabei stehen weniger die informationstechnischen Details der Werkzeuge im Vordergrund. Vielmehr werden die nötigen Schritte bis zum effektiv einsetzbaren System vermittelt und durchgeführt. Angefangen bei der Installation und einigen grundlegenden Funktionalitäten, binden die Teilnehmer die vom Projekt „Objekte im Netz” angebotene Konfiguration zur Sammlungserschließung in WissKI ein und erhalten somit ein einsetzbares System mit standardisierten Eingabe- und Anzeigemöglichkeiten. Darauf aufbauend werden Möglichkeiten der einfachen Anpassung der semantischen Modellierung aufgezeigt und selbständig geübt. Das Erfassen von (selbst mitgebrachten) Datensätzen rundet die praktische Einführung ab.&lt;/p&gt;
&lt;p dir="ltr"&gt;Neben einer allgemeinen Einführung in das Arbeiten mit WissKI und der semantischen Dokumentation von Daten sind die Teilnehmer nach dem Workshop in der Lage, einfache Erfassungsmasken zu modellieren, Daten mit WissKI zu erfassen und zu recherchieren.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>184</session_ID>
  <session_short>BMBF</session_short>
  <session_title>eHumanities Nachwuchsgruppen</session_title>
  <session_start>2018-02-27 09:00</session_start>
  <session_end>2018-02-27 12:30</session_end>
  <session_room_ID>14</session_room_ID>
  <session_room>S 21, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>9</session_ID>
  <session_short>Workshop_9</session_short>
  <session_title>Workshop_9</session_title>
  <session_start>2018-02-27 09:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>8</session_room_ID>
  <session_room>S 11, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>In recent years, the Digital Humanities have witnessed the steadily
growing popularity of models from distributional semantics, like LDA
topic modeling and Word2vec. But in spite of their huge potential for
Digital Humanities, multiple aspects of their application still remain
untapped. This workshop by the DARIAH working group *Text and Data
Analytics* and *Digital Humanities Flanders* (DHuF) will bring together
practicioners to present and discuss recent developments in the field,
with David Bamman (University of California, Berkeley) presenting a
keynote lecture. The workshop also aims to reach non-presenting
participants who take an active interest in distributional models.</session_abstract>
  <attendee_count>2</attendee_count>
  <sessionID>9</sessionID>
  <presentations>1</presentations>
  <p1_paperID>220</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Jannidis, Fotis
Kestemont, Mike</p1_authors>
  <p1_organisations>Universität Würzburg, Deutschland
Universität Antwerpen, Belgien</p1_organisations>
  <p1_emails>fotis.jannidis@uni-wuerzburg.de
mike.kestemont@gmail.com</p1_emails>
  <p1_presenting_author>Jannidis, Fotis</p1_presenting_author>
  <p1_title> Embedded Humanities</p1_title>
  <p1_abstract>&lt;p align="justify"&gt;In recent years, the Digital Humanities have witnessed the steadily growing popularity of models from distributional semantics which can be used to model the meaning of documents and words in large digital text collections. Well-known examples of influential distributional models include Latent Dirichlet Allocation for topic modelling (Blei et al.) or Word2vec for estimating word vectors (Mikolov et al. 2013). Such distributional models have recently gained much prominence in the fields of Natural Language Processing and, more recently, Deep Representation Learning (Manning 2016). Humanities data is typically sparse and distributional models help scholars obtain smoother estimations of them. Whereas, for instance, words are conventionally encoded as binary ‘one-hot vectors’ in digital text analysis, embedding techniques from distributional semantics allow scholars to obtain dense, yet rich representations of vocabularies. These embedded representations are known to capture all sorts of valuable relationships between data points, although embedding techniques are typically trained using unsupervised objectives and require relatively little parameter tuning from scholars. Inspiring applications of this emergent technology in DH have ranged from more technical work in cultural studies at large (Bamman et al. 2014), case studies in literary history (Mimno 2012; Schoech 2017) or valuable DH-oriented web apps, such as ShiCo (Martinez-Ortiz et al. 2016). The availability of high-quality implementations in the public domain, in software suites as gensim, word2vec, or mallet etc. has greatly added these methods’ popularity.&lt;/p&gt;
&lt;p align="justify"&gt;&lt;/p&gt;
&lt;p align="justify"&gt;In spite of their huge potential for Digital Humanities, multiple aspects of their application still remain untapped. Unsupervised models such as Word2vec, for instance, are notoriously hard to evaluate directly – often researchers have to resort to indirect evaluations in this respect. This renders it intriguing to which extent the output of distributional models should play a decisive role in hermeneutical debates or controversies in the Humanities. With other techniques for Distant Reading, distributional models moreover share the drawback that they typically only yield a &lt;em&gt;single reading &lt;/em&gt;for a particular corpus so that for example the polysemy of a word isn’t rendered adequately. Interesting progress into representing the complex variability of meaning has been achieved, for example on the level of diachronic word embeddings, where convincing attempts have been made to allow for semantic shifts in an individual word’s meaning (Hamilton et al. 2016). Likewise, critical studies have revealed how tightly distributional models reproduce cultural biases with respect to gender and race (Bolukbasi et al. 2016), which calls for a debate about the ethical aspects of the matter. Likewise, it deserves emphasis how distributional models depend on large datasets and typically yield poor estimates for more restrictive data collections. This might help explain why word embeddings so far have not that many applications in fields like stylometry, that mostly work with relatively small corpora.&lt;/p&gt;
&lt;p align="justify"&gt;&lt;/p&gt;
&lt;p align="justify"&gt;The DARIAH working group on Text and Data Analytics (@dariahtdawg), in collaboration with the FWO-sponsored scientific community Digital Humanities Flanders (DHuF) proposes to collocate a one-day workshop with the 2018 DHd conference in Cologne. The workshop aims to bring together ca. 20 practitioners from the Digital Humanities to present and discuss recent advances in the field, through 30-minute presentations on focused case studies, including work-in-progress or theoretical contributions. Additionally, the workshop aims to reach an audience of non-presenting participants who take an active interest in distributional models and who are planning to apply distributional models to their own data in the near future. We aim to bring together a diverse group of both junior and senior stakeholders in this nascent subfield of DH. The goal of the workshop is to identify the state of the art in the field, identify common challenges and share recommendations for a best practice. Special attention will be given to the (both hermeneutic and quantitative) evaluation of distributional models in the context of Humanities research, which remains a challenging issue. The workshop is open to scholars from all backgrounds with an interest in semantic representation learning and encourages submissions that deal with under-researched resource-scarce and/or historic languages. Abstracts (between 250 and 300 words, not including references) can be submitted to mike.kestemont@uantwerp.be. The workshop also explicitly welcomes submissions presenting previously published research which is of interest to the DH community (although this work should not overlap strongly with work presented at the main conference).&lt;/p&gt;
&lt;p align="justify"&gt;&lt;/p&gt;
&lt;p align="justify"&gt;Topics which seem of special interest to the DH community nowadays include, but are not limited to:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p align="justify"&gt;the general use of distributional semantics in DH (such as topic modelling and word embeddings), but also more specific case studies, including work in progress;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p align="justify"&gt;the diachronic study of cultural phenomena via distributed methods;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p align="justify"&gt;the evaluation of distributional models, both from an empiric and hermeneutic perspective;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p align="justify"&gt;modelling the role and behaviour of (individual) readers or reading communities;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p align="justify"&gt;…&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p align="justify"&gt;&lt;/p&gt;
&lt;p align="justify"&gt;In terms of technical requirements, the workshop would need a beamer to project from a laptop.&lt;/p&gt;
&lt;p align="justify"&gt;&lt;/p&gt;
&lt;p align="justify"&gt;As keynote speaker, we have found David Bamman (University of California, Berkeley) willing to join our workshop and give a plenary lecture. Bamman is an authority in the field and will certainly increase the attractiveness of the workshop to potential participants.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>68</session_ID>
  <session_title>Kaffeepause</session_title>
  <session_start>2018-02-27 10:30</session_start>
  <session_end>2018-02-27 11:00</session_end>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>24</session_ID>
  <session_short>Workshop_10b</session_short>
  <session_title>Annotationen and beyond – Using ATHEN</session_title>
  <session_start>2018-02-27 11:00</session_start>
  <session_end>2018-02-27 12:30</session_end>
  <session_room_ID>9</session_room_ID>
  <session_room>S 12, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>23</session_ID>
  <session_short>Workshop_11b</session_short>
  <session_title>Research Software Engineering und Digital Humanities. Reflexion, Kartierung, Organisation.</session_title>
  <session_start>2018-02-27 11:00</session_start>
  <session_end>2018-02-27 12:30</session_end>
  <session_room_ID>6</session_room_ID>
  <session_room>Tagungsraum, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>138</session_ID>
  <session_short>Workshop_12b</session_short>
  <session_title>Nutzerunterstützung und neueste Entwicklungen in Forschungsdatenrepositorien für audiovisuelle (Sprach-)Daten</session_title>
  <session_start>2018-02-27 11:00</session_start>
  <session_end>2018-02-27 12:30</session_end>
  <session_room_ID>12</session_room_ID>
  <session_room>S 15, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>26</session_ID>
  <session_short>Workshop_8b</session_short>
  <session_title>Digitale Sammlungserschließung mit WissKI und CIDOC CRM</session_title>
  <session_start>2018-02-27 11:00</session_start>
  <session_end>2018-02-27 12:30</session_end>
  <session_room_ID>7</session_room_ID>
  <session_room>S 01, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>4</attendee_count>
 </session>

 <session>
  <session_ID>180</session_ID>
  <session_short>Vernetzungstreffen 5</session_short>
  <session_title>DIN Sprachressourcen</session_title>
  <session_start>2018-02-27 11:30</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>18</session_room_ID>
  <session_room>S 25, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>172</session_ID>
  <session_short>PK</session_short>
  <session_title>Abschlusstreffen des PK (auf Einladung)</session_title>
  <session_start>2018-02-27 12:00</session_start>
  <session_end>2018-02-27 14:00</session_end>
  <session_room_ID>16</session_room_ID>
  <session_room>S 23, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>75</session_ID>
  <session_title>Mittagspause</session_title>
  <session_start>2018-02-27 12:30</session_start>
  <session_end>2018-02-27 14:00</session_end>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>163</session_ID>
  <session_short>AG 2</session_short>
  <session_title>AG-Graphentechnologien Treffen</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 15:00</session_end>
  <session_room_ID>19</session_room_ID>
  <session_room>S 26, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>139</session_ID>
  <session_short>Workshop_13a</session_short>
  <session_title>Workshop_13a</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 15:30</session_end>
  <session_room_ID>12</session_room_ID>
  <session_room>S 15, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Audio Mining für die Geistes- und Kulturwissenschaften besitzt ein großes Potential für die Arbeit in den einzelnen Fachwissenschaften, stellt die Sprachtechnologie aber vor komplexe und interessante Heerausforderungen.

Ziel des Workshops ist es daher, den Stand der Technik, aktuelle Herausforderungen und Perspektiven für den zukünftigen Einsatz und Weiterentwicklung der Audio Mining Technologien darzustellen und zu diskutieren. Dafür wollen wir Forscherinnen und Forscher aus den Sprachtechnologien mit Forscherinnen und Forscher aus den Geistes- und Kulturwissenschaften zusammenbringen. Der Workshop bietet einen Rahmen um technische Forstschritte zu präsentieren, Beispiele der Anwendungen von Sprachtechnologien in der Forschung vorzustellen, Erfahrungen und Herausforderungen zu diskutieren sowie neue Nutzungsszenarien zu entwickeln. So werden konkrete Ergebnisse und Anwendungen aus dem BMBF Projekt KA³, die im »Kölner Zentrum Analyse und Archivierung von AV-Daten« künftig zur Verfügung stehen, vorgestellt und Anwendungsmöglichkeiten diskutiert, die für die Weiterentwicklung der Projektergebnisse Anregungen geben sollen. In dem Workshop soll der Austausch zwischen Forscherinnen und Forschern aus der Informatik und den Geisteswissenschaften gefördert werden.

Der Workshop richtet sich nicht nur an Forscherinnen und Forscher aus dem Bereich der Sprachtechnologie oder der Geistes- und Kulturwissenschaften, die mit Sprachtechnologien arbeiten, sondern auch und besonders an interessierte Geistes- und Kulturwissenschaftler und -wissenschaftlerinnen, deren Forschung durch den Einsatz von Sprachtechnologien profitieren könnte, sowie an Archivare und Dokumentare, die audiovisuelle Daten bereitstellen.</session_abstract>
  <attendee_count>5</attendee_count>
  <sessionID>139</sessionID>
  <presentations>1</presentations>
  <p1_paperID>152</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Köhler, Joachim
Leh, Almut
Himmelmann, Nikolaus
Rau, Felix</p1_authors>
  <p1_organisations>Fraunhofer IAIS
FernUniversität in Hagen
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p1_organisations>
  <p1_emails>joachim.koehler@iais.fraunhofer.de
almut.leh@fernuni-hagen.de
sprachwissenschaft@uni-koeln.de
f.rau@uni-koeln.de</p1_emails>
  <p1_presenting_author>Köhler, Joachim
Leh, Almut
Himmelmann, Nikolaus
Rau, Felix</p1_presenting_author>
  <p1_title>Audio Mining für die Geistes- und Kulturwissenschaften: Nutzungsszenarien und Herausforderungen</p1_title>
  <p1_abstract>&lt;p&gt;Audio Mining für die Geistes- und Kulturwissenschaften besitzt ein großes Potential für die Arbeit in den einzelnen Fachwissenschaften, stellt die Sprachtechnologie aber vor komplexe und interessante Heerausforderungen.&lt;/p&gt;
&lt;p&gt;Ziel des Workshops ist es daher, den Stand der Technik, aktuelle Herausforderungen und Perspektiven für den zukünftigen Einsatz und Weiterentwicklung der Audio Mining Technologien darzustellen und zu diskutieren. Dafür wollen wir Forscherinnen und Forscher aus den Sprachtechnologien mit Forscherinnen und Forscher aus den Geistes- und Kulturwissenschaften zusammenbringen. Der Workshop bietet einen Rahmen um technische Forstschritte zu präsentieren, Beispiele der Anwendungen von Sprachtechnologien in der Forschung vorzustellen, Erfahrungen und Herausforderungen zu diskutieren sowie neue Nutzungsszenarien zu entwickeln. So werden konkrete Ergebnisse und Anwendungen aus dem BMBF Projekt KA³, die im »Kölner Zentrum Analyse und Archivierung von AV-Daten« künftig zur Verfügung stehen, vorgestellt und Anwendungsmöglichkeiten diskutiert, die für die Weiterentwicklung der Projektergebnisse Anregungen geben sollen. In dem Workshop soll der Austausch zwischen Forscherinnen und Forschern aus der Informatik und den Geisteswissenschaften gefördert werden.&lt;/p&gt;
&lt;p&gt;Der Workshop richtet sich nicht nur an Forscherinnen und Forscher aus dem Bereich der Sprachtechnologie oder der Geistes- und Kulturwissenschaften, die mit Sprachtechnologien arbeiten, sondern auch und besonders an interessierte Geistes- und Kulturwissenschaftler und -wissenschaftlerinnen, deren Forschung durch den Einsatz von Sprachtechnologien profitieren könnte, sowie an Archivare und Dokumentare, die audiovisuelle Daten bereitstellen.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>141</session_ID>
  <session_short>Workshop_14a</session_short>
  <session_title>Workshop_14a</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 15:30</session_end>
  <session_room_ID>6</session_room_ID>
  <session_room>Tagungsraum, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Ausgehend von komprimierten Präsentationen (jeweils 10 Min.) sollen folgende editionstheoretischen und -technischen Fragen diskutiert 
werden: 1. Offenheit und institutionelle Schließung (Patrick Sahle), 2. Akteure und Rollen (Jochen Strobel), 3. Kommentierung - ein Auslaufmodell? (Anne Bohnenkamp), 4. Versionierung/Zitation (Joachim Veit), 5. Hemmnisse und Katalysatoren digitaler Brief-Infrastrukturen (Thomas Stäcker), 6. Schnelle Wege zu den Briefen (Stefan Dumont), 7. Diskussion fachlicher und förderpolitischer Schlussfolgerungen (Thomas Bürger u.a.).
</session_abstract>
  <attendee_count>4</attendee_count>
  <sessionID>141</sessionID>
  <presentations>1</presentations>
  <p1_paperID>236</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Strobel, Jochen
Bürger, Thomas</p1_authors>
  <p1_organisations>Philipps-Universität Marburg, Deutschland
Sächsische Landesbibliothek - Staats- und Universitätsbibliothek Dresden</p1_organisations>
  <p1_emails>strobel@staff.uni-marburg.de
buerger@slub-dresden.de</p1_emails>
  <p1_presenting_author>Strobel, Jochen
Bürger, Thomas</p1_presenting_author>
  <p1_title>Zur Zukunft der Digitalen Briefedition – kooperative Lösungen im kulturwissenschaftlichen Forschungsdatenmanagement </p1_title>
  <p1_abstract>&lt;p&gt;Ausgehend von komprimierten Präsentationen (jeweils 10 Min.) sollen folgende editionstheoretischen und -technischen Fragen diskutiert werden: 1. Offenheit und institutionelle Schließung (Patrick Sahle), 2. Akteure und Rollen (Jochen Strobel), 3. Kommentierung - ein Auslaufmodell? (Anne Bohnenkamp), 4. Versionierung/Zitation (Joachim Veit), 5. Hemmnisse und Katalysatoren digitaler Brief-Infrastrukturen (Thomas Stäcker), 6. Schnelle Wege zu den Briefen (Stefan Dumont), 7. Diskussion fachlicher und förderpolitischer Schlussfolgerungen (Thomas Bürger u.a.).&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>143</session_ID>
  <session_short>Workshop_15a</session_short>
  <session_title>Workshop_15a</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 15:30</session_end>
  <session_room_ID>10</session_room_ID>
  <session_room>S 13, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Die wachsende Vernetzung zwischen Datenrepositorien und der zunehmende Einsatz von Normdaten im Cultural Heritage-Bereich machen Wikidata zu einer vielversprechenden Infrastruktur. Kann Wikidata als Referenzinstrument und flexible Beschreibungsumgebung für Kulturgüter genutzt werden? Anhand von Beispielen aus der Praxis vermittelt der Workshop Möglichkeiten, aber auch Grenzen des Einsatzes von Wikidata in der geisteswissenschaftlichen Forschung.</session_abstract>
  <attendee_count>3</attendee_count>
  <sessionID>143</sessionID>
  <presentations>1</presentations>
  <p1_paperID>278</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Müller-Birn, Claudia
Schelbert, Georg
Raspe, Martin
Wübbena, Thorsten</p1_authors>
  <p1_organisations>Freie Universität Berlin, Deutschland
Humboldt-Universität zu Berlin
Max-Planck-Institut für Kunstgeschichte Rom
Goethe-Universität Frankfurt</p1_organisations>
  <p1_emails>clmb@inf.fu-berlin.de
georg.schelbert@hu-berlin.de
raspe@biblhertz.it
wuebbena@kunst.uni-frankfurt.de</p1_emails>
  <p1_presenting_author>Müller-Birn, Claudia
Schelbert, Georg
Raspe, Martin
Wübbena, Thorsten</p1_presenting_author>
  <p1_title>Wikidata: Nutzungsmöglichkeiten und Anwendungsbeispiele für den Bereich Digital Cultural Heritage</p1_title>
  <p1_abstract>&lt;p&gt;Die wachsende Vernetzung zwischen Datenrepositorien und der zunehmende Einsatz von Normdaten im Cultural Heritage-Bereich machen Wikidata zu einer vielversprechenden Infrastruktur. Kann Wikidata als Referenzinstrument und flexible Beschreibungsumgebung für Kulturgüter genutzt werden? Anhand von Beispielen aus der Praxis vermittelt der Workshop Möglichkeiten, aber auch Grenzen des Einsatzes von Wikidata in der geisteswissenschaftlichen Forschung.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>145</session_ID>
  <session_short>Workshop_16a</session_short>
  <session_title>Workshop_16a</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 15:30</session_end>
  <session_room_ID>7</session_room_ID>
  <session_room>S 01, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Unser Anliegen basiert auf einer Erfahrung, die vielen sicher allzu gut bekannt ist: Ausgangspunkt ist die Suche nach einer bestimmten Abbildung, einer historischen Quelle, wie zum Beispiel ein Foto eines Gebäudes. So begibt man sich auf die verschiedenen Online-Plattformen und sucht. Entweder werden gar keine Treffer ausgegeben, etwa, wenn das Gebäude keinen Eigennamen trägt und nicht übermäßig bekannt ist. Oder aber man erhält eine unüberschaubare Flut an Ergebnissen, die leider oftmals nicht relevant für das eigene Suchanliegen sind. So kann die gewünschte Arbeitserleichterung durch digitale Mittel leider fast schon hinderlich wirken.</session_abstract>
  <attendee_count>3</attendee_count>
  <sessionID>145</sessionID>
  <presentations>1</presentations>
  <p1_paperID>292</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Friedrichs, Kristina
Münster, Sander
Niebling, Florian
Maiwald, Ferdinand
Bruschke, Jonas
Barthel, Kristina</p1_authors>
  <p1_organisations>Universität Würzburg, Deutschland
TU Dresden, Deutschland
Universität Würzburg, Deutschland
TU Dresden, Deutschland
Universität Würzburg, Deutschland
TU Dresden, Deutschland</p1_organisations>
  <p1_emails>kristina.friedrichs@tu-dresden.de
sander.muenster@tu-dresden.de
florian.niebling@uni-wuerzburg.de
ferdinand.maiwald@tu-dresden.de
jonas.bruschke@uni-wuerzburg.de
kristina.barthel@tu-dresden.de</p1_emails>
  <p1_presenting_author>Friedrichs, Kristina
Münster, Sander
Niebling, Florian
Maiwald, Ferdinand
Bruschke, Jonas</p1_presenting_author>
  <p1_title>Digitale Bildrepositorien – wirkliche Arbeitserleichterung oder zeitraubend?</p1_title>
  <p1_abstract>&lt;p&gt;Unser Anliegen basiert auf einer Erfahrung, die vielen sicher allzu gut bekannt ist: Ausgangspunkt ist die Suche nach einer bestimmten Abbildung, einer historischen Quelle, wie zum Beispiel ein Foto eines Gebäudes. So begibt man sich auf die verschiedenen Online-Plattformen und sucht. Entweder werden gar keine Treffer ausgegeben, etwa, wenn das Gebäude keinen Eigennamen trägt und nicht übermäßig bekannt ist. Oder aber man erhält eine unüberschaubare Flut an Ergebnissen, die leider oftmals nicht relevant für das eigene Suchanliegen sind. So kann die gewünschte Arbeitserleichterung durch digitale Mittel leider fast schon hinderlich wirken.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>147</session_ID>
  <session_short>Workshop_17a</session_short>
  <session_title>Workshop_17a</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 15:30</session_end>
  <session_room_ID>9</session_room_ID>
  <session_room>S 12, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <session_abstract>Reisewege gut und genau zu kennen, ist für die Geschichtsforschung von enormer Bedeutung. Nicht nur, dass so persönliche Kontakte reflektiert werden können, so zeigen die Reisewege vielmehr auf, wie sich Güter oder Ideen verbreitet haben können. Weiterhin zeigen Reisewege auch auf, welches Ortswissen eine bestimmte Person verfügt hat. </session_abstract>
  <attendee_count>1</attendee_count>
  <sessionID>147</sessionID>
  <presentations>1</presentations>
  <p1_paperID>261</p1_paperID>
  <p1_contribution_type>Worshop</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Aschauer, Anna
Büchler, Marco
Gradl, Tobias
Henrich, Andreas</p1_authors>
  <p1_organisations>Institut für Europäische Geschichte, Deutschland
Institut für Europäische Geschichte, Deutschland
Otto-Friedrich-Universität Bamberg
Otto-Friedrich-Universität Bamberg</p1_organisations>
  <p1_emails>aschauer@ieg-mainz.de
buechler@ieg-mainz.de
tobias.gradl@uni-bamberg.de
andreas.henrich@uni-bamberg.de</p1_emails>
  <p1_presenting_author>Aschauer, Anna
Gradl, Tobias</p1_presenting_author>
  <p1_title>Reisewege in Raum und Zeit</p1_title>
  <p1_abstract>&lt;p&gt;Reisewege gut und genau zu kennen, ist für die Geschichtsforschung von enormer Bedeutung. Nicht nur, dass so persönliche Kontakte reflektiert werden können, so zeigen die Reisewege vielmehr auf, wie sich Güter oder Ideen verbreitet haben können. Weiterhin zeigen Reisewege auch auf, welches Ortswissen eine bestimmte Person verfügt hat. &lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>164</session_ID>
  <session_short>AG 3</session_short>
  <session_title>AG-Referenzcurriculum Treffen</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 17:00</session_end>
  <session_room_ID>14</session_room_ID>
  <session_room>S 21, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>162</session_ID>
  <session_short>AG 1</session_short>
  <session_title>AG-Datenzentren Treffen</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>17</session_room_ID>
  <session_room>S 24, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>165</session_ID>
  <session_short>AG 4</session_short>
  <session_title>AG-Digitales Publizieren Treffen</session_title>
  <session_start>2018-02-27 14:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>15</session_room_ID>
  <session_room>S 22, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>166</session_ID>
  <session_short>AG 5</session_short>
  <session_title>AG-Museum Treffen</session_title>
  <session_start>2018-02-27 15:00</session_start>
  <session_end>2018-02-27 17:00</session_end>
  <session_room_ID>16</session_room_ID>
  <session_room>S 23, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>69</session_ID>
  <session_title>Kaffeepause</session_title>
  <session_start>2018-02-27 15:30</session_start>
  <session_end>2018-02-27 16:00</session_end>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>169</session_ID>
  <session_short>Vernetzungstreffen 1</session_short>
  <session_title>Digitale Musikwissenschaft</session_title>
  <session_start>2018-02-27 16:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>19</session_room_ID>
  <session_room>S 26, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>140</session_ID>
  <session_short>Workshop_13b</session_short>
  <session_title>Audio Mining für die Geistes- und Kulturwissenschaften: Nutzungsszenarien und Herausforderungen</session_title>
  <session_start>2018-02-27 16:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>12</session_room_ID>
  <session_room>S 15, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>4</attendee_count>
 </session>

 <session>
  <session_ID>142</session_ID>
  <session_short>Workshop_14b</session_short>
  <session_title>Zur Zukunft der Digitalen Briefedition – kooperative Lösungen im kulturwissenschaftlichen Forschungsdatenmanagement</session_title>
  <session_start>2018-02-27 16:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>6</session_room_ID>
  <session_room>Tagungsraum, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>144</session_ID>
  <session_short>Workshop_15b</session_short>
  <session_title>Wikidata: Nutzungsmöglichkeiten und Anwendungsbeispiele für den Bereich Digital Cultural Heritage</session_title>
  <session_start>2018-02-27 16:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>10</session_room_ID>
  <session_room>S 13, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>146</session_ID>
  <session_short>Workshop_16b</session_short>
  <session_title>Digitale Bildrepositorien – wirkliche Arbeitserleichterung oder zeitraubend?</session_title>
  <session_start>2018-02-27 16:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>7</session_room_ID>
  <session_room>S 01, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>148</session_ID>
  <session_short>Workshop_17b</session_short>
  <session_title>Reisewege in Raum und Zeit</session_title>
  <session_start>2018-02-27 16:00</session_start>
  <session_end>2018-02-27 17:30</session_end>
  <session_room_ID>9</session_room_ID>
  <session_room>S 12, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>82</session_ID>
  <session_title>Eröffnung von DHd 2018 und Eröffnungs-Keynote - Prof. Dr. Dr. h. c. Sybille Krämer</session_title>
  <session_start>2018-02-27 17:45</session_start>
  <session_end>2018-02-27 19:15</session_end>
  <session_room_ID>1</session_room_ID>
  <session_room>Hörsaal B, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Speer, Andreas</chair1>
  <session_info>Der ‚Stachel des Digitalen‘ - Anreiz zur Selbstreflexion in den Geisteswissenschaften? Ein philosophischer Kommentar zu den Digital Humanities.</session_info>
  <attendee_count>8</attendee_count>
  <chair1_name>Andreas Speer</chair1_name>
  <chair1_organisation>Universität zu Köln</chair1_organisation>
  <chair1_email>andreas.speer@uni-koeln.de</chair1_email>
  <chair1_ID>2290</chair1_ID>
 </session>

 <session>
  <session_ID>84</session_ID>
  <session_title>Eröffnungsempfang mit Buffet</session_title>
  <session_start>2018-02-27 19:15</session_start>
  <session_end>2018-02-27 23:59</session_end>
  <session_room_ID>20</session_room_ID>
  <session_room>Hörsaalgebäude</session_room>
  <attendee_count>7</attendee_count>
 </session>

 <session>
  <session_ID>96</session_ID>
  <session_title>Öffnungszeiten Konferenzsekretariat</session_title>
  <session_start>2018-02-28 08:30</session_start>
  <session_end>2018-02-28 17:30</session_end>
  <session_room_ID>5</session_room_ID>
  <session_room>Hörsaal F, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>28</session_ID>
  <session_short>VP_1a</session_short>
  <session_title>Theorie der digitalen Geisteswissenschaften</session_title>
  <session_start>2018-02-28 09:00</session_start>
  <session_end>2018-02-28 10:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>König, Mareike</chair1>
  <attendee_count>6</attendee_count>
  <chair1_name>Mareike König</chair1_name>
  <chair1_organisation>Deutsches Historisches Institut Paris</chair1_organisation>
  <chair1_email>mkoenig@dhi-paris.fr</chair1_email>
  <chair1_ID>1631</chair1_ID>
  <sessionID>28</sessionID>
  <presentations>3</presentations>
  <p1_paperID>125</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Schilz, Andrea</p1_authors>
  <p1_organisations>Universität Passau, Deutschland</p1_organisations>
  <p1_emails>andrea.schilz@uni-passau.de</p1_emails>
  <p1_presenting_author>Schilz, Andrea</p1_presenting_author>
  <p1_title> &lt;em&gt;Exakt Historisch&lt;/em&gt; im Digitalen? Versuch einer Anleihe</p1_title>
  <p1_abstract>&lt;p&gt;„&lt;em&gt;Der Schlaf der Vernunft gebiert Monster“&lt;/em&gt;, wusste Francisco de Goya. Ein &lt;em&gt;Schlaf&lt;/em&gt; der Quellenkritik auch. Deshalb ist eine dem Digitalen angepasste, auf Daten erweiterte quellenkrititische Methodik üblich in den Digital Humanities. Ebenso ist im Diskurs ein reaktives Moment ablesbar, abzielend auf mangelnde Datenkritik und daraus resultierende problematische Aussagen. In Konsequenz heißt dies: Nur eine kontextuell orientierte Quellenkritik im Digitalen kann eine Basis bilden für kulturkritische Perspektiven. &lt;br /&gt;In diesem Zusammenhang steht das Ziel des Vortrags, der sich in zwei Blöcke gliedert: Einer Analyse von Quellenspezifika im Digitalen folgt, vergleichend und übertragend, die Skizze eines digital-quellenkritischen Leitfadens, der Kriterien der &lt;em&gt;exakt historischen Methode&lt;/em&gt; auf Born Digital spiegelt. Es wird methodisch Anleihe genommen an der volkskundlichen &lt;em&gt;Münchner Schule&lt;/em&gt;, die wiederum auf “Klassiker” der Quellenkritik zurückgreift. &lt;br /&gt;Für die dezidiert &lt;em&gt;historische&lt;/em&gt; Sichtweise, die hier eingeniommen wird, spricht, dass auch digitale Quellen historisch bedingt sind und ihre Deutung - im Sinne einer Ganzheitlichkeit - dem Rechnung tragen sollte. &lt;br /&gt;Es erscheint sinnvoll, eine Systematik anzuwenden, die hilft, Kontexte entsprechend zu identifizieren und transparent in hermeneutische Prozesse miteinzubeziehen. Das transdisziplinäre Experiment versteht sich so als „synkretistischer“ Versuch, eine tradierte Denkschule auf den Raum des Digitalen zu projizieren.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>146</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Raunig, Michael
Höfler, Elke</p2_authors>
  <p2_organisations>Universität Graz, Österreich
Universität Graz, Österreich</p2_organisations>
  <p2_emails>michael.raunig@uni-graz.at
elke.hoefler@uni-graz.at</p2_emails>
  <p2_presenting_author>Raunig, Michael</p2_presenting_author>
  <p2_title>Digitale Methoden sind weder digital noch innovativ</p2_title>
  <p2_abstract>&lt;p&gt;Der Beitrag argumentiert, dass der Einsatz digitaler Werkzeuge in methodologischer Hinsicht generell nicht relevant ist bzw. dass die Rede von "digitalen Methoden" einer argumentativen Grundlage entbehrt. Neben Begriffsklärungen und - präzisierungen werden unterschiedliche Thesen zum Zusammenhang von "Methodologie" und "Digitalität" im wissenschaftlichen Kontext formuliert.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>196</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Heßbrüggen-Walter, Stefan</p3_authors>
  <p3_organisations>National Research University Higher School of Economics, Russland</p3_organisations>
  <p3_emails>shessbru@hse.ru</p3_emails>
  <p3_presenting_author>Heßbrüggen-Walter, Stefan</p3_presenting_author>
  <p3_title> Die Angst vor dem „Elektronengehirn“: Topoi der Kybernetik-Kritik in der bundesdeutschen Nachkriegsphilosophie</p3_title>
  <p3_abstract>&lt;p lang="de-de" xml:lang="de-de"&gt;Mein Beitrag greift einen Diskurs auf, der meines Wissens selbst in der Wissenschaftsgeschichte der Kybernetik noch keine Beachtung gefunden hat und erst recht für die ‚Vorgeschichte‘ der &lt;em&gt;digital humanities&lt;/em&gt; im deutschsprachigen Raum noch nicht ausgewertet worden ist: die philosophische Kritik der Kybernetik in der Bundesrepublik der 60er Jahre. Ich beschränke mich dabei exemplarisch auf die Analyse dreier Aufsätze, die in der &lt;em&gt;Zeitschrift für philosophische Forschung&lt;/em&gt; zwischen den Jahren 1965 und 1970 veröffentlicht worden sind. Die Reihe eröffnete der Gründer und Herausgeber der Zeitschrift Georgi Schischkoff (Schischkoff 1965). Der zweite Text wurde vom in Karlsruhe lehrenden österreichischen Philosophen Simon Moser verfasst (Moser 1967). Sein Tübinger Kollege Walter Gölz äußerte sich abschließend (Gölz 1970). Mein Beitrag soll zur Genealogie heutiger Kritik digitaler Geisteswissenschaft beitragen.&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;In der Kybernetik wurden Regelungstheorie, Informationstheorie und Theorie der Nachrichtenverarbeitung zusammengeführt (Steinbuch 1963: 317). Sie sollte als „zukünftige Universalwissenschaft“ (Steinbuch 1963: 340) den „Weg zu einer neuen Einheit der Wissenschaften“ (Steinbuch 1963: 319) bahnen, indem sie Modelle und Erklärungsansätze der Technik auf die Erklärung von Lebewesen, insbesondere des Menschen, und Gesellschaften überträgt (Kline 2015: 11-12). Jedenfalls zielte sie auf den Abbau von Barrieren zwischen Disziplinen und Verbesserung des Austauschs zwischen ihnen beitragen (KLine 2015: 63). Der institutionelle Erfolg der Kybernetik trug jedoch zur Verwischung ihres Profils bei. Zugleich wuchs der Zweifel, ob ihre Versprechungen überhaupt einlösbar erschienen (Kline 2015: 179-181, Aumann 2015: 25).&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;Ähnlich wie die digitalen Geisteswissenschaften nutzte die Kybernetik also formalwissenschaftliche Werkzeuge zur Behandlung von Fragen, die auf den ersten Blick einer solchen Behandlung nicht zugänglich sind (Steinbuch 1963, 317). Hierzu zählten Analysen der Wahrnehmung ästhetischer Information (Steinbuch, 1963, 280) genauso wie der Plan einer formalen, aber qualitativen Informationstheorie (Steinbuch 1963, 315). Zu verweisen ist auch auf die Affinität zwischen Kybernetik und der Entwicklung strukturalistischer Linguistik, etwa bei Jakobson (Kline 2015, 41). „Das Eindringen der Kybernetik in die Geisteswissenschaft ist ein Markstein in der Geschichte der Wissenschaften.“ (Steinbuch 1963, 339) In den USA wurden Thesen der Kybernetik auch in der analytischen Philosophie und Wissenschaftstheorie rezipiert, nicht immer positiv (Kline 2015: 98-99).&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;Die Kybernetik-Kritik bundesdeutscher Philosophen fällt jedoch um einiges grundsätzlicher aus. Zunächst ist auf methodischer Ebene auf die Zuschreibung von ‚Kompetenz-Asymmetrie‘ hinzuweisen. Der Philosoph darf sich zur Kybernetik äußern, ohne über einschlägiges Fachwissen zu verfügen, denn dieses ist „hauptsächlich von technologischer Natur, und ihre Ausgangspunkte [sc. der Kybernetik] sowie speziell die anthropologischen Überlegungen und Analogien lassen sich im Rahmen allgemeiner philosophisch-methodologischer Interpretationen behandeln.“ (Schischkoff 1965: 251) Umgekehrt ist dem Kybernetiker die Beteiligung am philosophischen Diskurs zu verwehren, „weil ja die dürftigen philosophischen Voraussetzungen der Kybernetiker für eine Interpretation auf breiter Basis naturgemäß nicht ausreichen“ (Schischkoff 1965: 251). Ergänzend hinzu tritt die Argumentationsfigur der ‚Schuld durch Assoziierung‘. Durch Reduktion kybernetischer Thesen auf bekannte und im Diskurskontext als widerlegt geltende Positionen wird deren Haltlosigkeit offengelegt. So gilt der in der Kybernetik angeblich vorausgesetzte Physikalismus als „ein Beispiel höchster Steigerung des positivistischen Radikalismus“ (Schischkoff 1963: 252). 254-256). Der Materialismus ist falsch: die Psychologie setze Bewusstsein notwendig voraus und könne auf Introspektion nicht verzichten (Moser 1967, 65). Der Begriff des Wissens sei ebenfalls auf Bewusstsein verwiesen und deswegen kybernetischer Analyse prinzipiell unzugänglich (Gölz 1970, 256).&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;Auf der Sachebene kreisen die Argumente der Kybernetik-Kritik, sofern sie hier einschlägig sind, in der Hauptsache um drei Themen: den Begriff der Information, das Sprachverstehen und die Rolle der Formalwissenschaften Logik und Mathematik. Shannons Informationstheorie verzichtet, darin auch innerhalb der Kybernetik nicht unumstritten, auf jede Einbeziehung der Bedeutung sprachlicher Ausdrücke (Kline 2015: 15). Demgegenüber beharrt die Kybernetik-Kritik auf der Subjektgebundenheit des Begriffs: „Information über etwas gibt es nur von einem bewußten Wesen an ein anderes.“ (Moser 1965: 66) Sie ist „objektivierter Geist“, der immer nur „in bezug auf den Menschen Sinn und Bedeutung hat“ (Gölz 1970, 257). Entsprechend erfordern „geistig fundierte Texte“ – im Gegensatz zu bloßen inhaltlichen Mitteilungen – die Erfassung durch einen lebendigen Adressaten (Schischkoff 1965, 259). Eigentliches Lehren bedürfe der „lebendigen Sprache des Lehrers“,die „einen eigenen bildenden Wert hat“ (Schischkoff 1965, 267). Die Überschätzung der formalen Wissenschaften führe schließlich zu einem übersteigerten Rationalismus: die Kybernetik übersehe, dass sich die „innere geistig unerschöpfliche Sphäre des eigentlichen menschlichen Seins“ nicht in „rational erfaßbare[n] Strukturen“ abbilden lasse (Schischkoff 1965: 262). Die Mathematisierung der Kybernetik erzeuge „die Gefahr einer formallogischen und formal mathematischen Verdünnung des System- und Modellbegriffes gegenüber den materialkonkreten Bedürfnissen der Physik, Physiologie und Technik“ (Moser 1967: 67). Oder nach Gölz: „Die Fähigkeit der Maschinen, solche – im weitesten Sinne! – mechanischen Denkprozesse zu bewältigen, bestätigt aber nicht den ‚Geist‘ der Maschinen, sondern den mechanischen Charakter gewisser formaler Denkprozesse.“ (Gölz 1970: 259)&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;Ein solcher ‚seelenloser Materialismus‘ ist nicht nur abstrakt und theoretisch, sondern auch praktisch und politisch gefährlich. Der kybernetische Materialismus stehe im Bunde mit dem „östlichen“,also historischen oder dialektischen, Materialismus (Schischkoff 1965: 256). Besondere Aufmerksamkeit erhielt hier die wissenschaftspolitische Dimension. Die Entwicklung der Kybernetik wäre ohne politische Förderung und Patronage nicht möglich gewesen (Kline 2015: 99). Häufig diente der Begriff ‚Kybernetik‘ als Schlagwort,&lt;strong&gt; „&lt;/strong&gt;um Entscheidungsträgern modernes und zukunftszugewandtes Denken zu demonstrieren.“ (Aumann 2015: 32). Dies blieb auch ihren philosophischen Kritikern nicht verborgen. Ein großes Risiko bildet hierbei nach Schischkoff die technische Anwendbarkeit der Kybernetik: „Datenverarbeitungsanlagen, die auch als ‚Elektronengehirne‘ bezeichnet werden, lernende Automaten und Rechenmaschinen sind dafür weitbekannte Beispiele.“ (Schischkoff 1965: 250) Philosophen hingegen dürften wohl kaum über Patente verfügen (Schischkoff 1965: 250). Es folge vermutlich die „rasche Errichtung von Lehrstellen für Kybernetik, für die sich die finanziellen Mittel viel leichter finden, als etwa für die Errichtung neuer geisteswissenschaftlicher und philosophischer Lehrstühle.“ (Schischkoff 1965: 250) Dies gefährde die „bisherige Vordergrundstellung der klassischen Disziplinen des Geistes zumindest hinsichtlich deren praktischer Förderung“ (Schischkoff 1965: 250). Kybernetik schicke sich an, „an Stelle der Geisteswissenschaften und Philosophie treten zu können“.Dies werde zum „Aussterben der geistigen Elite“ führen (Schischkoff 1965: 266). Einzig die Philosophie erscheint einstweilen vor dem Zugriff eines solchen Imperialismus gefeit: „Die Forschungsarbeit tiefschürfenden philosophischen Denkens kann ihrem Wesen nach zum Glück nicht zu einem Spezialistentum der Modelltechnik führen, so daß also ein direkter Verrat des eigenen Faches undenkbar erscheint.“ (Schischkoff 1965: 275).&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;Die hier referierten methodischen Einwände erscheinen aus heutiger Perspektive sophistisch: eine durchgreifende Kritik digitaler Vernunft ist ohne vertiefte Kenntnis des kritisierten Sachgebiets kaum denkbar. So wäre auch der Schuldzuschreibung durch Assoziierung vorzubeugen: zu behandeln sind die konkreten Erzeugnisse digitaler Forschung, nicht deren vorgeblicher Zusammenhang mit angeblich haltlosen Lehrgebäuden. Bedenkenswert erscheint hingegen weiterhin die Frage, in welchem Ausmaß digitale Forschung in den Geisteswissenschaften die lebensweltliche Verankerung verwendeter Begriffe in Frage stellen darf, wie dies die kybernetische Informationstheorie vorschlug. Umgekehrt muss sich manche Kritik der digitalen Geisteswissenschaften vielleicht die Rückfrage gefallen lassen, inwiefern sie ähnlich wie ihre Vorläufer einem überkommenen Elite-Verständnis anhängt, das die Gabe zu geisteswissenschaftlicher Forschung in der ‚geistig unerschöpflichen Sphäre‘ besonders begabter Individuen verortet.&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;Auf disziplinpolitischer Ebene erlaubt die hier vorgeschlagene Rekonstruktion ebenfalls einige Schlussfolgerungen. So wie sich der Aufstieg der Kybernetik konkretem politischem Willen verdankte, war auch ihr Abstieg nicht zuletzt der immer größer werdenden Diskrepanz zwischen Anspruch und Wirklichkeit und dem damit einhergehenden Entzug politischen Wohlgefallens geschuldet. Wollen die digitalen Geisteswissenschaften diesem Schicksal entgehen, sollte die Sorge aber nicht allein ihrer materiellen und politischen Basis gelten, sondern auch ihrem theoretischen Überbau. Eine vorurteilsfreie Bestimmung der Grenzen digitaler Vernunft ist hierfür sicherlich ein erster Schritt, eine von gedanklicher Offenheit geprägte Auseinandersetzung über das Verhältnis digitaler und außerdigitaler Forschungspraxen vielleicht der zweite. An beidem hat es in der Auseinandersetzung über die Rolle der Kybernetik sicherlich gemangelt. Es wäre an uns, zumindest diese Fehler nicht zu wiederholen.&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;Bibliographie&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;&lt;strong&gt;Aumann, Philipp&lt;/strong&gt; (2015): „Neues Denken in Wissenschaft und Gesellschaft: Die Kybernetik in der Mitte des 20. Jahrhunderts“ in: Jeschke, Sabina / Dröge, Alicia / Schmitt, Robert (eds.): &lt;em&gt;Exploring Cybernetics: Kybernetik im interdisziplinären Diskurs&lt;/em&gt;, Wiesbaden: Springer Fachmedien 21-40&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;&lt;strong&gt;Gölz, Walter&lt;/strong&gt; (1970): „Philosophisches Problembewußtsein und kybernetische Theorie“,in: &lt;em&gt;Zeitschrift für philosophische Forschung&lt;/em&gt; 24: 253-264&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;&lt;strong&gt;Kline, Ronald R&lt;/strong&gt; (2015): &lt;em&gt;The Cybernetics Moment. Or Why we Call Our Age the Information Age&lt;/em&gt;. Baltimore: Johns Hopkins University Press&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;&lt;strong&gt;Moser, Simon&lt;/strong&gt; (1967): „Zur philosophischen Diskussion der Kybernetik in der Gegenwart“,in: &lt;em&gt;Zeitschrift für philosophische Forschung&lt;/em&gt; 21: 64-77&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;&lt;strong&gt;Schischkoff, Georgi&lt;/strong&gt; (1965): „Philosophie und Kybernetik. Zur Kritik am kybernetischen Positivismus“,in: &lt;em&gt;Zeitschrift für philosophische Forschung&lt;/em&gt; 19: 248-278&lt;/p&gt;
&lt;p lang="de-de" xml:lang="de-de"&gt;&lt;strong&gt;Steinbuch, Karl&lt;/strong&gt; (1963): &lt;em&gt;Automat und Mensch: Kybernetische Tatsachen und Hypothesen&lt;/em&gt;. Berlin / Göttingen / Heidelberg: Springer Verlag OHG&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>29</session_ID>
  <session_short>VP_1b</session_short>
  <session_title>Textmining I</session_title>
  <session_start>2018-02-28 09:00</session_start>
  <session_end>2018-02-28 10:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Henrich, Andreas</chair1>
  <attendee_count>2</attendee_count>
  <chair1_name>Andreas Henrich</chair1_name>
  <chair1_organisation>Otto-Friedrich-Universität Bamberg</chair1_organisation>
  <chair1_email>andreas.henrich@uni-bamberg.de</chair1_email>
  <chair1_ID>1207</chair1_ID>
  <sessionID>29</sessionID>
  <presentations>3</presentations>
  <p1_paperID>147</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Hellrich, Johannes
Stöger, Alexander
Hahn, Udo</p1_authors>
  <p1_organisations>Graduiertenkolleg „Modell Romantik“, Friedrich-Schiller-Universität Jena, Jena, Deutschland; Jena University Language &amp; Information Engineering Lab (JULIE Lab), Friedrich-Schiller-Universität Jena, Jena, Deutschland
Graduiertenkolleg „Modell Romantik“, Friedrich-Schiller-Universität Jena, Jena, Deutschland
Jena University Language &amp; Information Engineering Lab (JULIE Lab), Friedrich-Schiller-Universität Jena, Jena, Deutschland</p1_organisations>
  <p1_emails>johannes.hellrich@uni-jena.de
alexander.stoeger@uni-jena.de
udo.hahn@uni-jena.de</p1_emails>
  <p1_presenting_author>Hellrich, Johannes
Stöger, Alexander</p1_presenting_author>
  <p1_title> Wenn der Funke überspringt – Word Embeddings im Dienst der Wissenschaftsgeschichte</p1_title>
  <p1_abstract>&lt;p&gt;Das moderne Verständnis von Elektrizität fußt auf wissenschaftlichen Entdeckungen des 17. und 18. Jahrhunderts und den darauf aufbauenden Entwicklungen, die im 19. Jahrhundert zur Nutzbarmachung der Elektrizität im großen Stil führten. Unsere Studie wendet state-of-the-art Methoden der distributionellen diachronen Semantik an, um die frühen Vorstellungen dieses anfangs unbekannten, fremden Phänomens und ihre Eingliederung in die Naturwissenschaften anhand des damit einhergehenden Wandels der Semantik von &lt;em&gt;Elektrizität&lt;/em&gt; und &lt;em&gt;electricity&lt;/em&gt; nachzuvollziehen. Im Fokus liegt die Entwicklung von einer als schwer begreifliche Naturerscheinung wahrgenommenen Entität zu einem durch wissenschaftliche Experimente gezielt untersuchten Kraft, welche im 19. Jahrhundert soweit erschlossen wurde, dass sie als Energiequelle genutzt und bis heute zu einem unverzichtbaren Teil des menschlichen Alltags wurde. Dabei sind wir zwar von anderen Studien zur Entwicklung wissenschaftlicher Konzepte (Hall u.a. 2008; Mimno 2012; Fankhauer u.a. 2016; Schumann 2016) inspiriert, bewegen uns, anders als diese, aber technisch nicht auf einer Konzeptebene (also Aggregaten von Wörtern, erzeugbar durch Topic Modeling (Blei u.a. 2003)), sondern auf der Wortebene. Dies ermöglicht es uns, genauere Aussagen zur semantischen Entwicklung einzelner Wörter zu machen, was im Anfangsstadium des Themenfeldes Elektrizität sinnvoll ist, da das sich entwickelnde Verständnis von Elektrizität als Konzept noch stark fluktuiert. Darüber hinaus vermeiden wir somit Probleme bei der Reproduktion von Studien, die sich aus der stochastischen Natur von Topic Modeling ergeben.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>156</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Andresen, Melanie</p2_authors>
  <p2_organisations>Universität Hamburg, Deutschland</p2_organisations>
  <p2_emails>Melanie.Andresen@uni-hamburg.de</p2_emails>
  <p2_presenting_author>Andresen, Melanie</p2_presenting_author>
  <p2_title>Sprachliche Variation in der Germanistik: eine n-Gramm-basierte Stilanalyse</p2_title>
  <p2_abstract>&lt;p&gt;Einleitung&lt;/p&gt;
&lt;p&gt;An zahlreichen Universitäten werden die wissenschaftlichen Disziplinen Linguistik und Literaturwissenschaft in einem gemeinsamen Studiengang angeboten, der beispielsweise „Germanistik“ oder „Deutsche Sprache und Literatur“ heißt. Dies suggeriert eine große fachliche Nähe dieser Disziplinen, die jedoch im Selbstverständnis der meisten Wissenschaftler/innen dieser Fächer keine Entsprechung hat. Linguistik und Literaturwissenschaft unterscheiden sich in ihrem Erkenntnisinteresse, ihren Methoden und auch in ihrer Sprache, wie punktuell bereits beschrieben wurde: So stellt Haggan (2004) bei der Untersuchung von Titeln wissenschaftlicher Publikationen in Linguistik, Literatur- und Naturwissenschaft fest, dass die Sprache der Literaturwissenschaft sich (auch) an ästhetischen Prinzipien orientiert. Afros und Schryer (2009) kommen zu einem ähnlichen Ergebnis bezüglich der Verwendung von „promotional metadiscourse“ und attestieren sogar verschwimmende Grenzen mit den literarischen Texten selbst (S. 305). Die Studierenden von Studiengängen wie „Germanistik“ finden sich also mit (mindestens) zwei unterschiedlichen Fachkulturen und Sprachen konfrontiert, deren Erwerb überwiegend auf dem Weg der Imitation erfolgt (Graefen 1999). Dieser Beitrag hat das Ziel, die stilistischen Unterschiede zwischen den beiden Fächern mithilfe einer n-Gramm-Analyse zu beschreiben und damit bei Lehrenden und Studierenden zu einem höheren Bewusstsein für die damit verbundenen Herausforderungen beizutragen.&lt;/p&gt;
&lt;p&gt;Methode&lt;/p&gt;
&lt;p&gt;Die hier verwendete Methode ist eine n-Gramm-Analyse, die (fast) rein datengeleitet funktioniert und keine spezifischen Hypothesen erfordert. Ein n-Gramm ist eine Sequenz aus &lt;em&gt;n&lt;/em&gt; Elementen, im einfachsten Fall aus Wörtern. N-Gramm-basierte Verfahren sind insbesondere in der Computerlinguistik verbreitet, wenn es um einfach zu berechnende Modellierungen von Sprache geht (Jurafsky und Martin 2009). Auch für die linguistische Interpretation wurden n-Gramme bereits genutzt: Scharloth u. a. (2012) zeigen bei der Analyse von Tonbandprotokollen zweier 68-Kommunen, dass auf diese Weise charakteristische Muster identifiziert werden können, die mit außersprachlichen Merkmalen der beiden Gruppen in Verbindung gebracht werden können. Mahlberg (2013) nutzt ein ähnliches Vorgehen zur Charakterisierung der Prosa Charles Dickens’, Biber u. a. (2004) beschreiben unterschiedliche Formen der Wissenschaftssprache anhand sog. lexical bundles. Mit Ausnahme von Scharloth u. a. (2012) wird in diesen Ansätzen nur die Tokenebene einbezogen. Im Rahmen des hier präsentierten Vorhabens sollen die Potentiale zusätzlicher syntaktischer Informationen ermittelt werden.&lt;/p&gt;
&lt;p&gt;Die Datengrundlage der folgenden Analyse ist ein Korpus aus 60 deutschen Dissertationen (30 pro Fach, ca. 3,5 Mio. Token) aus dem Zeitraum von 2003 bis 2016, die an 15 unterschiedlichen deutschen Universitäten eingereicht wurden und über universitäre Server online zur Verfügung stehen. Im Rahmen der Datenaufbereitung wurden semiautomatisch Textelemente ausgeschlossen, die nicht zur Zielvarietät gehören (Zitate), nicht aus Fließtext bestehen (Tabellen, Abbildungen,...) oder den Textfluss unterbrechen (Fußnoten). Die Texte wurden außerdem automatisch mit Informationen zu Lemma, Wortart und syntaktischen Dependenzstrukturen annotiert.[1]&lt;/p&gt;
&lt;p&gt;Aus diesen Daten wurden n-Gramme der Größe &lt;em&gt;n&lt;/em&gt; = 1 bis 5 generiert, die die Token bzw. die Wortartentags nutzen. Neben traditionellen, linearen n-Gramme, die der Reihenfolge der Wörter an der Textoberfläche folgen, wurden zusätzlich syntaktische n-Gramme generiert, die der Dependenzstruktur im Satz folgen (beschrieben von Sidorov u. a. 2012, Goldberg und Orwant 2013, siehe Abbildung 1). Die Frequenzen aller n-Gramme mit mindestens 10 Vorkommen (rund 500.000) wurde signifikanzbasiert mit dem t-Test verglichen (siehe Empfehlungen in Lijffijt u. a. 2014, Paquot und Bestgen 2009). Die Auswertung bezieht sich auf die n-Gramme mit den größten Unterschieden zwischen den linguistischen und literaturwissenschaftlichen Texten.&lt;/p&gt;
&lt;p&gt;Abbildung 1: Lineare und syntaktische n-Gramme im Vergleich an einem Beispielsatz (vgl. ANONYM 2017)&lt;/p&gt;
&lt;p&gt;Ergebnisse&lt;/p&gt;
&lt;p&gt;Exemplarisch werden hier die Ergebnisse zu den Wortarten-Unigrammen sowie den linearen und syntaktischen Token-Trigrammen präsentiert. Abbildung 2 zeigt die 20 Wortarten[2] mit den größten Unterschieden zwischen den beiden Disziplinen. Der mit Abstand größte Unterschied zeigt sich in den attributiv gebrauchten Possessivpronomen (PPOSAT, z. B. &lt;em&gt;seine Existenz &lt;/em&gt;(Lit_Stu_30[3])). Verwandt hiermit sind Unterschiede in den Reflexivpronomen (PRF) und Personalpronomen (PPER). Zusammen mit einer ebenfalls deutlich höheren Frequenz von Eigennamen (NE) spiegelt sich hier, dass sich literaturwissenschaftliche Texte in weitaus höherem Maße als die Linguistik mit Personen beschäftigen, seien es reale Autor/inn/en oder literarische Figuren, zum Beispiel:&lt;/p&gt;
&lt;p&gt;(1) So bildet ihr autobiographisches Werk eine Brücke zwischen Tradition und Moderne. (Lit_Stu_30)&lt;/p&gt;
&lt;p&gt;Weitere Unterschiede zeigen sich im Zusammenhang mit der Verbverwendung: Bei den finiten Verben der literaturwissenschaftlichen Texte handelt es sich eher um Vollverben (VVFIN), während finite Modal- und Auxiliarverben[4] (VMFIN, VAFIN) in der Linguistik frequenter sind. Korrespondierend dazu sind Auxiliarverben im Infinitiv (insb. &lt;em&gt;werden&lt;/em&gt; in Passivkonstruktionen mit Modalverb) und Vollverben in ihrer Partizipform (VVPP) in der Linguistik häufiger. Lediglich die Form des Passiv Perfekts ist in der Literaturwissenschaft häufiger, wie das Tag VAPP zeigt. Insgesamt lässt sich sagen, dass die Sprache der Linguistik im Vergleich mit der Literaturwissenschaft durch komplexe Verbkonstruktionen gekennzeichnet ist. &lt;/p&gt;
&lt;p&gt;In der Linguistik zeigt sich außerdem eine höhere Frequenz von attribuierenden Indefinitpronomen (PIAT). Darunter sind die Lemmta &lt;em&gt;kein&lt;/em&gt;, &lt;em&gt;aller&lt;/em&gt; und &lt;em&gt;beide&lt;/em&gt; am häufigsten. Dies kann damit in Verbindung gebracht werden, dass die Linguistik in stärkerem Maße auf Generalisierungen abzielt. Die höhere Frequenz von Zahlen (CARD) in der Linguistik überrascht nicht, da quantitative Verfahren hier deutlich häufiger zum Einsatz kommen als in der Literaturwissenschaft. Das Tag PRELS (Relativpronomen) weist auf eine häufigere Verwendung von Relativsätzen in der Literaturwissenschaft hin.&lt;/p&gt;
&lt;p&gt;Abbildung 2: Die distinktivsten Wortarten, visualisiert anhand der Teststatistik t. Visualisierung inspiriert durch das R-Paket &lt;em&gt;stylo&lt;/em&gt; (Eder u. a. 2015)&lt;/p&gt;
&lt;p&gt;Ergänzend werden in Tabelle 1 Informationen auf Wortebene herangezogen, zunächst ohne zusätzliche syntaktische Information. Gezeigt werden die 10 distinktivsten linearen Trigramme (inkl. Interpunktion). Hier spiegeln sich viele der Phänomene, die bereits auf Ebene der Wortarten erkennbar waren. Die für die Literaturwissenschaft charakteristischen Muster scheinen mehrheitlich aus Relativsätzen zu stammen. Dabei ist zu bedenken, dass der Anfang von Nebensätzen besonders leicht durch eine n-Gramm-Analyse erfasst werden kann, da hier nur ein begrenztes Maß an Variation möglich ist. Das klarste Trigramm für die Linguistik hingegen weist auf die bereits beschriebene häufigere Verwendung von Passiv und Modalverben hin, hier speziell in Kombination miteinander. Interessant ist das n-Gramm&lt;em&gt; die bei der&lt;/em&gt;, das in 36 von 43 Fällen aus einem Relativsatz stammt, aber in der Linguistik häufiger ist. Mit dem Relativpronomen &lt;em&gt;die&lt;/em&gt; kann es sich auf Feminina beziehen, in der Mehrzahl handelt es sich im Korpus aber um Substantive im Plural. Das passt zu der bereits oben genannten Annahme, dass die Literaturwissenschaft sich tendenziell exemplarisch mit konkreten Einzelphänomenen beschäftigt, die Linguistik hingegen in stärkerem Maße Generalisierungen anstrebt, die den Plural wahrscheinlich machen.&lt;/p&gt;
&lt;p&gt;Tabelle 1: Die distinktivsten linearen Token-Trigramme&lt;/p&gt;
&lt;p&gt;Tabelle 2 zeigt die häufigsten syntaktischen Trigramme, die zusätzlich Informationen zur Dependenzstruktur im Satz nutzen. „&gt;“ zeigt hier ein syntaktisches Dominanzverhältnis an. Die Relativsatzmuster sind hier nicht vorhanden, da ihre gute Erkennbarkeit in der Analyse vermutlich primär auf der linearen Abfolge von Interpunktion, Relativpronomen und folgendem Wort beruht. Das Muster &lt;em&gt;können&gt;werden&gt;.&lt;/em&gt; ist höher gerankt als das Gegenstück in der linearen Analyse, da hier nicht nur unmittelbar aufeinanderfolgende Instanzen erfasst werden, sondern auch solche mit Distanzstellung:&lt;/p&gt;
&lt;p&gt;(2) Einige Substantive können nicht eindeutig einer Geschlechtskategorie zugeordnet werden [...]. (Lin_Bam_01)&lt;/p&gt;
&lt;p&gt;Zusätzlich tauchen Kombinationen von Passiv mit dem Modalverb &lt;em&gt;müssen&lt;/em&gt; und &lt;em&gt;können&lt;/em&gt; mit &lt;em&gt;sein &lt;/em&gt;auf. Viele der für die Literaturwissenschaft charakteristischen Muster habe eine direkte lineare Entsprechung: So steht das syntaktische n-Gramm &lt;em&gt;für&gt;Leben&gt;das&lt;/em&gt; für die lineare Abfolge &lt;em&gt;für das Leben&lt;/em&gt;. Allerdings umfasst das syntaktische n-Gramm zusätzlich Instanzen, in denen beispielsweise das Substantiv noch durch Attribute modifiziert wird, z. B. &lt;em&gt;für&lt;/em&gt; &lt;em&gt;das eigene Leben &lt;/em&gt;(Lit_Jen_19).&lt;/p&gt;
&lt;p&gt;Tabelle 2: Die distinktivsten syntaktischen Token-Trigramme&lt;/p&gt;
&lt;p&gt;Fazit&lt;/p&gt;
&lt;p&gt;In der hier präsentierten Analyse konnten deutliche stilistische Unterschiede zwischen Linguistik und Literaturwissenschaft gezeigt werden. Die Linguistik zeichnet sich demzufolge durch komplexe Verben (Passiv und Modalverben), die stärkere Verwendung von Zahlen sowie Mustern der Generalisierung aus. In der Literaturwissenschaft finden sich mehr Bezüge auf Personen und komplexe Nominalphrasen mit Relativsätzen.&lt;/p&gt;
&lt;p&gt;Die verwendete Methode hat stark explorativen Charakter, sodass viele der hier angebotenen Interpretationen zunächst als Hypothesen betrachtet werden sollten und einer sorgfältigen Prüfung in Folgestudien bedürfen. Zusätzlich ergibt sich mit der Methode eine Beschränkung auf Phänomene, die sich auf konstante Weise auf der sprachlichen Oberfläche niederschlagen.&lt;/p&gt;
&lt;p&gt;Im Rahmen dieses Beitrags wurden nur besonders hoch gerankten n-Gramme betrachtet und interpretiert. ANONYM (2017) präsentiert ergänzend ein Annotationsexperiment, in dessen Rahmen insgesamt 420 Token- und Wortarten-n-Gramme auf die enthaltenen linguistischen Informationen hin ausgewertet wurden. In Folgearbeiten gilt es das Potential unterschiedlicher n-Gramm-Typen für eine Stilanalyse zu erforschen. Der Fokus wird dabei auf stärker syntaktisch informierten Formen liegen, die beispielsweise Informationen zu Token und Wortart kombinieren oder die syntaktischen Dependenzrelationen zwischen den Elementen einbeziehen.&lt;/p&gt;

&lt;p&gt;[1] mit einem auf der Dependenzversion des TIGER-Korpus (Seeker und Kuhn 2012) trainierten Modell für MATE (Bohnet 2010)&lt;/p&gt;
&lt;p&gt;[2] Die Wortartentags stammen aus dem STTS (Schiller u. a. 1999).&lt;/p&gt;
&lt;p&gt;[3] Die Bezeichnung der Korpustexte setzt sich aus einem Kürzel für die Disziplin, die Universität und einer fortlaufenden Zahl zusammen.&lt;/p&gt;
&lt;p&gt;[4] Bei den Verben &lt;em&gt;haben&lt;/em&gt;, &lt;em&gt;sein&lt;/em&gt; und &lt;em&gt;werden&lt;/em&gt; wird nicht zwischen einer Verwendung als Auxiliar- oder Vollverb unterschieden.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>230</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Salgaro, Massimo
Rebora, Simone
Lauer, Gerhard
Herrmann, J. Berenike</p3_authors>
  <p3_organisations>University of Verona, Italy
University of Verona, Italy
University of Basel, Switzerland
University of Basel, Switzerland</p3_organisations>
  <p3_emails>massimo.salgaro@univr.it
simone.rebora@univr.it
gerhard.lauer@unibas.ch
berenike.herrmann@unibas.ch</p3_emails>
  <p3_presenting_author>Rebora, Simone</p3_presenting_author>
  <p3_title>The &lt;em&gt;Tiroler Soldaten-Zeitung&lt;/em&gt; and its Authors. A Computer-Aided Search for Robert Musil</p3_title>
  <p3_abstract>&lt;p&gt;Robert Musil, one of the most important authors of the twentieth-century German literature, fought in the Austrian army at the Italian front. During the First World War, between 1916 and 1917, Musil was chief editor of the &lt;em&gt;Tiroler Soldaten-Zeitung&lt;/em&gt; in Bozen. This activity has always been a philological problem for Musil scholars, who have not been able to attribute with certainty a range of texts to the presentationAuthor. However, their identification is fundamental in the study of his political thinking. With this paper, we present a new approach, that combines historical and philological research with stylometric methods.&lt;/p&gt;
&lt;p&gt;The starting point for the determination of possible authorship is the screening of previous attempts. The number of articles attributed to Musil has so far varied extensively:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Attribution proposed by - Number of TSZ  articles attributed to Musil&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;(Dinklage 1960) - 3&lt;/p&gt;
&lt;p&gt;(Roth 1972) - 19&lt;/p&gt;
&lt;p&gt;(Corino 1973, 2003, and 2010) - 8&lt;/p&gt;
&lt;p&gt;(Arntzen 1980) - 22&lt;/p&gt;
&lt;p&gt;(Fontanari / Libardi 1987) - 36&lt;/p&gt;
&lt;p&gt;(Amann &lt;em&gt;et al. &lt;/em&gt; 2009) - 36&lt;/p&gt;
&lt;p&gt;We have limited our test set to the 38 TSZ articles listed by (Schaunig 2014), for which Musil’s authorship has been proposed at least once. The major problem for carrying out a stylometric analysis on this corpus is text length. As demonstrated by recent research, the minimum length for a reliable authorship attribution is around 5,000 words (see Eder 2015). However, the average length of the 38 disputed TSZ articles is slightly below 1,000 words (see Figure 1). As a possible solution for this issue, we decided to develop a combinatory design that analyzes longer chunks composed by the juxtaposition of single texts. To reduce the number of combinations, we excluded the nine shortest texts (below 500 word), together with the only text attributed to Musil on solid philological ground (see Corino 1973). Combining 6 texts out of a total of 28, we obtained 376,740 text chunks with an average length of N=6,963 words and a standard deviation of n=909 words.&lt;/p&gt;
&lt;p&gt;As for the composition of the training set, we drew both on the “impostors method” (see Koppel / Winter 2014) and on historiographical research. Following (Juola 2015), we fixed the number of “impostors” to a minimum of three: Franz Blei, Franz Kafka, and Stefan Zweig. Subsequently, we selected three authors suggested by (Urbaner 2001) as possible TSZ collaborators: Marie delle Grazie, Hugo Salus, and Albert Ritter (his texts were not available in digitized format, so we OCRed and manually refined them). The training set was then completed by a selection of articles published by Musil in various journals between 1911 and 1919. For each presentationAuthor, the retrieved material was subdivided in three text chunks with a length comprised between 6,000 and 8,000 words: the training set was thus composed by 21 text chunks (see Figure 2).&lt;/p&gt;
&lt;p&gt;The analysis was carried out using the R package &lt;em&gt;Stylo&lt;/em&gt; (see Eder / Rybicki / Kestemont 2016). For each iteration, the distances between test set and training set were saved in the tabular form provided by the package. At the end of the process, mean values were calculated. Notwithstanding the employment of a high-standard computational power (provided by GWDG, University of Göttingen), a first experiment using 50–500 most frequent words (MFW) and Eder’s Delta distance took more than one week to be completed. However, when repeating the experiment with only one-tenth of the combinations (i.e. 37,674 iterations), results were rather identical (see Figure 3) and the process took less than one day. When the experiment was repeated without any combination, results were extremely noisier (see Figure 4), thus confirming that the combinatory design was able to better discern authorial signals.&lt;/p&gt;
&lt;p&gt;To validate the results, the experiment has been repeated with 16 different configurations, by combining Eder’s Delta, Burrow’s Delta, Canberra, and Cosine distances with 10–100, 20–200, 50–500, and 100–1,000 MFW. In all configurations, Ritter and Musil are the only authors disputing the authorship of the TSZ articles. This evidence has been corroborated by the discovery of a document in the &lt;em&gt;Kriegsarchiv &lt;/em&gt;in Wien, which confirms that Albert Ritter was part of the TSZ editorial team (see Figure 5).&lt;/p&gt;
&lt;p&gt;Final results have been synthetized here:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;TSZ articles’ titles and dates of publication - Agreement between classifiers on Musil’s authorship&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1. „Kameraden arbeitet mit!“ (6. 8. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;2. „Bin ich ein Österreicher?“ (20. 8. 1916) - 87,50%&lt;/p&gt;
&lt;p&gt;3. „Herr Tüchtig und Herr Wichtig” (27. 8. 1916) - 81,25%&lt;/p&gt;
&lt;p&gt;4. „Das Schlagwort” (27. 8. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;5. „Die Erziehung zum Staat” (3. 9. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;6. „Bauernleben” (1. 10. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;7. „Sonderbare Patrioten” (15. 10. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;8. „Noch einmal Bauernleben” (29. 10. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;9. „Opportunität” (12. 11. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;10. „Eine gute persönliche Beziehung” (26. 11. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;11. „Eine österreichische Kultur” (10. 12. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;12. „Der Nörgler und der neue Österreicher” (17. 12. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;13. „Das Kompromiß” (24. 12. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;14. „Heilige Zeit” (31. 12. 1916) - 100,00%&lt;/p&gt;
&lt;p&gt;15. „Zentralismus und Föderalismus” (7. 1. 1917) - 68,75%&lt;/p&gt;
&lt;p&gt;16. „Föderalismus oder Zentralismus” (14. 1. 1917) - 68,75%&lt;/p&gt;
&lt;p&gt;17. „Zu Milde und zu Wilde” (11. 2. 1917) - 93,75%&lt;/p&gt;
&lt;p&gt;18. „Neu-Altösterreichisches” (25. 2. 1917) - 87,50%&lt;/p&gt;
&lt;p&gt;19. „Ist die »österreichische Frage« schwierig?” (4. 3. 1917) - 62,50%&lt;/p&gt;
&lt;p&gt;20. „Seiner Hochwohlgeboren!” (4. 3. 1917) - 100,00%&lt;/p&gt;
&lt;p&gt;21. „Luxussteuern” (4. 3. 1917) - 93,75%&lt;/p&gt;
&lt;p&gt;22. „Positive Ziele” (11. 3. 1917) - 81,25%&lt;/p&gt;
&lt;p&gt;23. „Der Frieden versprochen!” (18. 3. 1917) - 68,75%&lt;/p&gt;
&lt;p&gt;24. „Das Staatsprogramm der Deutschen” (18. 3. 1917) - 87,50%&lt;/p&gt;
&lt;p&gt;25. „Wehe dem Staatsmann!” (25. 3. 1917) - 68,75%&lt;/p&gt;
&lt;p&gt;26. „Der Frieden und die Zukunft” (1. 4. 1917) - 62,50%&lt;/p&gt;
&lt;p&gt;27. „Presse und Krieg” (8. 4. 1917) - 68,75%&lt;/p&gt;
&lt;p&gt;28. „Vermächtnis” (15. 4. 1917) - 100,00%&lt;/p&gt;
&lt;p&gt;A general trend is evident: while, for the articles published in 1916, Musil’s authorship is almost unexceptionable, many more doubts emerge with the articles published in 1917. In no case, however, Ritter’s signal becomes dominant. Notwithstanding the high margins of uncertainty, these results are to be considered as significant for multiple reasons. First, the combinatory design, while having shown the dominance of Musil’s signal throughout the test set, may have overshadowed different, minor signals. Second, it should be considered the fact that Musil, in the role of chief editor, may have altered many articles in the journal, thus intermixing his authorial signal with those of others. All this considered, further research is advisable, while the focus should be shifted towards the texts on which classifiers disagree.&lt;/p&gt;
&lt;p&gt;Among possible future developments of the research, is the definition of new training sets to validate the results and an expansion of the test set. Both these developments, however, will require an extensive digitization effort: most of the useful texts, in fact, are not available in a clean plain-text format. In addition, other software should be tested on the already defined corpus, from JGAAP (see Juola &lt;em&gt;et al. &lt;/em&gt;2008) to the CLEF/PAN software (see Stamatatos &lt;em&gt;et al.&lt;/em&gt; 2014), focusing specifically on different methods for authorship attribution, from lower-level features such as character n-grams (see Halvani &lt;em&gt;et al. &lt;/em&gt;2016), to higher-level features such as syntactic labels (see Hirst / Feiguina 2007), taking into consideration also machine-learning techniques (see Jockers / Witten 2010). With our study, we hope to have cast the groundwork for a research that can have long-lasting consequences on the history of German literature, confirming at the same time how quantitative methods are not in opposition, but complementary to qualitative analysis (see Herrmann 2017).&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>30</session_ID>
  <session_short>VP_1c</session_short>
  <session_title>Sammlungsdigitalisierung I</session_title>
  <session_start>2018-02-28 09:00</session_start>
  <session_end>2018-02-28 10:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Münzmay, Andreas</chair1>
  <attendee_count>7</attendee_count>
  <chair1_name>Andreas Münzmay</chair1_name>
  <chair1_organisation>Universität Paderborn</chair1_organisation>
  <chair1_email>andreas.muenzmay@uni-paderborn.de</chair1_email>
  <chair1_ID>1847</chair1_ID>
  <sessionID>30</sessionID>
  <presentations>3</presentations>
  <p1_paperID>215</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Busch, Hannah
Bös, Eva</p1_authors>
  <p1_organisations>Center for Digital Humanities, Universität Trier, Deutschland
Buchbinderei Mohr, Trier, Deutschland</p1_organisations>
  <p1_emails>buschh@uni-trier.de
ef_fa@web.de</p1_emails>
  <p1_presenting_author>Busch, Hannah
Bös, Eva</p1_presenting_author>
  <p1_title>Auf der Suche nach der verlorenen Materialität. </p1_title>
  <p1_abstract>&lt;p&gt;Auf der Suche nach der verlorenen Materialität. &lt;br /&gt;Kodikologie und Restaurierungswissenschaften im Zeitalter der (Massen-) Digitalisierung.&lt;/p&gt;
&lt;p&gt;Die Materialitäten von Handschriften (darüber hinaus natürlich auch Druckwerken) und ihrer digitalen Abbilder bieten den Ausgangspunkt unseres Vortrags, der zwei Disziplinen in einen Dialog bringt, die sich mit der Materialität handgeschriebener Artefakte beschäftigen und an der Digitalisierung beteiligt sind: die digitale Kodikologie und die Restaurierungwissenschaft. Die handwerkliche Arbeit der Buchrestauratoren ist für Digitalisierungsprojekte unerlässlich und gehört zur Vor- und Nachbereitung: iIn der Regel geht jeder Kodex zunächst durch ihre Hände, da unter Umständen der Zustand des Objekts bewertet, Festigungsmaßnahmen oder andere Eingriffe vorgenommen werden müssen, um die Digitalisierung physisch zu ermöglichen. Restauratoren sind aber mehr als bloße Dienstleister, sondern verfügen über einen speziellen Blick auf die Materialität von Originalen, der zusätzliche Informationen für digital basierte kodikologische Untersuchungen bereitstellen kann. Umgekehrt verfügen die Digital Humanities über Methoden, die für Anliegen der Restaurierung nützlich sein und gemeinsam weiterentwickelt werden könnten.&lt;/p&gt;
&lt;p&gt;Nur durch interdisziplinäre Ansätze, so die These, kann das Potenzial des Digitalisats  erkannt und ausgeschöpft werden. Gleichzeitig führt die Synergie zur Kompetenzerweiterung der beteiligten Disziplinen.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>218</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Goedel, Martina
Zimmer, Sebastian
Schmidt, Johannes</p2_authors>
  <p2_organisations>Cologne Center for eHumanities
Cologne Center for eHumanities
Niklas Luhmann-Archiv, Fakultät für Soziologie, Universität Bielefeld</p2_organisations>
  <p2_emails>mgoedel@uni-koeln.de
sebastian.zimmer@uni-koeln.de
johannes.schmidt@uni-bielefeld.de</p2_emails>
  <p2_presenting_author>Goedel, Martina
Zimmer, Sebastian
Schmidt, Johannes</p2_presenting_author>
  <p2_title>Digitale Differenz. Luhmanns Zettelkasten als physisch-historisches Objekt und als vernetzter Navigationsraum</p2_title>
  <p2_abstract>&lt;p dir="ltr"&gt;Niklas Luhmann (1927-1998) zählt zu den bedeutendsten Soziologen des 20. Jahrhunderts. Im Laufe seiner 35-jährigen Forschungstätigkeit entwickelte er eine universale Sozial- und Gesellschaftstheorie, die er in annähernd fünfzig Monographien und 500 Aufsätzen publiziert hat. Als Basis für diese erstaunliche Produktivität diente Luhmann ein Zettelkasten, den er über vierzig Jahre lang systematisch gefüllt und gepflegt hat. Im Zuge der seit 2015 laufenden Nachlasserschließung wurden die ca. 90.0000 Zettel des aus zwei Sammlungen bestehenden Kastens zunächst digitalisiert, in einem zweiten Schritt werden sie nun transkribiert und fachwissenschaftlich editiert sowie in eine eigens dafür entwickelte Internetpräsentation überführt. Ziel dieses Prozesses ist eine digitale Reproduktion des Zettelkastens, die zugleich die Möglichkeiten der modernen digitalen Technik nutzt, um die Sammlung lesbar und ihre Genese nachvollziehbar zu machen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Der Vortrag thematisiert mögliche Rückwirkungen der Projektarbeit auf die fachwissenschaftliche Arbeit: Wie hat sich aus fachwissenschaftlicher Sicht der Blick auf den Zettelkasten seit Antragstellung des Digitalisierungsprojektes verändert? Hat das Projekt Konsequenzen für die soziologische Forschung zu Niklas Luhmanns Theorie? &lt;/p&gt;
&lt;p dir="ltr"&gt;Dazu werden zunächst die Prinzipien der Anlage des Zettelkastens erläutert und anschließend die Modellierung der Daten in TEI und deren weitere Verarbeitung vorgestellt, sowie das Forschungsportal und seine Visualisierungen präsentiert. Abschließend wird kritisch gefragt, ob die Digitalisierung des Zettelkastens diesen nur reproduziert oder ob nicht die digitale Reproduktion letztlich einen anderen Zettelkasten erzeugt.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p2_abstract>
  <p3_paperID>239</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Wagner, Sarah
Scholz, Martin
Andraschke, Udo</p3_authors>
  <p3_organisations>Germanisches Nationalmuseum, Deutschland
Friedrich-Alexander Universität Erlangen-Nürnberg, Deutschland
Friedrich-Alexander Universität Erlangen-Nürnberg, Deutschland</p3_organisations>
  <p3_emails>s.wagner@gnm.de
martin.scholz@fau.de
udo.andraschke@fau.de</p3_emails>
  <p3_presenting_author>Wagner, Sarah
Scholz, Martin</p3_presenting_author>
  <p3_title>Objekte im Netz – Die Digitalisierung der Sammlungen der Universität Erlangen-Nürnberg als Gegenstand und Methode.</p3_title>
  <p3_abstract>&lt;p&gt; Die Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) entwickelt in Zusammenarbeit mit dem Germanischen Nationalmuseum Nürnberg (GNM) eine gemeinsame Dokumentations- und Digitalisierungsstrategie für die Sammlungen der FAU, um ihre Sicht- und Nutzbarkeit zu erhöhen und sie als bedeutende und noch immer zu wenig genutzte Infrastrukturen für Forschung und Lehre auszubauen. Digitalisierung kommt dabei nicht nur als Methode und praktische Anwendung zum Einsatz, sondern wird ebenso als kritisch zu befragender Gegenstand untersucht. Der Beitrag stellt Hintergrund, Ziele und bisherige Ergebnisse der Zusammenarbeit vor und skizziert Vorgehen und Methodik.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Universitäre Sammlungen als (digitale) Forschungsinfrastrukturen   &lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Universitäre Sammlungen erleben in den letzten Jahren eine beachtliche Renaissance. Allein in Deutschland existieren rund 1.000 solcher Sammlungen an über 80 Universitäten, die eine Vielzahl an Dingen und Disziplinen umfassen. Das wissenschaftliche Potenzial universitärer Sammlungen ist enorm, weshalb ihr Ausbau zu Forschungsinfrastrukturen in einer Empfehlung des Wissenschaftsrats von 2011 dringend empfohlen wurde. Allerdings sind längst nicht alle Sammlungen und ihre Objekte erfasst, nur gut ein Drittel ist digital zugänglich. Die systematische Erfassung und Erschließung wissenschaftlicher Sammlungen sind jedoch grundlegende Voraussetzungen, um sie möglichst effektiv in Forschung und Lehre einsetzen zu können. Die Vorteile einer digitalen Dokumentation sind dabei inzwischen unbestritten. Der Relevanz universitärer Sammlungen für die Forschung und dem Bedarf nach ihrer Nutz- und Verfügbarkeit stehen nicht nur unzureichende monetäre Mittel und fehlendes Personal entgegen, sondern auch häufig das Fehlen von angemessenen Software-Lösungen und Know-How für eine flächendeckende Digitalisierung und Online-Präsenz. Die Situation der digitalen Dokumentation universitärer Sammlungen hat sich in den vergangenen Jahren zwar erheblich verbessert, die Fortschritte sind allerdings meist auf einzelne Sammlungen begrenzt. Übergreifende Strukturen wurden hingegen kaum entwickelt und haben sich auch nicht ergeben. Zahlreiche Sammlungen wurden ad hoc inventarisiert, jedoch ohne hinreichend standardisierte Erfassung oder weiterführende Digitalisierungsstrategie. Das vom Bundesministerium für Bildung und Forschung geförderte Projekt „Objekte im Netz” setzt hier an und entwickelt im Verbund mit dem Germanischen Nationalmuseum eine gemeinsame Digitalisierungsstrategie für die Sammlungen der FAU.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objekte im Netz - Die Sammlungen der Universität Erlangen-Nürnberg als Testlandschaft&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Die FAU besitzt über 20 Sammlungen aus den verschiedensten Fachbereichen. Ihre Vielfalt spiegelt sich nicht nur in ihren Objekten, Entstehungskontexten oder Funktionen wider, sondern auch im überaus divergenten Stand ihrer Erfassung sowie in den verschiedenartigen Verfahren ihrer Dokumentation. Sie stellen somit eine ideale Testlandschaft für die exemplarische Entwicklung von fächer- und sammlungsübergreifenden Konzepten zur digitalen Erfassung und Erschließung dar, wie sie der Wissenschaftsrat in seinen Empfehlungen gefordert hat.&lt;/p&gt;
&lt;p dir="ltr"&gt;Das Forschungsprojekt “Objekte im Netz” zielt auf die Entwicklung einer digitalen Infrastruktur, die langfristig eine gesicherte Erfassung und Vernetzung der Bestände der FAU erlaubt sowie deren weitere Sicht- und Nutzbarkeit befördert. Von zentraler Bedeutung sind dabei gemeinsame Erfassungsstandards und -formate sowie eine gemeinsame Software-Lösung und Webpräsenz. Als Ergebnis des Vorhabens werden weiterhin ein allgemeines Konzept zur digitalen Dokumentation sowie eine dazu passende Software zur Verfügung gestellt. Damit dient das Projekt längst nicht nur der infrastrukturellen Verbesserung und Dynamisierung der hiesigen Bestände, sondern bietet weit darüber hinaus auch anderen wissenschaftlichen Sammlungen Werkzeuge und Workflows an, mit deren Hilfe sich heterogene Bestände erfassen, erforschen und vernetzen lassen. Nicht zuletzt strengt das Projekt einen kritischen Dialog über die Herausforderungen, Hindernisse und Folgen der Digitalisierung wissenschaftlicher Sammlungen an und trägt damit zu einem notwendigen Diskurs bei, der bislang nur wenig entwickelt ist und kaum in die digitale Sammlungspraxis einfließt.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vorgehen und Vernetzung&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Das Projekt „Objekte im Netz“ versteht sich als gemeinschaftliches Vorhaben sämtlicher Projektbeteiligter und wird daher im überaus engen Austausch vorangetrieben. Für die Entwicklung eines standardisierten Erfassungsschemas für die universitären Sammlungen wurden insgesamt sechs Teilbestände der FAU ausgewählt, die mit ihren heterogenen Beständen und ihrem unterschiedlichen Stand der Erschließung, Digitalisierung sowie den dazu eingesetzten Methoden und Werkzeugen die Bandbreite universitärer Sammlungen repräsentieren sollen: Die Graphische Sammlung, die Medizinische Sammlung, die Geowissenschaftliche Sammlung, die Schulgeschichtliche Sammlung, die Ur- und Frühgeschichtliche Sammlung sowie die Studiensammlung Musikinstrumente und Medien der Universität Würzburg mit den Beständen des ehemaligen musikwissenschaftlichen Seminars der FAU. Ausgehend von und anhand dieser Auswahl wurde ein Metadatenschema unter Berücksichtigung bestehender Dokumentationsstandards entwickelt, das mit Blick auf die übrigen Sammlungen grundlegende Aspekte, aber auch sammlungsspezifische Eigenheiten zu berücksichtigen hat.&lt;/p&gt;
&lt;p dir="ltr"&gt;Vom Ziel einer gemeinsamen digitalen Erfassung, Datenspeicherung und Präsentation leiten sich auch die zentralen Forschungsfragen nach der Tragfähigkeit und Umsetzbarkeit eines gemeinsamen Datenmodells sowie einer abgestimmten Terminologie zur Abbildung der in ihrer Materialität, Funktion und Provenienz heterogenen Sammlungsbestände ab. Dabei werden die Erkenntnisse ausgehend von den oben erwähnten Sammlungen konsequent auf ihre Generizität hin geprüft, um die Anwendbarkeit auf weitere universitäre Sammlungen hin zu gewährleisten.&lt;/p&gt;
&lt;p dir="ltr"&gt;Als technische Grundlage dient die virtuelle Forschungsumgebung „WissKI“ (Wissenschaftliche KommunikationsInfrastruktur), die im Hinblick auf die spezifischen Anforderungen und Eigenarten universitärer Sammlungen – insbesondere in Bezug auf die semantische Wissensrepräsentation (Görz 2011, Hohmann 2011, Hohmann/Schiemann 2014) – anzupassen und auszubauen ist. Ihr Einsatz erlaubt die Vernetzung unterschiedlichster Bestände und komplexer Bestandsinformationen sowie weiterer digitaler Ressourcen, aus der sich neue Forschungsfragen und erhebliche Erkenntnispotentiale ergeben können.&lt;/p&gt;
&lt;p dir="ltr"&gt;WissKI erweitert die Idee und Konzepte des Wiki zu einer webbasierten virtuellen Forschungsumgebung, die insbesondere auf die Belange und Besonderheiten der Forschung und Dokumentation von kulturellem Erbe ausgerichtet ist. Das System setzt auf offene Datenformate und Standards, die den Langzeiterhalt der verwalteten Datenbestände sichern. Dazu werden die Technologie des Linked Open Data und des Semantic Web benutzt. Eine Schlüsselrolle kommt hierbei dem dem Conceptual Reference Model (CRM) des ICOM-CIDOC als formaler Referenzontologie zu, gleichermaßen um fach- und sammlungsspezifische Anwendungsontologien erweitert werden kann.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die notwendige Weiterentwicklung von WissKI hinsichtlich der Spezifika universitärer Sammlungsbestände sowie den Anforderungen einer objektbezogenen Forschung und Lehre erfolgt dabei weniger technik- als sammlungsgetrieben. Dem entspricht das durchaus aufwendige Vorgehen im Projekt: die avisierte digitale Infrastruktur wird von Sammlungsbeauftragten und Informatikern gemeinsam aufgebaut. Auf diese Weise finden die Belange der einzelnen Sammlungen unmittelbar Eingang in die Entwicklung der Software und benötigten Funktionalitäten. Den Sammlungen kann somit ein digitales Werkzeug zur Verfügung gestellt werden, das nicht bloß die Erfassung ihrer Bestände fördert, sondern darüber hinaus spezifische forschungs- und lehrrelevante Anwendungen ermöglicht sowie betriebliche Abläufe des Sammlungsalltags unterstützt.&lt;/p&gt;
&lt;p dir="ltr"&gt;Das gewählte Vorgehen setzt allerdings eine hohe Bereitschaft der beteiligten Sammlungsbeauftragten voraus, sich eingehend mit oft fachfremden Werkzeugen und Techniken auseinanderzusetzen, im Gegenzug festigt es das Verständnis für die eingesetzten Technologien und Verfahren. Nicht zuletzt bauen die Sammlungen über die beteiligten Mitarbeiterinnen und Mitarbeiter eigene Kompetenzen im Bereich der digitalen Dokumentation auf. Die Herausforderungen der ontologischen Modellierung sowie die fächer- und sammlungsübergreifende Anlage des Projekts führen dabei zu einer vertieften Reflexion über die eigenen Bestände und Sammlungslogiken, aber auch zur Einsicht in die notwendige Standardisierung von Begrifflichkeiten, Werkzeugen und Workflows, wie sie eine übergreifende Digitalisierungs- und Dokumentationsstrategie unbedingt zu berücksichtigen hat.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ansätze zu einer reflexiven Digitalisierung&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Die neuen Möglichkeiten der Informationstechnologien und die allerorten wachsenden digitalen Dingarchive verändern die Arbeit, Forschung und Lehre an und mit den Objektbeständen. Das Thema Digitalisierung wird im Rahmen des Projektes deshalb nicht alleine aus Sicht methodischer und technischer Aspekte der digitalen Dokumentation behandelt, sondern im Sinne einer reflexiven Digitalisierung auch hinsichtlich der Herausforderungen und Folgen des Aufbaus digitaler Infrastrukturen sowie des Einsatzes digitaler Mittel und Methoden. In welchem Verhältnis stehen analoge und digitale Bestände, Original und Digitalisat? Welchen technischen, rechtlichen und nicht zuletzt epistemologischen Problemen hat sich die Anwendung digitaler Methoden in der Sammlungspraxis zu stellen und welche Herausforderungen muss sie bewältigen? Welche Kompetenzen erfordert sie und welchen Wandel erfahren Sammlungspraxis und sammlungsbezogene Forschung dadurch? Inwiefern verändern also digitale Sammlungen die kustodiale und wissenschaftliche Arbeit an und mit den Beständen? Zur Beantwortung solcher Fragen arbeitet das Projekt eng mit dem Interdisziplinären Zentrum für digitale Geistes- und Sozialwissenschaften der FAU (IZdigital) zusammen und bietet darüber hinaus ein fächer- und sammlungsübergreifendes Lehrangebot im Bereich der Digital Humanities und der Museologie an. Die Ergebnisse und Erkenntnisse der innerhalb des Projekts geführten Diskurse finden wiederum Eingang in die Sammlungspraxis und digitale Dokumentation der Bestände. Der Einsatz digitaler Werkzeuge und Praktiken erscheint aus dieser Sicht gleichermaßen als zu untersuchender Gegenstand und angewandte Methode. Für die Entwicklung einer an der Arbeit mit und an wissenschaftlichen Sammlungen orientierte Digitalisierungsstrategie gilt es beide Perspektiven angemessen zu berücksichtigen und möglichst zusammenzuführen.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>128</session_ID>
  <session_short>VP_1d</session_short>
  <session_title>Visualisierung I</session_title>
  <session_start>2018-02-28 09:00</session_start>
  <session_end>2018-02-28 10:30</session_end>
  <session_room_ID>1</session_room_ID>
  <session_room>Hörsaal B, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Windhager, Florian</chair1>
  <attendee_count>6</attendee_count>
  <chair1_name>Florian Windhager</chair1_name>
  <chair1_organisation>Donau-Universität Krems</chair1_organisation>
  <chair1_email>florian.windhager@donau-uni.ac.at</chair1_email>
  <chair1_ID>1065</chair1_ID>
  <sessionID>128</sessionID>
  <presentations>3</presentations>
  <p1_paperID>197</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Kleymann, Rabea
Meister, Jan Christoph
Stange, Jan-Erik</p1_authors>
  <p1_organisations>Universität Hamburg, Deutschland
Universität Hamburg, Deutschland
Universität Hamburg, Deutschland</p1_organisations>
  <p1_emails>Rabea.Kleymann@uni-hamburg.de
jan-c-meister@uni-hamburg.de
jan-erik.stange@uni-hamburg.de</p1_emails>
  <p1_presenting_author>Kleymann, Rabea
Stange, Jan-Erik</p1_presenting_author>
  <p1_title>Perspektiven kritischer Interfaces für die Digital Humanities im 3DH-Projekt</p1_title>
  <p1_abstract>&lt;p align="center"&gt; &lt;strong&gt; &lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I.          Visualisierung als Reflexionsmoment der Textinterpretation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Eine Haupttätigkeit der literaturwissenschaftlichen Arbeit ist die Interpretation von literarischen Texten. Unter „Interpretation“ wird „ein Sprechen oder Schreiben über Texte, in dem ihnen auf methodische und argumentierende Weise Bedeutungen zugeschrieben werden“ (Albrecht et. al. 2015: 1) verstanden.&lt;sup&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/sup&gt; Mit der Art der Bedeutungszuschreibung und -produktion sowie der damit verbundenen Frage nach einem Textverstehen setzt sich insbesondere die Hermeneutik als „Kunstlehre des Verstehens“ (Metzler 2008: 281) auseinander.&lt;sup&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/sup&gt; In &lt;em&gt;Wahrheit und Methode &lt;/em&gt;(1960) schreibt Hans-Georg Gadamer: „[D]en Text selbst zu verstehen […] bedeutet, […] sich wahrhaft anzueignen, was in dem Texte gesagt ist“ (Gadamer 1990: 392). Eine ‚wahrhafte‘ Aneignung des Textes im hermeneutischen Interpretationsprozess scheint mit dem Einzug digitaler Methoden in die literaturwissenschaftliche Praxis erneut zur Disposition zu stehen (vgl. Rockwell/Sinclair 2016). Eine Rolle in der Gestaltung von digitalen Zugängen zur Textinterpretation spielt unter anderem die Visualisierung von literaturwissenschaftlichen Verfahren sowie ihrer Resultate, d.h. ihren Daten.&lt;/p&gt;
&lt;p&gt;Wie solche digitalen Visualisierungen und Interfaces&lt;sup&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/sup&gt; an den Bedürfnissen interpretierender Literaturwissenschaftler orientiert sein sollten, um eine hermeneutische Textinterpretation sinnvoll zu unterstützen, ist Teil des Forschungsanliegens des Projekts 3DH –&lt;em&gt;Dreidimensionale dynamische Datenvisualisierung und Exploration für Digital Humanities-Forschungen&lt;/em&gt; der Universität Hamburg.&lt;sup&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Im Beitrag möchten wir erste Ergebnisse der praktischen Entwicklung geisteswissenschaftlicher Interfaces für hermeneutische Interpretationsprozesse vorstellen und diskutieren. Wir argumentieren, dass wir durch die Einbindung von Methoden aus dem nutzerzentrierten Design in den Interpretationsprozess über geeignete Werkzeuge verfügen, um die Bedürfnisse hermeneutisch arbeitender Geisteswissenschaftlerinnen und Geisteswissenschaftler zu identifizieren und an diese Bedürfnisse angepasste Interfacelösungen zu entwickeln.&lt;/p&gt;
&lt;p&gt;Wir stellen zunächst die aus unserer Sicht notwendigen normativen Anforderungen an ein reflektiertes Visualisierungskonzept mit Bezug zur Interpretationspraxis vor (II), um dann beispielhaft darzulegen, welche Strategie wir in der Gestaltung und Entwicklung solcher Interfaces verfolgen (III). Im Anschluss sollen erste Ergebnisse und Schlussfolgerungen aus dem Prozess aufgezeigt werden (IV).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;II.         Postulate eines Visualisierungskonzeptes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Eine kritische Bestandsaufnahme aktueller Visualisierungstools und -metaphern und deren Anwendung in den Geisteswissenschaften (vgl. Bradley 2008; Drucker 2011, 2014; Gibbs/Owens 2012) führte im 3DH-Projekt bereits zur Formulierung von vier konzeptionellen Postulaten.&lt;sup&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/sup&gt; Im Anschluss an Grinsteins (2012) „grand challenge“ der visuellen Expansion verstehen wir unter Postulaten notwendige Prinzipien, die einer Visualisierung zugrunde gelegt werden müssen, sofern diese interpretatorische Prozesse sinnvoll unterstützen soll. Es handelt sich um die Postulate&lt;/p&gt;
&lt;p&gt;1. des „Two-Way-Screen“: Der Bildschirm soll vom Renderer zum Two-Way-Screen werden.&lt;/p&gt;
&lt;p&gt;2. der „Parallaxe“: Die Stärke der visuellen Multiperspektivität soll mit der epistemischen Tiefendimension einhergehen.&lt;/p&gt;
&lt;p&gt;3. der „Qualität“: Datenvisualisierungen in und für die Geisteswissenschaften sollen nicht als Repräsentationsmittel von Daten fungieren, sondern Möglichkeiten der interaktiven Datenmanipulation bereithalten.&lt;/p&gt;
&lt;p&gt;4. der „Diskursivität“: Die Visualisierung soll funktionaler Bestandteil des Interpretationsprozesses sein.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Die normativen Anforderungen an ein reflektiertes Visualisierungskonzept rekurrieren auf die spezifischen Eigenschaften der hermeneutischen Textinterpretation. Diese folgt keinem finiten Set von methodischen Regeln oder Verfahrensweisen, wie sie beispielsweise in den Natur- und Sozialwissenschaften verwendet werden. Vielmehr tritt die Textinterpretation als komplexer epistemisch und ästhetisch geformter sowie zirkulär organisierter Prozess auf, der sich als in hohem Maße subjektiv und kontextsensitiv erweist (vgl. Winko 2015: 486).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Für die praktische Entwicklung geisteswissenschaftlicher Interfaces gehen wir von zwei Annahmen aus.&lt;/p&gt;
&lt;p&gt;Die Komplexität des Interpretationsprozesses kann erstens auf eine Reihe von basalen „routineförmige[n] Tätigkeiten“ (Albrecht et al. 2015: 2) bzw. „scholarly primitives“ (Unsworth 2000) zurückgeführt werden, die iterativ von Literaturwissenschaftlerinnen und Literaturwissenschaftler zur Bedeutungszuweisung durchgeführt werden. Diese routineförmigen Tätigkeiten sind „oftmals nicht vollständig durch explizierbare Regeln oder Methoden bestimmt, sondern [beruhen] in hohem Maße auf implizitem Wissen und Können“ (Albrecht et al. 2015: 2).&lt;/p&gt;
&lt;p&gt;Zweitens nehmen wir an, dass nutzerzentrierte Gestaltung als etablierter Prozess, dessen Methodenrepertoire sich sowohl aus dem an der Praxis orientierten Design speist, als auch aus der eher wissenschaftlich orientierten Human-Computer-Interaction (HCI), als Vorgehen besonders geeignet ist, implizites Wissen und Können von Nutzern herauszufinden. In diesem Sinne steht die Entwicklung eines Interfaces nicht am Ende „as an afterthought, thrown together after completing the core functionality” (Gibbs/Owens 2012), sondern ist Teil der Ausdifferenzierung und Analyse von Textinterpretation.&lt;/p&gt;
&lt;p&gt;Unser Ziel ist es, mit der Integration von Designmethoden der von Gibbs und Owens beklagten mangelnden Nutzerzentriertheit bestehender DH-Tools begegnen zu können.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;III.        Annäherung an die hermeneutische Praxis über nutzerzentrierte Szenarien&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Mithilfe von Szenarien, die das Spektrum literaturwissenschaftlicher Interpretationsarbeit aus der Praxis heraus beschreiben, sollen besagte ‚routineförmige Tätigkeiten‘ untersucht und erfassbar gemacht werden.&lt;sup&gt;&lt;sup&gt;[6]&lt;/sup&gt;&lt;/sup&gt; Kollaborativ wurden bislang drei generische Szenarien konstruiert, die das Spektrum typischer interpretierender Forschungstätigkeiten repräsentieren und im weiteren Projektverlauf iterativ weiter ausgebaut werden.&lt;sup&gt;&lt;sup&gt;[7]&lt;/sup&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Die Konzepte erstrecken sich vom klassischen „Close Reading“-Beispiel mit Fokus auf noch nicht taxonomisch gelenkte freie Annotationen und Kommentare bis zum „Distant Reading“-Beispiel unter Anwendung des probabilistischen Verfahrens „Topic Modeling“.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Analog zu den Szenarien ist auch der hier verfolgte nutzerzentrierte Gestaltungsprozess typischerweise durch ein iteratives Vorgehen gekennzeichnet.&lt;sup&gt;&lt;sup&gt;[8]&lt;/sup&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Abbildung 1 zeigt ausschnitthaft eine Auswahl von Varianten als sogenannte „Wireframes“ möglicher Interfaces für die Tätigkeit „Kommentieren“ in dem Szenario „Exploration freier Annotationen zwecks Schärfung der literaturwissenschaftlichen Fragestellung“.&lt;/p&gt;
&lt;p&gt;In der Abbildung wird der Entwicklungsstand der visuellen Gestaltung dieser Tätigkeit zu zwei verschiedenen Zeitpunkten sichtbar. Der rechte Wireframe präsentiert einen weiter fortgeschrittenen Stand. Deutlich wird insbesondere die Verschränkung und Abhängigkeit einzelner Tätigkeiten, die nicht isoliert betrachtet werden kann. Hier sieht man etwa den Zusammenhang zwischen den Tätigkeiten „Annotieren“ und „Kommentieren“ dargestellt, in dem Sinne, dass ein Kommentar in der Regel (wenn auch nicht immer) einer Annotation zugeordnet ist, die eine diskrete Position im Text besitzt, also häufig nicht isoliert von der Tätigkeit des Annotierens betrachtet werden kann.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IV.       Erste Ergebnisse und Schlussfolgerungen&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bereits in diesem im letzten Abschnitt nur grob umrissenen Beispiel möglicher Varianten einer visuellen Unterstützung der Tätigkeit „Kommentieren“, manifestieren sich zwei der wesentlichen Herausforderungen an eine die Textinterpretation unterstützende und konstituierende Visualisierung.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Die Überlagerung und Wechselwirkung routinemäßiger Tätigkeiten&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Aufgabe der folgenden Iterationen von Szenarien und Wireframes wird es unter anderem sein, für die Überlagerung und Wechselwirkung der routinemäßigen Tätigkeiten in der Gestaltung des Interfaces eine adäquate Entsprechung zu finden.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Auf Grundlage der bisher erstellten Wireframes für die drei Szenarien (gegenwärtig sind es etwa 40) ließen sich in Diskussionen zwei Modi des literaturwissenschaftlichen Interpretationsverfahrens ausmachen. Zum einen besteht der Interpretationsprozess aus Tätigkeiten, die primär die analytische, investigative Erforschung des Textes adressieren (Annotieren, Sammeln, Kommentieren).&lt;sup&gt;&lt;sup&gt;[9]&lt;/sup&gt;&lt;/sup&gt; Komplementär dazu verhalten sich zum anderen die argumentativen, vornehmlich synthetisierenden Tätigkeiten der Textinterpretation (Gruppieren, Ordnen, Strukturieren). Um den Interpretationsprozess adäquat unterstützen zu können, müssen hermeneutische Interfaces beide Modi in enger Verschränkung umfassen und stetigen Wechsel zwischen diesen ermöglichen. Es lässt sich von einer schleifenartigen Struktur des Prozesses sprechen, ähnlich zur zirkulären Gestalt der Hermeneutik. Abbildung 2 zeigt einen ersten Versuch, die Ebene der Bedeutungsproduktion – von uns als „semantic plane“ bezeichnet – also die Ebene, die argumentative und synthetisierende Tätigkeiten umfasst, in die Interface-Entwürfe zu integrieren.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Mit der gleichzeitigen Darstellung der beiden Modi werden einzelne literaturwissenschaftliche Tätigkeiten sichtbar gemacht und können möglicherweise im Interpretationsprozess in Rekurs auf einen literatur- oder kulturtheoretischen Ansatz explizit gemacht werden.&lt;sup&gt;&lt;sup&gt;[10]&lt;/sup&gt;&lt;/sup&gt; Grundsätzlich erscheint es sinnvoll, einen möglichst modularisierten Aufbau des Interfaces anzustreben.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Flexibilität und Manipulierbarkeit von Visualisierungen&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Aufgrund des schwer formalisierbaren Verlaufes der Textinterpretation muss das Interface so generisch wie möglich gestaltet werden. Neben der flexiblen Unterstützung von unterschiedlichen Tätigkeiten und deren Zusammenspiel, erscheint auch der Einsatz von verschiedenen Visualisierungen sinnvoll (vgl. Cheema et al. 2015). Dazu gehört auch die Option, verschiedene Visualisierungen einzusetzen und deren Resultate vergleichen zu können, sowie u.a. Darstellungsform, Interaktivität und jeweils repräsentierten Datenausschnitt individuell bestimmen zu können.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Für eine derart weitgehende Justierbarkeit der Visualisierungen erscheint es uns sinnvoll, uns an Visualisierungssystematiken zu orientieren, wie Wilkinson sie mit seinem „Grammar-of-Graphics“-Ansatz vorschlägt (vgl. Wilkinson et al. 2005). Es existieren bereits verschiedene Implementierungen dieses Ansatzes, die es erlauben, die oben genannten Parameter einer Visualisierung mithilfe einer Grammatik beschreibbar und manipulierbar zu machen (vgl. Satyanarayan et al. 2017). In unserem Kontext ist die deklarative Sprache VEGA&lt;sup&gt;&lt;sup&gt;[11]&lt;/sup&gt;&lt;/sup&gt; interessant, die auf der verbreiteten Javascript-Visualisierungsbibliothek D3&lt;sup&gt;&lt;sup&gt;[12]&lt;/sup&gt;&lt;/sup&gt; beruht und es Nutzerinnen und Nutzern ermöglichen würde, ihre Visualisierungsspezifikationen in JSON zu formulieren.&lt;/p&gt;
&lt;p&gt;Im Sinne einer nutzerzentrierten Gestaltung ist es des Weiteren wichtig, diese Funktionalitäten an die unterschiedlichen Vorkenntnisse der Nutzerinnen und Nutzer anpassen zu können. Weniger fortgeschrittene Nutzerinnen und Nutzer sollten stets auf Standardvisualisierungen zurückgreifen können. Unserem Anspruch gemäß sollte immer die Unterstützung des interpretierenden Prozesses als Ganzheit mit dem Interface im Vordergrund stehen.&lt;/p&gt;
&lt;p&gt;Im weiteren Verlauf des Projektes werden wir uns damit befassen, mithilfe der Szenarien und Wireframes weitere analytische und synthetisierende Tätigkeiten und deren mögliches Zusammenspiel zu untersuchen. Zu einem späteren Zeitpunkt wird ferner das Testen mithilfe von Prototypen aufschlussreich sein, denn hier zeigt sich inwieweit die Gestaltungsansätze tatsächlich an den Bedürfnissen und Arbeitsweisen von Nutzern ausgerichtet sind.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p1_abstract>
  <p2_paperID>233</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Barth, Florian
Kim, Evgeny
Murr, Sandra
Klinger, Roman</p2_authors>
  <p2_organisations>Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland
Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland
Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland
Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland</p2_organisations>
  <p2_emails>florian.barth@ilw.uni-stuttgart.de
evgeny.kim@ims.uni-stuttgart.de
sandra.murr@ilw.uni-stuttgart.de
roman.klinger@ims.uni-stuttgart.de</p2_emails>
  <p2_presenting_author>Barth, Florian
Murr, Sandra</p2_presenting_author>
  <p2_title>A Reporting Tool for Relational Visualization and Analysis of Character Mentions in Literature</p2_title>
  <p2_abstract>&lt;p&gt;to be added&lt;/p&gt;
</p2_abstract>
  <p3_paperID>256</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Wieners, Jan
Schubert, Zoe
Eide, Øyvind</p3_authors>
  <p3_organisations>Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p3_organisations>
  <p3_emails>jan.wieners@uni-koeln.de
zoe.schubert@uni-koeln.de
oeide@uni-koeln.de</p3_emails>
  <p3_presenting_author>Wieners, Jan
Schubert, Zoe
Eide, Øyvind</p3_presenting_author>
  <p3_title>Modellieren durch mediale Transformation: Das Theater Brechts in der virtuellen Realität</p3_title>
  <p3_abstract>&lt;p&gt;Um den Einfluss von Virtual Reality (VR) auf die Modellierung von Inhalten und Narrationen zu ergründen, führt das Institut für Digital Humanities, Universität zu Köln seit dem mehreren Semestern Lehrveranstaltungen durch, in denen die Theorie um Medialität im Kontext virtueller Welten reflektiert und von Studierenden des vom Institut angebotenen Verbundsstudienganges Medieninformatik /  Medienkulturwissenschaften praktisch erprobt wird. Mit dem vorliegenden Abstract seien die Forschungsfragen vorgetragen, wie sie sich aus der theoretischen und praktischen Beschäftigung mit VR konkretisierten.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>70</session_ID>
  <session_title>Kaffeepause</session_title>
  <session_start>2018-02-28 10:30</session_start>
  <session_end>2018-02-28 11:00</session_end>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>32</session_ID>
  <session_short>VP_2a</session_short>
  <session_title>Theorie der digitalen Geisteswissenschaften II</session_title>
  <session_start>2018-02-28 11:00</session_start>
  <session_end>2018-02-28 12:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Gius, Evelyn</chair1>
  <attendee_count>4</attendee_count>
  <chair1_name>Evelyn Gius</chair1_name>
  <chair1_organisation>Universität Hamburg</chair1_organisation>
  <chair1_email>evelyn.gius@uni-hamburg.de</chair1_email>
  <chair1_ID>1099</chair1_ID>
  <sessionID>32</sessionID>
  <presentations>3</presentations>
  <p1_paperID>141</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Lauer, Gerhard</p1_authors>
  <p1_organisations>Universität Basel, Schweiz</p1_organisations>
  <p1_emails>gerhard.lauer@unibas.ch</p1_emails>
  <p1_presenting_author>Lauer, Gerhard</p1_presenting_author>
  <p1_title>Kulturelle Evolution. Zur Kritik der literaturhistorischen Methode</p1_title>
  <p1_abstract>&lt;p&gt;Zu den Provokationen der Literaturwissenschaft durch die Digital Humanities gehört ihre Arbeit an großen Datenmengen. Nicht das besondere Buch, der Kanon oder der Großschriftsteller, sondern die vielen Bücher und Literaturen sind der ‚andere‘ Gegenstand der computergestützten Literaturwissenschaft. Ein radikaler Ansatz, die Geschichte der Literatur anders als bisher zu modellieren, ist der Ansatz der kulturellen Evolution. Wie der Name schon andeutet, verschiebt der Ansatz den Akzent von der Geschichte auf die Evolution. In den Blick rückt nicht weniger als die Menschheitsgeschichte. Der Ansatz kommt denn auch aus der biologischen Anthropologie, nicht aus der Literaturgeschichte oder verwandten geisteswissenschaftlichen Fächern. Das leitende Paradigma ist Darwins Theorie der Evolution. Die Theorie der kulturellen Evolution überträgt dieses Modell auf die Kulturgeschichte der Menschheit. Sie geht also von der These aus, dass die Entwicklung der menschlichen Kulturen der gleichen evolutionären Entwicklungslogik folgt, der auch die Natur unterliegt. Mit computergestützten Modellen und experimentellen Methoden untersucht die Theorie die Evolution der Kultur.&lt;/p&gt;
&lt;p&gt;Mein Vortrag stellt diese Theorie der kulturellen Evolution vor und ist ein Plädoyer, die kulturelle Evolutionstheorie für eine Literaturgeschichte der langen Dauer zu nutzen. Er zeigt wesentliche Ansätze und Methoden dieser Theorie auf.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>224</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Lindemann, David
Kliche, Fritz
Kutzner, Kristin</p2_authors>
  <p2_organisations>Universität Hildesheim, Deutschland
Universität Hildesheim, Deutschland
Universität Hildesheim, Deutschland</p2_organisations>
  <p2_emails>david.lindemann@uni-hildesheim.de
fritz.kliche@uni-hildesheim.de
kristin.kutzner@uni-hildesheim.de</p2_emails>
  <p2_presenting_author>Lindemann, David
Kliche, Fritz</p2_presenting_author>
  <p2_title> Lexikographie: Explizite und implizite Verortung in den Digital Humanities</p2_title>
  <p2_abstract>&lt;p&gt;Zusammenfassung&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ziel der hier vorgestellten Studie ist eine Beschreibung der Schnittmenge von Diskursräumen in der Lexikographie bzw. Metalexikographie und den Digital Humanities (DH). Dabei geht es um die Bestimmung von explizit bzw. implizit als Teil der DH aufzufassenden Beiträgen zu lexikographischen Themen und, andersherum, von lexikographierelevanten Themen, die in den DH diskutiert werden. Zur Bestimmung der Diskursräume, von Schnitt- und disjunktiven Mengen werden Volltexte und Metadaten analysiert, bibliometrische Netzwerke (Autoren- bzw. Zitationsnetzwerke) verglichen und Topic Modelings vorgenommen.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Einleitung&lt;/p&gt;
&lt;p&gt;Der Einzug digitaler Methoden und Werkzeuge in die Geistes- und Sozialwissenschaften, genauer: der als „computational turn“ (Berry 2011) bezeichnete methodisch-epistemologische Quantensprung, lässt sich in allen Disziplinen der Humanities beobachten. In der Sprachwissenschaft hat sich dieser Wandel bekanntermaßen besonders deutlich in der Etablierung der Computerlinguistik als eigene Disziplin niedergeschlagen. Neben computerlinguistischen Verfahren der Textanalyse sind eine maschinenlesbare Wissensrepräsentation und -organisation, sind Formate für digitale Editionen und komputationell erstellte Visualisierungen heute in allen textbasierten Disziplinen in Gebrauch.&lt;/p&gt;
&lt;p&gt;Der angesprochene Wandel lässt sich ebenfalls in der Lexikographie feststellen. Die Lexikographie bzw. Metalexikographie, als solche bereits seit geraumer Zeit als Disziplin emanzipiert (Tarp 2008; Wiegand 2013), haben den Übergang zum digitalen Medium inzwischen vollzogen (cf. zum frühen Stand der Dinge De Schryver 2003) und sind beständig dabei, ihr komputationell informiertes methodisches Instrumentarium weiterzuentwickeln (Heid 2013). Als zentrale Aspekte gelten hier der Einzug korpuslinguistischer Verfahren in die Lexikographie (Hanks 2008; Heid 2008), komputationelle Methoden zur Datenrepräsentation (Spohr 2012), speziell auch für die digitale Edition historischer Wörterbücher (Lemnitzer u. a. 2013) und zur Implementierung funktionsgerichteter Benutzerschnittstellen (Heid 2014) sowie zur Wörterbuchbenutzungsforschung (Müller-Spitzer 2014).&lt;/p&gt;
&lt;p&gt;Alle angesprochenen Aspekte dieses digitalen Fortschritts sind Themen, die die Digital Humanities betreffen, und dies in zweifacher Hinsicht. Zum einen finden komputationelle Methoden und Werkzeuge, die in der Lexikographie nützlich sind, in anderen textbasierten Disziplinen ebenfalls Anwendung, weshalb ein über die Digital Humanities vermittelter interdisziplinärer Austausch zweifellos sinnvoll erscheint. Zu dieser praktischen Sicht auf die Dinge kommt zum anderen ein Bedarf an Abstraktion von und Theoriebildung über den digitalen Fortschritt, der die Metalexikographie umtreibt und selbstverständlich auch in den Digital Humanities allgemein diskutiert wird, wie die Rahmenthemen für diesen Kongress bezeugen. Zum Kernthema dieser Tagung, also der Kritik der Digitalisierung in ihren Facetten, kann und will das Nachdenken über digitale Wörterbücher einiges beitragen. Gedacht sei in dieser doppelten Hinsicht etwa an die Chancen, die ein solcher interdisziplinärer Austausch bei Forschungen zur Mensch-Maschine-Interaktion und Usability bzw. der Wörterbuchbenutzungsforschung und Wörterbuchkritik eröffnet. Neben den methodisch-praktischen Aspekt, der hier auf der Hand liegt, tritt eine gemeinsame abstrahierend kritische Fragestellung: Ungeachtet des traditionellen Qualitätskriteriums, welches sich aus Nutzersicht etwa am wahrgenommenen Prestige des Wörterbuchverlags festmachen lässt, tritt nun mutmaßlich die Art und Weise der Datenpräsentation in den Vordergrund: Neu geschaffene Digitalangebote aller Art lassen ausgewiesene Wörterbuchklassiker im Regal verstauben; scheint also bei diesem Triumph der Form über den Inhalt Besorgnis angebracht?&lt;/p&gt;
&lt;p&gt;In der hier vorgestellten Studie betrachten wir diesen gemeinsamen Diskursraum, diese Schnittmenge von Lexikographie und Digital Humanities. Nach einer nicht exhaustiven und per Hand durchgeführten Voruntersuchung folgen wir der Ausgangshypothese, der gemeinsame Diskursraum sei um ein Vielfaches größer als man annehmen könnte, folgte man allein denjenigen &lt;em&gt;T&lt;/em&gt;&lt;em&gt;opics&lt;/em&gt;, die als lexikographierelevant gelten können und die in Publikationen diskutiert werden, die &lt;em&gt;explizit&lt;/em&gt; zum Bereich der DH gehören (vgl. Abb. 1).&lt;/p&gt;
&lt;p align="center"&gt;Abb. 1: Ausgangshypothese. Explizite und implizite Schnittmengen&lt;/p&gt;
&lt;p&gt;Diejenigen Arbeiten, die im DH-Kontext veröffentlicht werden und explizit einem Thema der Lexikographie zugeordnet werden, sind recht leicht über relevante Schlüsselwörter bestimmbar. Dazu tritt die Gruppe jener Publikationen, die zur eingangs skizzierten Schnittmenge zu zählen sind, ohne dass sie sich selbst ausdrücklich den Digital Humanities zuordnen. Es ist das Ziel dieser Untersuchung, zu bestimmen, welche in der Lexikographie diskutierten Themen und welche Autoren zu dieser Gruppe gerechnet werden und also eine Zurechnung zu den Digital Humanities &lt;em&gt;implizieren&lt;/em&gt; können.&lt;/p&gt;
&lt;p&gt;Voruntersuchung und Zwischenergebnis: Explizite Verortung in den DH&lt;/p&gt;
&lt;p&gt;Als Voruntersuchung zum benannten Gegenstand haben wir eine Recherche in den Archiven bedeutender englischsprachiger Zeitschriften der Digital Humanities&lt;sup&gt;1&lt;/sup&gt; sowie in den Proceedings der ADHO-Jahreskonferenzen&lt;sup&gt;2&lt;/sup&gt; durchgeführt. Über die Suchbegriffe „Lexicography“ und „Dictionary“ finden sich in den genannten Archiven 31 englischsprachige Beiträge, die sich mit lexikographischen Themen befassen, und die sich qua Erscheinen in DH-Medien zu denjenigen Publikationen zählen lassen, die sich explizit in den Digital Humanities verorten.&lt;/p&gt;
&lt;p&gt;Eine manuelle Zuordnung lexikographierelevanter Schlüsselwörter zu den genannten 31 Beiträgen ergibt das in Tabelle 1 wiedergegebene Bild; dabei sind mehrfache Zuordnungen möglich. Zunächst lässt sich ohne Verwunderung feststellen, dass in allen Beiträgen die digitale Repräsentation lexikalischer Daten eine Rolle spielt, allerdings mit unterschiedlichen Fragestellungen, Herangehensweisen und Zielsetzungen. Die drei größten Themencluster haben wir hier, in dieser Reihenfolge, mit den Schlagwörtern „e-Wörterbücher / Visualisierung lexikalischer Daten“, „Historische Lexikographie“ und „Korpuslinguistik“ bezeichnet. Ersteres benennt Fragen der Produktion digitaler Wörterbücher einschließlich neuer Methoden der Visualisierung, letzteres die Erstellung und Nutzung elektronischer Textkorpora zu einer Reihe lexikographischer Zwecke. Beide Bereiche sind durch die Heraufkunft digitaler Methoden überhaupt erst möglich geworden und haben die Lexikographie revolutioniert. Die Historische Lexikographie kann als philologische Disziplin gelten, die sich mit der Edition historischer lexikalischer Datensammlungen befasst; die diesem Schlüsselwort zugeordneten Beiträge befassen sich grundsätzlich mit Methoden digitaler Edition, einem Kernbereich der DH.&lt;/p&gt;
&lt;p align="center"&gt;Tabelle 1: Schlüsselwörter, manuelles Clustering, manuelle Zählung&lt;/p&gt;
&lt;p&gt;Unter den weniger häufig gewählten Schlüsselwörtern sticht das „Wörterbuchnetz“ hervor, das Strategien zur Vernetzung lexikalischer Ressourcen bezeichnet. Hinzu kommen noch lexikalische Datensammlungen zur Anwendung in der maschinellen Sprachverarbeitung („NLP-Lexicon“), Methoden zum Entwurf zweisprachiger Wörterbuchinhalte („Bilingual Dictionary Drafting“), das „Autorenwörterbuch“, also Extraktionen aus Korpora, die aus dem Schaffen jeweils einer Literatin oder eines Literaten bestehen, sowie in einem Fall eine dialektologische Forschung mit digitalen Methoden.&lt;/p&gt;
&lt;p&gt;Methode für die Bestimmung implizit in den DH verorteter Arbeiten&lt;/p&gt;
&lt;p&gt;Wir betreiben den Aufbau eines Textkorpus, das im Zeitraum 1995 bis zur Gegenwart erschienene englischsprachige Beiträge aus Zeitschriften, Kongressakten und Handbüchern zu den Digital Humanities (Subkorpus &lt;em&gt;DigHum&lt;/em&gt;) und der Lexikographie (Subkorpus &lt;em&gt;Lexicog&lt;/em&gt;) enthält. Dabei werden die ausgewählten Zeitschriften bzw. Sammelbände jeweils vollständig berücksichtigt; die Beiträge werden zusammen mit Metadaten, u. a. Verfasser (Name und Affiliation), Datum, Textsorte, Umfang und Identifier (ISBN, DOI), im Tool Zotero&lt;sup&gt;1&lt;/sup&gt; verwaltet. Die Volltexte werden semiautomatisch bereinigt, lemmatisiert (TreeTagger (Schmid, 1994)) und zusammen mit Metadatensätzen in das Korpus aufgenommen (Geist (zitiert als Werkbank in Kliche u. a. 2014)). Darüber hinaus werden die in den Volltexten enthaltenen bibliographischen Referenzen extrahiert (GROBID, Lopez 2009).&lt;/p&gt;
&lt;p align="center"&gt;Abb. 2: Pipeline und eingesetzte Tools.&lt;/p&gt;
&lt;p&gt;1. Topic Modeling&lt;/p&gt;
&lt;p&gt;Unüberwachtes Topic Modeling (LDA, eingesetztes Tool: MALLET (McCallum 2002)) wird es uns ermöglichen, die in Abb. 1 grob skizzierten Mengen als sich überschneidende Diskursräume zu quantifizieren und zu visualisieren. Die Performanz des LDA-Tools wird mit unterschiedlichen Parametern und einer zusätzlich zur Lemmatisierung durch Termextraktion erweiterten Pipeline getestet. Nach Testläufen und der Definition des geeignetsten Versuchsaufbaus können die errechneten Topics dann als Cluster dargestellt und nach ihrer Zugehörigkeit zu &lt;em&gt;DigHum&lt;/em&gt; oder &lt;em&gt;Lexicog&lt;/em&gt; bzw. zu beiden Schnittmengen klassifiziert werden.&lt;/p&gt;
&lt;p&gt;2. Citation Network&lt;/p&gt;
&lt;p&gt;Die Berechnung eines bibliographischen Netzwerks als Graph auf der Grundlage von Koautorenschaft und Koreferenzen (eingesetzte Tools: CitNetExplorer und VOSviewer (Eck und Waltman 2017)) wird die Zuordnung von Autoren, Autorenclustern und Zitationsclustern zu den benannten Diskursräumen ermöglichen. Insbesondere werden wir signifikante Zitationscluster benennen können, deren Vektoren die Grenzen der in Abb. 1 skizzierten Teildiskursräume überschreiten, also auch Querverweise von der Lexikographie in die allgemeineren Digital Humanities.&lt;/p&gt;
&lt;p&gt;Ergebnisse und Schlussfolgerungen&lt;/p&gt;
&lt;p&gt;Die vorgestellten korpuslinguistischen und bibliometrischen Untersuchungen bieten wie beschrieben Aufschluss über Schnitt- und disjunkte Mengen von Themen- und Autorenclustern der Lexikographie und der Digital Humanities. Visualisierungen dieser Cluster und Listen der relevanten Keywords, Topics und Autoren werden bereitgestellt.&lt;/p&gt;
&lt;p&gt;Die gezeigten Ergebnisse können zunächst zu einer verbesserten gegenseitigen Wahrnehmung in Lexikographie und Digital Humanities beitragen sowie in der lexikographischen Community das Bewusstsein dafür stärken, ein Gutteil der Disziplin gehöre durch die inhaltliche und personelle Überschneidung &lt;em&gt;de facto&lt;/em&gt; zum Einflussbereich der Digital Humanities. Dies wiederum dürfte in der Zukunft zu einer stärkeren expliziten Verortung relevanter lexikographischer Beiträge in den Digital Humanities führen.&lt;/p&gt;
&lt;p&gt;Weiterhin haben wir mit den für diese Studie durchgeführten Arbeiten eine annotierte bibliographische Datensammlung angelegt und die dazugehörigen Volltexte mit korpuslinguistischen Methoden annotiert und analysiert. Diese Sammlung soll auch weiterhin gepflegt werden und wird zur weiteren Verwendung öffentlich zugänglich gemacht.&lt;/p&gt;
&lt;p&gt;1http://zotero.org&lt;/p&gt;
&lt;p&gt;1Digital Humanities Quarterly: http://www.digitalhumanities.org/dhq; &lt;br /&gt; DSH (ex LLC): https://academic.oup.com/dsh; &lt;br /&gt; TEI Journal of the Text Encoding Initiative: http://jtei.revues.org; &lt;br /&gt; DHCommons: http://dhcommons.org.&lt;/p&gt;
&lt;p&gt;2http://adho.org&lt;/p&gt;
</p2_abstract>
  <p3_paperID>288</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Bubenhofer, Noah</p3_authors>
  <p3_organisations>Universität Zürich, Schweiz</p3_organisations>
  <p3_emails>bubenhofer@cl.uzh.ch</p3_emails>
  <p3_presenting_author>Bubenhofer, Noah</p3_presenting_author>
  <p3_title>Wissenschaft ohne Geist: Herausforderungen der Digital Humanities am Beispiel der Korpuslinguistik</p3_title>
  <p3_abstract>&lt;p&gt;Alle, die mit maschinellen Methoden Sprache analysieren, erleben momentan einen tiefgreifenden methodologischen Wandel. Einerseits erfreut man sich vielleicht als Geisteswissenschaftler/in am immer stärkeren Interesse der Ingenieurtechniken und der Informatik für Sprache. Dies kann durchaus als Erfolg der Linguistik betrachtet werden, zurückgehend auf den Linguistic Turn, der viele andere Disziplinen schon seit Jahrzehnten beeinflusst. Unternehmen interessieren sich für ihre Reputation im massenmedialen Diskurs oder sind der Überzeugung, ihr in unzähligen Dokumenten versprachlichtes Wissen besser verwalten zu können, wenn sie es nach sprachlichen Kriterien neu ordnen. Das Geschäftsmodell von Internetunternehmen basiert ganz erheblich darauf, sprachliche Kommunikation maschinell zu verarbeiten um daraus Wissen aufzubauen und Vorhersagen über das Handeln von Kunden zu machen. Auch in der Politik ist die Analyse von Sprachgebrauch ein wichtiger Faktor, um Wahlkämpfe zu gewinnen.&lt;/p&gt;
&lt;p&gt;Andererseits beschert einen dieses Interesse eine Vielzahl von neuen Methoden für die maschinelle Analyse von Text, die auch für geisteswissenschaftliche Fragestellungen interessant sind. Die Digital Humanities sind ein Beispiel für eine Disziplin, die sich den Experimenten mit diesen Methoden verschrieben hat. Auch die Korpuslinguistik profitiert maßgeblich von diesen neuen Methoden.&lt;/p&gt;
&lt;p&gt;Aktuell erfahren in der Computerlinguistik und generell im Data Mining neuronale Netze großen Zuspruch, die den Prozess des maschinellen Lernens nach dem Modell des menschlichen Gehirns gestalten. Solche Systeme, „Deep Learning“-Systeme genannt, sind in der Lage, Muster in den Daten zu erkennen, ohne dass vorher explizit die Eigenschaften festgelegt werden, die getestet werden sollen. Zudem findet das Lernen auf mehreren verborgenen Ebenen statt, so dass das Lernen nicht beobachtet und damit auch die Frage, welche Eigenschaften nun welchen Einfluss auf das gelernte Modell haben, kaum beantwortet werden kann.&lt;/p&gt;
&lt;p&gt;In der Computerlinguistik wurden bereits für viele Probleme Deep-Learning-Algorithmen eingesetzt, meist mit Erfolg. Erfolg bedeutet, dass die statistischen Modelle besser den Goldstandard voraussagen können, aber nicht, dass das grundlegende linguistische Problem (z.B.: Sentiment-Analyse: wie werden Gefühle und Meinungen ausgedrückt; Textklassifikation: wie drückt sich Stil, Autorschaft, Textsorte, Thema etc. aus) besser gelöst wäre.&lt;/p&gt;
&lt;p&gt;Überall wo Sprachgebrauch quantitativ und maschinell analysiert wird, gibt es einen starken Trend, möglichst ohne linguistischen Kategorien und Theorien auszukommen und Black-Box-Systeme zu verwenden. Das ist nachvollziehbar, da es in den meisten Fällen darum geht, ein System zu bauen, das eine klar definierte Aufgabe sehr gut lösen kann. Obwohl diese Ansätze natürlich auch für die Korpuslinguistik interessant sind, genügen sie linguistischen Forschungsinteressen eigentlich nicht, da sie keinen Beitrag dazu leisten, sprachliche Phänomene zu verstehen und erklären zu können.&lt;/p&gt;
&lt;p&gt;Viel dramatischer ist jedoch, dass die Linguistik offensichtlich nicht in der Lage ist, einen nützlichen Beitrag an die Probleme der maschinellen Textanalyse zu leisten. Die Linguistik scheint für die quantitative Analyse von Text weitgehend bedeutungslos zu werden.&lt;/p&gt;
&lt;p&gt;Um der Bedeutungslosigkeit zu entgehen, muss die Linguistik ein kritisches Verhältnis zur Forschungslogik in den ingenieurstechnischen Disziplinen pflegen und auf zwei Prinzipien bestehen: 1) Mehr linguistische Theorie. 2) Ergebnisse von quantitativen Analysen müssen gedeutet werden.&lt;/p&gt;
&lt;p&gt;Zu 1): Nicht nur für die Linguistik, sondern für alle geistes- und sozialwissenschaftlichen Disziplinen gilt: Eine theoretische Fundierung der Analysekategorien ist essentiell. Dafür werden valide Analysekategorien benötigt, die deutbar sind. Dieses Prinzip richtet sich jedoch keinesfalls gegen datengeleitete Verfahren, im Gegenteil: Sie sind es, die die theoretischen Modelle herausfordern und schärfen können. Aber das Ziel aller Analysen muss darin liegen, ein Puzzleteil zu einem besseren Verständnis sprachlicher Strukturen, von Sprachgebrauch oder gesellschaftlichen und kulturellen Bedeutungen von Sprache zu führen. Wir benötigen White-Box-, nicht Black-Box-Systeme.&lt;/p&gt;
&lt;p&gt;Das Problem der fehlenden Validität zeigt sich z.B. im Feld der sog. „Authorship Attribution“, also der Zuordnung eines Textes X zu einem Autor A, B, C, …. Um dies zu tun, stehen Texte zur Verfügung, von denen die Autorschaft bekannt ist. Die Frage ist dann also, ob über die sprachlichen Merkmale des Textes X automatisch bestimmt werden kann, wer der Autor/die Autorin (aus der Menge der möglichen Autoren/innen) von Text X ist. Genauer lautet die Frage aber, ob und wie sich persönlicher Schreibstil sprachlich niederschlägt.&lt;/p&gt;
&lt;p&gt;Besonders erfolgreich für diese Aufgabe sind Methoden maschinellen Lernens, die das Problem als Klassifikationsaufgabe auffassen und anhand von Trainingskorpora typische sprachliche Merkmale der Texte der jeweiligen Autor/innen lernen. Dabei zeigt sich, dass „low-level features like character n-grams are very successful for representing texts for stylistic purposes” (Stamatatos, 2009, S. 24). Das bedeutet, solche Modelle, die auf der Distribution von Buchstaben-N-Grammen beruhen, sind, gemessen an einem Goldstandard, am erfolgreichsten. Allein: Solche Modelle lassen sich nicht linguistisch deuten, da völlig unklar ist, was sie eigentlich messen. Ist es Stil, Thema, Textsorte, …? Es handelt sich also weder um eine valide, noch um eine deutbare Kategorie (insbesondere, wenn das statistische Modell nicht einsehbar ist). Für spezifische Aufgaben der Autorschaftsattribution mag das ausreichend sein, aber bereits für forensische Anwendungen, beispielsweise vor Gericht, ist eine solche Modellierung fragwürdig und gefährlich. Und für eine linguistische Deutung des Phänomens Autorschaftsstil ist sie gänzlich unbrauchbar.&lt;/p&gt;
&lt;p&gt;Die Kritik geht jedoch nicht nur in Richtung des Textminings und der Computerlinguistik, manchmal nicht-valide Kategorien einzusetzen (was zudem oft für die dortigen Zwecke auch sinnvoll ist), sondern auch in die Richtung der Linguistik: Die Computer- und die Korpuslinguistik zeigen beide gleichermaßen, wie wichtig es ist, auch abstrakte Kategorien versuchen so zu definieren, dass überhaupt eine Chance besteht, sie für eine quantitative Analyse operationalisierbar zu machen. Wenn eine linguistische Kategorie so vage ist, dass sich selbst (geschulte) Menschen uneinig darüber sind, wenn sie an authentischem Sprachegebrauch angewendet werden, scheitert die quantitativ-maschinelle Lösung unweigerlich.&lt;/p&gt;
&lt;p&gt;2) Die Ergebnisse von quantitativen Analysen sind nicht Antworten auf Fragestellungen, sondern neue Daten, die vor einem geistes- und sozialwissenschaftlichen Hintergrund genauso hermeneutisch gedeutet werden müssen, wie einzelne Texte. Das ist vielleicht das größte Missverständnis, wenn Textminer und Computerlinguistinnen mit Korpuslinguistinnen zusammenarbeiten: Erstere wollen, dass ein Werkzeug ein Ergebnis hervorbringt, das an einem Goldstandard evaluiert werden kann. Das Ergebnis ist dann im Einzelfall richtig oder falsch und in der Gesamtheit genügend präzise oder nicht. Das Ergebnis ist dann auch im Idealfall die Lösung der Forschungsfrage. Bei den meisten geistes- und sozialwissenschaftlichen Fragestellungen beginnt auf der Grundlage dieser Ergebnisse jedoch ein Interpretationsprozess, um (meist in Kombination mit weiteren Analysen) eine plausible Deutung zu ermöglichen – eine vorläufige Deutung. Die Stärke der Geistes- und Sozialwissenschaften liegt dabei ja gerade darin, dass in ihrer Methodologie ein Zweifeln inhärent ist, mit dem die „gegenwärtig besiegelten Bedeutungen jeweils eingeklammert oder angezweifelt [werden], um zu prüfen, inwiefern sich nach rationalem Ermessen nicht bessere Lösungen, überlegenere Interpretationen oder zustimmungsfähigere Regelungen finden lassen“ (Honneth, 2016, S. 312).&lt;/p&gt;
&lt;p&gt;Neben der Suche nach validen Analysekategorien und dem Hochhalten geisteswissenschaftlicher Prinzipien der Deutung sehe ich einen weiteren Aspekt, der helfen sollte, der Korpuslinguistik eine deutliche linguistische Prägung zu verleihen. Es ist der Versuch, korpuslinguistisches Arbeiten als „diagrammatisches Operieren“ aufzufassen. Mit dem Diagramm-Begriff folge ich Krämer (2016), die deutlich macht, dass Diagramme als Formen der Visualisierung von Daten „Denkzeuge“ sind, mit denen operiert wird: Ich kann Daten in einem Diagramm darstellen (auf einer Karte, in einem Netzwerkgraph, einem Punkteplot, …) und danach damit operieren, um neue Erkenntnisse daraus zu ziehen. Wenn man einem breiten Diagramm-Begriff folgt, wird deutlich, dass auch Listen, Tabellen und dergleichen diagrammatischen Charakter haben (Siegel, 2009; Steinseifer, 2013). Dies sind nun aber Formen, die in der Korpuslinguistik zentral sind: Die Keyword in Context-Liste (zurückgehend etwa auf Zettelkästen im 16. Jahrhundert) etwa kann als Keimzelle eines völlig neuen Textverständnisses angesehen werden, mit dem die Einheit des Textes zerstört wird, um eine neue Sicht auf Textdaten zu gewinnen. Viele weitere Formen der Anordnung von Textdaten spielen ebenfalls wichtige Rollen, entscheidend etwa die Überführung von Textdaten in den Vektorraum, in dem operiert werden kann (z.B. in Form geometrischer Operationen – Lagen von Vektoren und ihren Winkeln zueinander). Aber auch die Erfindung der Partiturdarstellung bei Gesprächstrankripten, mit der überhaupt erst eine moderne Gesprächslinguistik möglich wurde, zeigt die Kraft von diagrammatischen Umformungen, um Daten neu lesbar zu machen.&lt;/p&gt;
&lt;p&gt;Ich meine, es lohnt sich, korpuslinguistisches Arbeiten unter diagrammatischer Perspektive zu reflektieren, um die Mechanismen und Möglichkeiten der Gegenstandskonstitution besser zu verstehen. Repräsentiert in einem Vektorraum geben die gleichen Daten einen völlig anderen Gegenstand ab als dargestellt in einer Keyword in Context-Liste. Und es müsste vordringliches Ziel sein, noch ganz andere Formen der diagrammatischen Darstellung von Text zu finden, um damit andere Gegenstandskonstitutionen und Fragestellungen zu ermöglichen. Dafür nötig sind semiotische und natürlich auch wissenschaftstheoretische Überlegungen, die für alle Disziplinen, die mit maschineller Textanalyse befasst sind, relevant sein müssten.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bibliographie&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Honneth, Axel: Denaturierung der Lebenswelt. Vom dreifachen Nutzen der Geisteswissenschaften. In: Panteos, A./Rojek, T. (Hrsg.): &lt;em&gt;Texte zur Theorie der Geisteswissenschaften&lt;/em&gt;, &lt;em&gt;Reclams Universal-Bibliothek&lt;/em&gt;. Stuttgart : Reclam, 2016, S. 283–315&lt;/p&gt;
&lt;p&gt;Krämer, Sybille: &lt;em&gt;Figuration, Anschauung, Erkenntnis: Grundlinien einer Diagrammatologie&lt;/em&gt; : Suhrkamp Verlag, 2016&lt;/p&gt;
&lt;p&gt;Nakov, Preslav/Ritter, Alan/Rosenthal, Sara/Stoyanov, Veselin/Sebastiani, Fabrizio: SemEval-2016 Task 4: Sentiment Analysis in Twitter. In: &lt;em&gt;Proceedings of the 10th International Workshop on Semantic Evaluation&lt;/em&gt;, &lt;em&gt;SemEval ’16&lt;/em&gt;. San Diego, California : Association for Computational Linguistics, 2016&lt;/p&gt;
&lt;p&gt;Siegel, Steffen: &lt;em&gt;Tabula: Figuren der Ordnung um 1600&lt;/em&gt;. Berlin / Boston : Akademie-Verlag, 2009&lt;/p&gt;
&lt;p&gt;Stamatatos, Efstathios: A Survey of Modern Authorship Attribution Methods. In: &lt;em&gt;J. Am. Soc. Inf. Sci. Technol.&lt;/em&gt; Bd. 60 (2009), Nr. 3, S. 538–556&lt;/p&gt;
&lt;p&gt;Steinseifer, Martin: Texte sehen – Diagrammatologische Impulse für die Textlinguistik. In: &lt;em&gt;Zeitschrift für germanistische Linguistik&lt;/em&gt; Bd. 41 (2013), Nr. 1, S. 8–39&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>33</session_ID>
  <session_short>VP_2b</session_short>
  <session_title>Textmining II</session_title>
  <session_start>2018-02-28 11:00</session_start>
  <session_end>2018-02-28 12:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Zinsmeister, Heike</chair1>
  <attendee_count>5</attendee_count>
  <chair1_name>Heike Zinsmeister</chair1_name>
  <chair1_organisation>Universität Hamburg</chair1_organisation>
  <chair1_email>heike.zinsmeister@uni-hamburg.de</chair1_email>
  <chair1_ID>1312</chair1_ID>
  <sessionID>33</sessionID>
  <presentations>3</presentations>
  <p1_paperID>155</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Beck, Jens
Willand, Marcus
Reiter, Nils</p1_authors>
  <p1_organisations>Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland
Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland
Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland</p1_organisations>
  <p1_emails>jens_beck@gmx.de
marcus.willand@ilw.uni-stuttgart.de
Nils.Reiter@ims.uni-stuttgart.de</p1_emails>
  <p1_presenting_author>Beck, Jens
Willand, Marcus</p1_presenting_author>
  <p1_title>Was Lesende denken: Assoziationen zu Büchern in Sozialen Medien </p1_title>
  <p1_abstract>&lt;p dir="ltr"&gt;Im vorliegenden Abstract stellen wir eine Methode sowie erste Ergebnisse vor, um Assoziationen realer Leserinnen und Leser zu untersuchen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Literaturwissenschaftliche Rezeptions, Lese- und Lesertheorien gehen seit ihren hermeneutischen und wirkungsästhetischen Anfängen (Schleiermacher 1838, insb. 309f.; Iser 1976) von professionellen (Dijkstra 1994), informierten (Fish 1970, 86), Modell- (Eco 1979) oder sogar idealen (Schmid 2005) Lesern aus (vgl. Willand, 2014). Diesen wird die Kompetenz zugeschrieben, idealerweise sämtliche Textmerkmale referentialisieren zu können, wobei je nach literaturtheoretischer Provenienz unterschiedliche Kontexte die Grundlage der Zuschreibungen an den Text bilden. Dazu gehören u.a. Informationen über den Autor oder über die sozialhistorischen Bedingungen der Textproduktion, über die Rezeptionsbedingungen, über Vorgänger- oder zeitgenössische Texte oder über Wissen aus dem Bereich der Literaturwissenschaftlerin/des Lesers selbst.&lt;/p&gt;
&lt;p dir="ltr"&gt;An bestimmte Wissensbestände realer Leserinnen und Leser literarischer Texte können wir uns durch eine computergestützte empirische Analyse von Rezeptionszeugnissen, also etwa Berichte in sozialen Medien, annähern. Konkret ist unser Ziel die Rekonstruktion und Analyse der von einem literarischen Text ausgelösten Assoziationen. Dabei beschränken wir uns auf die Assoziationen von realen (z.B. Personen des öffentlichen Lebens) oder fiktiven Entitäten (z.B. Figuren aus anderen Werken).&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Plattform Goodreads bietet Leserinnen und Lesern die Möglichkeit des freien schriftlichen Austauschs über literarische Texte in einer großen Community. 55 Mio. Mitglieder haben bis 2017 über 50 Mio. Reviews geschrieben, wobei die Besprechungen die Inhalte der Bücher selbst und nicht - wie etwa bei Verkaufsplattformen wie Amazon - die Distribution, den Preis o.ä. fokussieren (Piper et al. 2015).&lt;/p&gt;
</p1_abstract>
  <p2_paperID>266</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Pfahler, Lukas
Elwert, Frederik
Tabti, Samira
Morik, Katharina
Krech, Volker</p2_authors>
  <p2_organisations>Technische Universität Dortmund
RUB Bochum, Deutschland
RUB Bochum, Deutschland
Technische Universität Dortmund
RUB Bochum, Deutschland</p2_organisations>
  <p2_emails>lukas@wandelt-pfahler.de
frederik.elwert@rub.de
Samira.Tabti@ruhr-uni-bochum.de
katharina.morik@tu-dortmund.de
volkhard.krech@rub.de</p2_emails>
  <p2_presenting_author>Pfahler, Lukas
Elwert, Frederik
Tabti, Samira</p2_presenting_author>
  <p2_title>What do you do with 5 million posts? Versuche zum distant reading religiöser Online-Foren</p2_title>
  <p2_abstract>&lt;p&gt;Einleitung&lt;/p&gt;
&lt;p&gt;Religiöse Kommunikation als Teil moderner Gesellschaften findet zunehmend auch über internetbasierte Medien statt. Dabei sind es nicht nur liberale Gruppen, die diese neuen Medien nutzen, sondern gerade auch neo-konservative Gemeinschaften wie etwa Evangelikale oder Salafisten. Vor diesem Hintergrund nehmen wir ein spezielles Segment gegenwärtiger Religiosität in den Blick: Neo-konservative christliche und islamische Bewegungen (etwa Evangelikale oder Anhänger der Salafiyya) haben in den letzten Jahren mit eigenen Online-Foren Kommunikationsplattformen geschaffen, in denen sie jeweils eigene Auslegungen in Theologie und Fragen der Lebensführung diskutieren (Becker 2009: 9, Neumaier 2016).&lt;/p&gt;
&lt;p&gt;Bei allen Unterschieden zeichnen sich diese Bewegungen durch zwei Merkmale aus: a) eine Universalisierung von Religion im Sinne einer Ablösung „reiner“ Religion von Kultur und Politik, u. b) eine religiöse Durchdringung aller Lebensbereiche, die sich insbesondere durch eine umfassende Regulierung der Lebensführung ausdrückt (Roy 2010: 57). Die Analyse dieser Online-Communities erlaubt es, Rückschlüsse über die Entwicklung und Verbreitung bestimmter Vorstellungen, aber auch über die Genese sozialer Strukturen und neuer Autoritäten zu ziehen.&lt;/p&gt;
&lt;p&gt;Ein besonderes Augenmerk legen wir auf die diskutierten Inhalte. Themen und ihre zeitliche Entwicklung werden über Topic Models, Keyword-Analysen und ähnliche Verfahren untersucht. Damit lassen sich thematische Konjunkturen und religiöse Traditionseinflüsse identifizieren.&lt;/p&gt;
&lt;p&gt;Datenerhebung und -aufbereitung&lt;/p&gt;
&lt;p&gt;Als Grundlage unserer Analysen dienen vier Online-Foren: Zwei christliche (jesus.de seit 2009 online/Christianchat.com seit 2012 online) und zwei muslimische (ahlu-sunnah.com von 2008-2016 online/Ummah.com seit 2002 online), wobei jeweils eins überwiegend deutschsprachig und eins überwiegend englischsprachig ist. Mithilfe eines Web-Crawlers wurden erhebliche Teile der Foren heruntergeladen und für die Analyse zur Verfügung gestellt. Aus den erhobenen HTML Daten werden alle Formatierungen entfernt, sodass der reine textuelle Inhalt vorliegt. Standardtechniken zur digitalen Verarbeitung natürlichsprachlicher Daten werden angewandt, um den Text weiter zu normalisieren. Dazu gehören Tokenisierung, Konvertierung aller Buchstaben zu Kleinbuchstaben, Entfernen von Sonder- und Satzzeichen, etc.. Wir entfernen Wörter, die insgesamt seltener als 10 mal verwendet werden. So erhalten wir insgesamt über 5,52 Mio. Posts in über 260,000 Threads oder mehr als 470 Mio. Wörter an Daten.&lt;/p&gt;
&lt;p&gt;Des Weiteren verwenden wir domänenspezifische Ersetzungsregeln, um verschiedene Schreibweisen von Referenzen auf externe Quellen wie Koran und Bibel oder externe religiöse Autoritäten wie Schriftgelehrte zu normalisieren. Dies erlaubt es uns ganze Foren hinsichtlich ihrer religiösen Ausrichtung zu untersuchen, da sich je nach Ausrichtung die vornehmlich referenzierten Gelehrten und Textpassagen unterscheiden.&lt;/p&gt;
&lt;p&gt;Methoden&lt;/p&gt;
&lt;p&gt;Ein häufiges Problem automatischer Textverarbeitung ist, dass zwei Texte zum selben Thema vollständig verschiedene Wörter verwenden können. Ein zentraler Analyseschritt ist aber das automatische Gruppieren ähnlicher Dokumente, genannt Clustering. Clustering eignet sich zum einen, um einen Überblick über die vorherrschenden Themen in Online-Foren zu verschaffen, andererseits eignet es sich auch als Sampling-Instrument um fokussiert Teilmengen für eine manuelle Inhaltsanalyse auszuwählen. Wie aber erkennt man thematische Ähnlichkeiten, wenn ein Vergleich der Mengen der Wörter nicht ausreicht?&lt;/p&gt;
&lt;p&gt;Das populäre Topic-Modeling-Verfahren Latent Dirichlet Allocation (LDA) (Blei et al. 2003) berechnet in einem Schritt latente Repräsentationen und Gruppierungen: Dokumente werden einem oder mehreren Topics zugewiesen, die Vektoren der Topic-Zugehörigkeiten dienen als latente Repräsentation. Gängige Topic-Modelling-Softwares wie gensim oder Mallet verwenden dazu randomisierte Ansätze wie Gibbs Sampling oder Variational Inference. Folglich haben Analyseergebnisse einen Zufallsanteil und lassen sich zu späterem Zeitpunkt nicht eindeutig reproduzieren. Stellt sich bei der Auswertung der Ergebnisse heraus, dass die Anzahl der Themen zu unpassend gewählt wurde, muss die Berechnung mit veränderten Parametern wiederholt werden. Dabei ist nicht garantiert, dass sich genau die Topics vereinigen oder aufspalten, an denen der Anwender festgemacht hat, dass die Anzahl falsch gewählt wurde. Weiterhin ist es rechenaufwendig, die Granularität der Analyse zu verändern, da die volle Berechnung mit den veränderten Parametern wiederholt werden.&lt;/p&gt;
&lt;p&gt;Statt LDA-Repräsentationen zu berechnen, verwenden wir die konzeptionell einfacheren Document Embeddings nach Le und Mikolov (2014). Die latente Repräsentation  ist hier ein niedrig-dimensionaler, reellwertiger Parametervektor einer diskreten kategorischen Verteilung über Wörter in einem Dokument . Diese Parametervektoren sowie der Parametervektor der Wortverteilungen  werden so gewählt, dass die Likelihood der Daten maximiert wird. Dieses Optimierungsproblem betrachten wir als Matrix-Faktorisierungsproblem; statt über die und  zu optimieren, optimieren wir über die Matrix der jeweiligen Skalarprodukte.  Mithilfe eines numerischen Optimierungsverfahrens können die latenten Repräsentationen berechnet werden. Hierbei ist garantiert, dass eine optimale Lösung gefunden wird (Pfahler et al. 2017), da das Problem durch die Reformulierung konvex ist.&lt;/p&gt;
&lt;p&gt;In einem zweiten Schritt wird das Clustering der Dokumentenkollektion auf basis der latenten Repräsentationen berechnet. Hierzu verwenden wir das Agglomerative Hierarchische Clustering. Das Verfahren berechnet einen Cluster-Baum, an dessen Blättern die einzelnen Dokumente liegen; durch sukzessive Vereinigung der zwei ähnlichsten Cluster entsteht ein Baum. Dieser erlaubt uns beliebige Anzahlen von Gruppen zu identifizieren, indem der Baum auf einer festgelegten Höhe abgeschnitten wird. Soll die Clusteranzahl verändert werden, müssen die latenten Repräsentationen der Dokumente nicht neu berechnet werden, es muss lediglich der vollständige Baum anders abgeschnitten werden. Dazu gibt es verschiedene Möglichkeiten: Einzelne Cluster können in ihre zwei Untercluster aufgespalten werden oder andersrum wiedervereinigt werden oder es kann eine andere feste Gesamtanzahl angegeben werden. So kann interaktiv und dynamisch ein Clustering der Dokumente erarbeitet werden; jede Änderung der Cluster-Hierarchie ist in wenigen Sekunden berechnet. Weiterhin ist das verwendete Verfahren deterministisch, somit sind Ergebnisse garantiert reproduzierbar.&lt;/p&gt;
&lt;p&gt;Damit das Clustering interpretiert werden kann, werden Repräsentanten der Cluster berechnet. Wie die Topics bei LDA-Modellen handelt es sich auch hierbei um Wortlisten, die prinzipiell angeben, wie oft die jeweiligen Wörter in jedem Cluster vorkommen. Besser interpretierbare Ergebnisse werden erzielt, wenn statt der Anzahl der TFiDF-Score (Robertson 2004) der Wörter angegeben wird; dieser setzt die Anzahlen ins Verhältnis zur Anzahl der Dokumente, in denen die jeweiligen Wörter verwendet werden.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Vorläufige Ergebnisse&lt;/p&gt;
&lt;p&gt;Am Beispiel ahlu-sunnah.com zeigt sich, dass bestimmte salafistische Schriftgelehrte eine bedeutendere Rolle spielen als andere. Die Zwischenergebnisse lassen erste Rückschlüsse über die religiösen Richtungen und Referenzpraktiken zu: welche salafistische Gelehrte finden mehr Zustimmung als andere und welche salafistischen Traditionsschulen lassen sich identifizieren (Wahabiya, Ad-Da'wa As-Salafiya, Madchalia etc.). So sind z.B. die Gelehrten Ibn Taymiyyah u. al-Albani wichtige Referenzgrößen. Ibn Taymiyyah (13. Jhd.) als “Vater der Salafiyya” findet große Verehrung bei den puristischen sowie auch bei den politisch-aktivistischen Gruppierungen der Salafiya-Bewegung. Al-Albani (20. Jhd.) dagegen findet eher in der puristischen Bewegung Zuspruch und auch kontrovers diskutiert.&lt;/p&gt;
&lt;p&gt; (Grafik)&lt;/p&gt;
&lt;p&gt;Das oben vorgestellte Verfahren erlaubt es große Datenmengen (z.B.  ahlu-sunnah.com: 32700 Threads) so aufzubereiten, um die weitere qualitative Feinanalyse gezielt anzuwenden. Die folgende Tabelle zeigt exemplarisch 6 verschiedene Themen, die wir in den Foren-Diskussionen identifiziert haben. In diesen semantischen Feldern finden die unterschiedlichen Diskurse immer im Rahmen religiöser Argumentation und Legitimation statt. Beginnend mit Diskussionen über das Weltgeschehen, besonders im islamischen Raum (Themen 4,5), zur Lebensführung, in diesem Fall Ernährung (Thema 1), über religiöse Rituale oder Handlungen wie Heirat oder Gottesdienst  (Thema 2,3) bis zu den unmittelbaren theologischen Diskussionen wie bspw. Diskussionen religiöser Schriften (Thema 6).&lt;/p&gt;
&lt;p&gt;(Modell)&lt;/p&gt;
&lt;p&gt;Ebenso lassen sich aus einer vergleichenden Perspektive (verschiedener Foren) inhaltliche Konjunkturen und semantische Schwerpunkte herauslesen.&lt;/p&gt;
&lt;p&gt;Ausblick&lt;/p&gt;
&lt;p&gt;Eine besondere Herausforderung, welcher wir uns annehmen wollen, ist die Frage nach der Beziehung von Strukturen und Semantiken in den Foren. „Strukturen von Themen“ sind dabei die Strukturmuster, die sich auf der Ebene der Kommunikationsinhalte identifizieren lassen: Wie hängen bestimmte Themen miteinander zusammen? Wie entwickeln sich Diskursstränge über die Zeit? Wo lassen sich Brüche oder Konvergenzen beobachten?&lt;/p&gt;
&lt;p&gt;„Themen in Strukturen“ zielen dagegen darauf ab, die sozialen Formen mit den Inhalten zusammenzubringen: Welche Themen werden von bestimmten Teilgruppen aufgegriffen? Welche Diskurse ziehen sich durch ganze Online-Gemeinschaften, welche sind Partikulardiskurse abgegrenzter Gemeinschaften? Wie hängt die Verbreitung bestimmter Inhalte mit den Struktureigenschaften, etwa den Autoritäten, einer Online-Community zusammen?&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Bibliographie&lt;/p&gt;
&lt;p&gt;Becker, Carmen (2009): “Gaining Knowledge: Salafi Activism in German and Dutch Online Forums” in:  &lt;em&gt;Masaryk University Journal of Law and Technology&lt;/em&gt; 3 (1): 79–98.&lt;/p&gt;
&lt;p&gt;Blei, David M./Andrew Y. Ng/Michael I. Jordan (2003): “Latent Dirichlet Allocation,” &lt;em&gt;J. Mach. Learn. Res.&lt;/em&gt;, vol. 3, no. 4–5, pp. 993–1022.&lt;/p&gt;
&lt;p&gt;Le ,Quoc V./ Tomas Mikolov (2014): “Distributed Representations of Sentences and Documents” in: &lt;em&gt;International Conference on Machine Learning - ICML 2014&lt;/em&gt;, vol. 32, pp. 1188–1196.&lt;/p&gt;
&lt;p&gt;Robertson, Stephen (2004) “Understanding inverse document frequency: on theoretical arguments for IDF,” in:  &lt;em&gt;J. Doc.&lt;/em&gt;, vol. 60, no. 5, pp. 503–520.&lt;/p&gt;
&lt;p&gt;Roy, Olivier (2010): &lt;em&gt;Heilige Einfalt: Über Die Politischen Gefahren Entwurzelter Religionen&lt;/em&gt;. München.&lt;/p&gt;
&lt;p&gt;Pang-Ning, Tan/Michael, Steinbach/Vipin, Kumar (2006): &lt;em&gt;Introduction to data mining&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Pfahler, Lukas/Katharina, Morik/Frederik, Elwert/Samira, Tabti/Volkhard, Krech (2017): “Learning Low-Rank Document Embeddings with Weighted Nuclear Norm Regularization” in: &lt;em&gt;Proceedings of the 4th DSAA&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Neumaier, Anna (2016): &lt;em&gt;Religion@home? Religionsbezogene Online-Plattformen Und Ihre Nutzung: Eine Untersuchung Zu Neuen Formen Gegenwärtiger Religiosität&lt;/em&gt;. Religion in Der Gesellschaft 39.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>284</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Schöch, Christof
Zehe, Albin
Calvo Tello, José
Hotho, Andreas</p3_authors>
  <p3_organisations>Universität Trier
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland</p3_organisations>
  <p3_emails>schoech@uni-trier.de
zehe@informatik.uni-wuerzburg.de
jose.calvo@uni-wuerzburg.de
hotho@informatik.uni-wuerzburg.de</p3_emails>
  <p3_presenting_author>Schöch, Christof
Zehe, Albin</p3_presenting_author>
  <p3_title>Burrows Zeta: Varianten und Evaluation</p3_title>
  <p3_abstract>&lt;p&gt;Der vorliegende Beitrag enthält methodische Überlegungen und Experimente zu “Zeta”, einem von Burrows vorgeschlagenen Maß für die Distinktivität oder “keyness” von textuellen Merkmalen (Wortformen, Lemmata, etc.). Mit solchen Maßen werden Merkmale ermittelt, die für eine bestimmte Gruppe von Texten gegenüber einer Vergleichsgruppe charakteristisch sind.&lt;/p&gt;
&lt;p&gt;Das Exposé gibt einen Überblick zu solchen Maßen, bevor die Funktionsweise von Zeta erläutert wird. Aufbauend auf einer Neu-Implementierung in Python (“pyzeta”, https://github.com/cligs/pyzeta) und Vorarbeiten (Schöch im Druck) liegt der spezifische Forschungsbeitrag dann in den folgenden Schritten: erstens werden mehrere Varianten von Zeta vorgeschlagen und implementiert; zweitens werden Verfahren zum Vergleich und der Evaluation der Ergebnisse erprobt. Ziel ist es, Zeta in seiner Funktionsweise und in seiner Beziehung zu vergleichbaren Maßen besser zu verstehen und vorhandene Nachteile des Maßes durch gezielte Modifikationen zu beheben.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>34</session_ID>
  <session_short>VP_2c</session_short>
  <session_title>Sammlungsdigitalisierung II</session_title>
  <session_start>2018-02-28 11:00</session_start>
  <session_end>2018-02-28 12:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Stäcker, Thomas</chair1>
  <attendee_count>4</attendee_count>
  <chair1_name>Thomas Stäcker</chair1_name>
  <chair1_organisation>Universitäts- und Landesbibliothek Darmstadt</chair1_organisation>
  <chair1_email>thomas.staecker@ulb.tu-darmstadt.de</chair1_email>
  <chair1_email2>t.staecker@posteo.de</chair1_email2>
  <chair1_ID>1017</chair1_ID>
  <sessionID>34</sessionID>
  <presentations>3</presentations>
  <p1_paperID>128</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Boenig, Matthias
Federbusch, Maria
Herrmann, Elisa
Neudecker, Clemens
Würzner, Kay-Michael</p1_authors>
  <p1_organisations>Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland
Staatsbibliothek zu Berlin Preußischer Kulturbesitz, Deutschland
Herzog August Bibliothek Wolfenbüttel, Deutschland
Staatsbibliothek zu Berlin Preußischer Kulturbesitz, Deutschland
Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland</p1_organisations>
  <p1_emails>boenig@bbaw.de
maria.federbusch@sbb.spk-berlin.de
herrmann@hab.de
clemens.neudecker@sbb.spk-berlin.de
wuerzner@bbaw.de</p1_emails>
  <p1_presenting_author>Boenig, Matthias
Würzner, Kay-Michael</p1_presenting_author>
  <p1_title>Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?</p1_title>
  <p1_abstract>&lt;p&gt;Der Einsatz von Referenzdaten für Training und Auswertung statistischer Annotations- und Analyseverfahren ist ein Kernmerkmal empirischer Forschung, zu der auch die Digital Humanities zählen möchten. Die wichtigste Grundlage für die erfolgreiche Verwendung solcher Verfahren liegt im Einsatz geeigneter, den Algorithmen zugrunde liegender Modelle. Für deren Erstellung ist neben einem passenden Lernverfahren das Vorhandensein von Ground Truth die wesentliche Voraussetzung. Werden Forschungsdaten, die mit quantitativen Methoden entstanden sind, mit Ground Truth verifiziert und interpretiert, ist ein kritischer Blick auf Auswahl, Erstellung und Umgang mit der verwendeten Ground Truth ein häufig vernachlässigter Bereich.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>149</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Löcker-Herschkowitz, Johannes A.
Wagner, Christian</p2_authors>
  <p2_organisations>Universität Wien, Österreich
Universität Wien, Österreich</p2_organisations>
  <p2_emails>j.a.loecker-herschkowitz@univie.ac.at
christian.wagner@univie.ac.at</p2_emails>
  <p2_presenting_author>Löcker-Herschkowitz, Johannes A.</p2_presenting_author>
  <p2_title>Fachspezifische Herausforderungen in der Realisierung des webbasierten digitalen Archivs THESPIS.DIGITAL</p2_title>
  <p2_abstract>&lt;p&gt;Ausgehend von Dramen und Libretti des italienischen Dramatikers Giacinto Andrea Cicognini (1606–1649) beschäftigt sich das FWF-Forschungsprojekt „Making of a Repertoire for German Theatre (1650–1730): The Reception of Cicognini“ (Projektleiter Univ.-Prof. Dr. Stefan Hulfeld) mit Theaterrepertoires des deutschsprachigen Wandertheaters im 17. und beginnenden 18. Jahrhundert. Dabei werden vorhandene Daten überprüft und neue (Meta-)Daten zu Dramen und deren Aufführungen generiert. Wandertheaterforschung ist aufgrund einer problematischen Quellenlage und -überlieferung ein höchst komplexes Vorhaben. Informationen zu Aktivitäten der Wandertruppen können über administrative Unterlagen, Rechnungsbücher von Städten und Fürstenhöfen, Theaterzettel, Manuskripte und zeitgenössische Schriften gefunden werden. Seit mehr als einem Jahrhundert sammeln und veröffentlichen Wissenschaftler_innen Daten zu deutschsprachigen Wandertruppen. Diese finden sich in Studien zu einzelnen Wandertruppen, in Arbeiten zu Dramensammlungen/Codices, in Untersuchungen zu einzelnen Spielorten sowie in Darstellungen spezifischer Zeiträume. Problematisch ist hier die Anordnung und Auswertung der genannten Quellen: Unzählige Artikel und Bücher müssen herangezogen werden, um Fehler und Missinterpretationen von zuverlässigen Fakten zu unterscheiden. Für die Überprüfung der Daten werden Bibliotheken und Archive konsultiert, da die entsprechenden Unterlagen nicht über Websites verfügbar sind. Die Entwicklung eines webbasierten Archivs, in der die validierten Daten gesichert werden, ist aufgrund der oben beschriebenen Herausforderungen naheliegend.&lt;/p&gt;
&lt;p&gt;Nähere Informationen zum Forschungsprojekt finden Sie unter https://thespis.digital/Forschungsprojekt/.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Beginnend mit einer Darstellung der Ausgangslage des Forschungsprojekts inklusive der im Forschungsfeld existierenden Datenbanken sowie bekannten Vorgangsweisen, welche als Vorbilder auch für unser digitales Archiv impulsgebend waren, werden wir in unserem Vortrag über die Gründe und Argumente für die Entscheidung des Einsatzes von Semantic MediaWiki (SMW) als webbasiertes Werkzeug zur kollaborativen Arbeit referieren. Dabei werden wir Anforderungen an ein webbasiertes Archiv, Möglichkeiten zur Kollaboration, Verwendung von offenen Standards und Formaten ebenso vorstellen wie die für das Projekt so entscheidende Vorgehensweise hinsichtlich der Entwicklung eines Datenmodells in enger Zusammenarbeit mit dem Forschungsteam.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Für die Erfassung der erhobenen und validierten Daten kommt ein Semantic MediaWiki (SMW) zum Einsatz – THESPIS.DIGITAL. Verzeichnet sind Daten und Metadaten zu Repertoirestücken, Aufführungen, Dokumenten, Personen und Datensätze, welche keinem Repertoirestück zuordenbar sind, allerdings wichtige Informationen für die Wandertruppenforschung darstellen. Das zu Grunde liegende Datenmodell wurde in enger Zusammenarbeit mit dem Forschungsteam erarbeitet; die Art der agilen Entwicklung wird in unserem Vortrag erörtert. Zentrale Bedingung für die Erfassung von Dokumenten bzw. Aufführungen ist der Bezug zur Quelle. Einzelne Quellen (Digitalisate von handschriftlichen Dramen, Theaterzettel etc.) sind im digitalen Archiv als Index- und Objektlinks verzeichnet. Digitalisate, die nicht über die Archive direkt im WWW zugänglich sind, werden von uns im Repositorium Phaidra (Digital Asset Management System mit Langzeitarchivierungsfunktion der Universität Wien, https://phaidra.univie.ac.at) abgelegt. Personen und Orte werden als Links zur deutschsprachigen Wikipedia verzeichnet. Das starke Interesse der Forschungsgruppe, Forschungsergebnisse einer interessierten Öffentlichkeit zugänglich zu machen, führt dazu, dass Personenartikel der Schauspieler_innen und Theaterleiter_innen direkt in der Wikipedia angelegt bzw. überarbeitet werden. Durch die so entstehende Verknüpfung mittels Verzeichnissen von Normdaten, über Wikipedia und THESPIS.DIGITAL zu Digitalisaten in Archiven wird eine Kette von Linked Open Data (LOD) hergestellt. Mit dem Blog https://thespis.hypotheses.org treten wir in Kommunikation nach außen und ergänzen so den Austausch mit Wissenschaftler_innen und der Öffentlichkeit. Ein weiterer wichtiger Aspekt beim Wissensaustausch mit Fachkolleg_innen im Feld der Wandertruppenforschung stellt der Einsatz des benutzerfreundlichen Werkzeugs SMW dar. Diese Form der Kollaboration wird verstärkt nach Projektende zu tragen kommen: der Datenbestand wird durch Wissenschaftler_innen aus dem Bereich der Wandertheaterforschung kontinuierlich ausgebaut. THESPIS.DIGITAL wird als zentrales Tool zur Erfassung, Auswertung und Visualisierung von (Aufführungs-)Daten im Zusammenhang mit Wandertheater im 17. und 18. Jahrhundert etabliert.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Bei SMW handelt es sich um eine Open-Source-Erweiterung für MediaWiki. Es stellt ein leistungsfähiges und flexibles Wissensmanagement-System zur Verfügung, in welchem das Speichern und Abfragen von Daten innerhalb von Wiki-Seiten möglich ist. Der entscheidende Vorteil von SMW liegt darin, dass Daten auf mehreren Ebenen mit semantischen Informationen angereichert und erstellte Daten über Semantic Web Standards veröffentlicht werden. Durch die Maschinenlesbarkeit der Daten besteht eine enorme Anschlussfähigkeit für weiterführende Anwendungen und Auswertungen. Die Verwendung offener Standards (Darstellung, Export, Softwarebasis, Dokumentation, Lizenz) fördern eine zukunftssichere Verwendung über das Projektende hinaus. SMW verfügt über eine Benutzer- bzw. Rechteverwaltung zur Unterstützung der Kollaboration durch die Vergabe differenzierter Benutzerrechte. Durch eine umfassende Versionskontrolle kann jeder Bearbeitungsschritt transparent nachvollzogen werden – sowohl bei der Entwicklung als auch bei den Änderungen der inhaltlichen Eingaben.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;In unserem Vortrag skizzieren wir die Ausgangslage des Projekts und Anforderungen an Werkzeuge sowie die Gründe für die Entscheidung zum Einsatz von SMW. Wir werden aber nicht nur auf Stärken und technische Funktionsweisen des Werkzeugs eingehen. Vielmehr wollen wir die Vorgangsweise im Projekt als eine Möglichkeit präsentieren, wie mit dem kritischen Anspruch der Theaterwissenschaft die technologische Entwicklung des Werkzeugs dominiert wird. Dies betrifft insbesondere die Einbeziehung des Forschungsteams in jede Phase des Entstehungsprozesses. Dabei spielt die technologische Basis zwar eine grundlegende Rolle, viel stärker treten allerdings fachwissenschaftliche Diskussionen und Übersetzungsstrategien bei Entscheidungen in den Vordergrund. Mit THESPIS.DIGITAL wurde ein Instrument geschaffen, dass sich in jeder Phase der Entwicklung an den Vorstellungen des Forschungsteams orientiert hat und theoretische Implikationen umsetzt, anstatt bestimmte Möglichkeiten oder Praktiken vorzugeben. Entstanden sind Funktionsweisen und Abläufe die dem Forschungsgegenstand entsprechen. Die Basis für alle Überlegungen hinsichtlich Datenmodell, Funktionsweisen und Darstellung stellen Überlegungen aus einer theaterwissenschaftlichen Perspektive dar. Die Umsetzung erfolgt anhand der Möglichkeiten von SMW, offener Standards und Technologien. In einer Demonstrationsphase geben wir Einblick in die Funktionsweise von THESPIS.DIGITAL und zeigen, wie die Vorteile des Werkzeugs zu tragen kommen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Die am Anfang des Forschungsprojekts formulierten Ziele und Vorstellungen hinsichtlich eines webbasierten Archivs stehen in der Tradition einer klassischen Datenbank und keiner klaren Formulierung von offenen Standards und Technologien. Im Zuge der agilen Entwicklung des Datenmodells und der verwendeten Werkzeuge konnten gemeinsam mit dem Projektteam auch die zu verwendenden Standards erarbeitet und Entscheidung getroffen werden, welche eine sehr offene Verwendung von Daten (sowohl die Projektdaten als auch Daten bspw. von Wikipedia bzw. Wikidata) ermöglicht. Unsere Daten stehen in der Creative Commons Lizenz CC BY-SA 4.0 zur Verfügung und können mittels offener Formate wie CSV, JSON, SPARQL, RDF und API ausgetauscht werden. Dies stellt eine entscheidende Erweiterung der ersten Überlegungen dar und führt zu einer Öffnung. Im Projektteam wurde es kritisch gesehen, dass die meisten „Datenbanken“ keinen offenen Zugang zu den Daten und somit keine automatische (maschinengestützten) Weiterverwendung der Daten ermöglicht. Aus diesem Grund war die Entscheidung, auf offene Standards und Daten zu setzen, nur folgerichtig.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Nicht die technischen Möglichkeiten von SMW standen im Mittelpunkt der Entwicklung von THESPIS.DIGITAL, sondern (zukünftige) wissenschaftliche Fragestellungen und die Frage nach einer, für Wissenschaftler_innen einfache, Möglichkeit, verifizierte Daten in einem digitalen webbasierten Archiv zu speichern und in weiterer Folge Auswertungs- und Visualisierungsmöglichkeiten zu schaffen, welche die Bedürfnisse der jeweiligen Wissenschaftler_innen abdecken können. Durch die Exportmöglichkeit der Daten sind hier wenig Grenzen gesetzt.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Das von uns entwickelte webbasierte, digitale Archiv wollen wir in unserem Vortrag als wirksamen Digitalisierungsprozess beschreiben der von wissenschaftlichen Fragestellungen und nicht von unmittelbaren Möglichkeiten und Praktiken der Technik dominiert ist. In unserem (kleinen) Bereich können wir über Auswirkungen und Rückwirkungen der Digitalität auf Wissenschaft berichten. Von der Definition des Anforderungskatalogs, bei dem noch keine technologischen Überlegungen erfolgten, ausgehend, wurde die erste Beschreibung des Datenmodells aus wissenschaftlichen Anforderungen heraus erstellt. Die Auswahl der technologischen Basis, in unserem Fall SMW, lässt viele Adaptionsmöglichkeiten zu und geht weit über die Anforderungen hinaus. Dies ermöglicht eine hohe Flexibilität bei der Entwicklung des webbasierten Archivs hinsichtlich der Bedürfnisse des Forschungsteams. Im Zentrum der Entwicklung des Datenmodells stand die gemeinsame Erarbeitung mit dem Forschungsteam. Die Inputs hinsichtlich Anpassungen/Änderungen/Erweiterungen des Datenmodells wurden von einer wissenschaftlichen Perspektive geleitet und unabhängig von technologischen Bedingtheiten zur Diskussion gestellt. Durch einen international besetzten Workshop mit Fachkolleg_innen wurde das Datenmodell diskutiert und weitere Vorschläge für Ergänzungen gemacht. Diese Vorgangsweise ermächtigt die wissenschaftlichen Mitarbeiter_innen des Forschungsprojekts, welche die technologischen Möglichkeiten selbstbestimmt einsetzen, ohne sich einer starren Vorgabe von technischer Seite hinzugeben.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>181</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Resch, Claudia
Kampkaspar, Dario
Schopper, Daniel</p3_authors>
  <p3_organisations>Österreichische Akademie der Wissenschaften, Österreich
Österreichische Akademie der Wissenschaften, Österreich
Österreichische Akademie der Wissenschaften, Österreich</p3_organisations>
  <p3_emails>claudia.resch@oeaw.ac.at
dario.kampkaspar@oeaw.ac.at
daniel.schopper@oeaw.ac.at</p3_emails>
  <p3_presenting_author>Resch, Claudia
Kampkaspar, Dario</p3_presenting_author>
  <p3_title>Historische Zeitungen kollaborativ erschließen: Die älteste, noch erscheinende Tageszeitung der Welt “under annotation”</p3_title>
  <p3_abstract>&lt;p&gt;Die digitale Erschließung historischer Zeitungen lag vor wenigen Jahrzehnten noch außerhalb des Vorstellbaren. Inzwischen ist die Zeitung als Forschungsgegenstand etabliert und damit für viele verschiedene Disziplinen ins Zentrum gerückt: NutzerInnen sehen Nachrichtenblätter nicht nur als Quelle, die punktuell und komplementär zu anderen Texten befragt wird, sondern haben auch ein besonderes Interesse an der diachronen Entwicklung von historischen Zeitungen und ihren Themen, von Textsorten und natürlich von Sprache ganz im Allgemeinen. Die Transformation historischer Zeitungen in ein digitales Format, die von Bibliotheken, Archiven und Forschungseinrichtungen in den letzten Jahren stark vorangetrieben worden ist, befördert Fragestellungen dieser Art und überlässt den geistes- und sozialwissenschaftlichen Disziplinen einen reichen Schatz an Daten.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Im Zuge des wachsenden Interesses an historischem Zeitungsmaterial in digitaler Form haben Bibliotheken eigene Portale mit Bild-Digitalisaten und Kalenderübersichten eröffnet  (vgl. etwa die Staats- und Universitätsbibliothek Bremen&lt;sup&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/sup&gt;, die Staatsbibliothek zu Berlin&lt;sup&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/sup&gt;, die Universität Bonn&lt;sup&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/sup&gt; oder die Österreichische Nationalbibliothek&lt;sup&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/sup&gt;) oder stellen ihre Daten Europeana&lt;sup&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/sup&gt; zur Verfügung. Um historische Zeitungen besser durchsuchbar zu machen, wird vereinzelt bereits an der sorgfältigen Volltexterschließung besonderer historischer Zeitungen gearbeitet, vgl. etwa das Projekt „Volltextdigitalisierung der Staats- und Gelehrte[n] Zeitung des Hamburgischen Unpartheyischen Correspondenten und ihrer Vorläufer (1712-1848)“&lt;sup&gt;&lt;sup&gt;[6]&lt;/sup&gt;&lt;/sup&gt; (Schuster, Wille 2016: 7-29) oder das “Mannheimer Korpus für Historische Zeitungen und Zeitschriften in COSMAS II”&lt;sup&gt;&lt;sup&gt;[7]&lt;/sup&gt;&lt;/sup&gt;. Internationale Vernetzungsinitiativen&lt;sup&gt;&lt;sup&gt;[8]&lt;/sup&gt;&lt;/sup&gt; machen einerseits auf die Herausforderungen bei der Erschließung historischer Zeitungen aufmerksam (enorme Textmenge, fehlerhafte OCR-Ergebnisse, wenig Trainingsdaten) und lassen andererseits die Entwicklung gemeinsamer Strategien und Standards bei der Aufbereitung erkennen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Die AutorInnen des vorliegenden Beitrags gehen davon aus, dass historische Zeitungen von Institutionen zwar “digital verfügbar” gemacht werden, aber bislang kaum an die Erkenntnisinteressen potentieller NutzerInnen angepasst sind - auch, aber nicht nur aufgrund der oben genannten Herausforderungen und des erheblichen Aufwandes, der mit der Erschließung einer historischen Zeitung verbunden ist. Bevor ein aufwändiges Projekt startet, ist daher zu überlegen, wie eine Erschließung geplant sein muss, sodass sie idealerweise für mehrere Disziplinen von Nutzen ist. Bei der Konzeption ist darauf zu achten, dass keine (für den überwiegenden Teil der antizipierten Disziplinen) relevanten Informationen vernachlässigt werden oder verloren gehen: Die Entscheidungen, die zu treffen sind, beginnen (1) bei  der Auswahl der Ausgaben, setzen sich (2) bei den Transkriptionsrichtlinien fort und lassen sich (3) bis zu den Annotationskonzepten weiterführen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Anhand eines laufenden Projektes, das sich der Volltextdigitalisierung ausgewählter Nummern des “Wien[n]erischen Diariums”&lt;sup&gt;&lt;sup&gt;[9]&lt;/sup&gt;&lt;/sup&gt; aus dem 18. Jahrhundert widmet, sollen all diese Aspekte kritisch hinterfragt und anhand von Beispielen, Erfahrungswerten und Zwischenergebnissen dargestellt werden. Das methodische Konzept, auf dem dieses konkrete Vorhaben beruht, wurde ausgehend von der Überzeugung gestaltet, dass insbesondere bei einer vielseitig nutzbaren Ressource wie dem “Wiennerischen Diarium”&lt;em&gt;&lt;/em&gt;die einzelnen Fachdisziplinen und NutzerInnen bereits möglichst früh in den interdisziplinären Erschließungsprozess einzubeziehen sind. Im Vortrag soll darüber berichtet werden, welche Maßnahmen bereits gesetzt wurden, um die Fachwissenschaften zu vernetzen, welche Tools im Projekt entwickelt wurden, um Personen außerhalb des Kernteams zu involvieren, und durch welche digitalen Angebote diese Kollaboration ermöglicht wird und gelingen kann.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Textauswahl:&lt;/p&gt;
&lt;p&gt;Um ein ausgewogenes Korpus von mehreren hundert Ausgaben verteilt über das 18. Jahrhundert zu erstellen, waren sowohl ExpertInnen mehrerer geisteswissenschaftlicher Disziplinen als auch LeserInnen der heutigen Wiener Zeitung dazu aufgefordert, jene Nummer(n) online zu nominieren, die sie als besonders relevant einstufen würden. Bei der Auswertung dieses Calls hat sich erneut bestätigt, wie breit das Themenspektrum und damit die individuellen Erkenntnisinteressen sind: Nominiert und zur Volltextdigitalisierung empfohlen wurden Ausgaben mit Geburten, Taufen und Sterbefällen bekannter Persönlichkeiten, Geburts- und Namenstage, Krönungen und Erbhuldigungen, kirchliche und weltliche Feste, Ankündigungen, Eröffnungen und Einweihungen sowie die Besuche prominenter Gäste in der Residenzstadt. Errungenschaften im weitesten Sinn – wie die Erklärung der Menschenrechte oder der Beginn der Luftfahrt – waren ebenso in der Auswahl wie das medienimmanente Thema der Herausgeberschaft. Die Ergänzungen, die das Projektteam letztlich vornehmen musste, betrafen daher weniger die Vielfalt der angesprochenen Themen, sondern waren darauf ausgerichtet, zeitliche Lücken zu füllen. Um das Wien[n]erische Diarium als Korpus in einer kontinuierlich chronologischen Jahresabfolge plausibel dokumentieren zu können, wurden die Nominierungen dahingehend komplettiert, dass sich die Umbrüche und Wendungen in einer Periode sich verändernder politischer, sozialer, wissenschaftlicher und künstlerischer Bedingungen im 18. Jahrhundert, idealerweise auch korpusgestützt nachvollziehen lassen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Texterstellung und -transkription:&lt;/p&gt;
&lt;p&gt;Ausgangspunkt für die Weiterverarbeitung sind jene Image-Digitalisate, welche die Österreichische Nationalbibliothek in ANNO (Austrian Newspapers Online) zur Verfügung stellt.&lt;sup&gt;&lt;sup&gt;[10]&lt;/sup&gt;&lt;/sup&gt; Die Erstellung des digitalen Textes für die ausgewählten Nummern erfolgt mit Transkribus&lt;sup&gt;&lt;sup&gt;[11]&lt;/sup&gt;&lt;/sup&gt;, genauer mittels der Handwritten Text Recognition (HTR). Anhand einer kleinen Zahl an Ausgaben, die im Rahmen von projektbezogenen Vorstudien (vgl. Resch et al. 2016) bearbeitet wurden, konnte ein erstes Modell für das Diarium erstellt werden. Je nach Beschaffenheit der Digitalisate liegt die Genauigkeit hiermit zwischen 70% und 95%. Um die Qualität zu steigern, werden mehrere Tranchen von 40 bis 50 Ausgaben von zwei unterschiedlichen externen Dienstleistern nach den für dieses Projekt erstellten Transkriptionsrichtlinien erfasst, sodass das Modell weiter trainiert  und die Erfassungsgenauigkeit erhöht werden kann. Ein “Reporting-Tool”&lt;sup&gt;&lt;sup&gt;[12]&lt;/sup&gt;&lt;/sup&gt;, das direkt auf die Transkribus-Plattform zugreift, dokumentiert den Bearbeitungsstatus der ausgewählten Einzelnummern, gibt Auskunft über deren Umfang (Zählung von Regionen, Zeilen und Wörtern) und informiert die Fachgemeinschaft tagesaktuell über den Fortschritt des Projekts.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Annotationskonzepte:&lt;/p&gt;
&lt;p&gt;Parallel dazu arbeitet das Kernteam an einer digitalen Arbeits- und Annotationsoberfläche: Für die Präsentation der Texte wird nach derzeitigem Projektstand eine auf eXist basierende Umgebung genutzt, die auch in anderen Projekten des Instituts und in anderen Institutionen Anwendung findet. In die HTML-Präsentation integriert ist die Möglichkeit zur Annotation des Textes. Dabei kann entweder der Text korrigiert, eine Entität ausgezeichnet und/oder identifiziert oder eine Volltextanmerkung geschrieben werden. Die Möglichkeiten der Annotation, die Modellierung der annotierten Entitäten, ihre Verwaltung und Darstellung sollen im Rahmen eines “Annotatathons” in Zusammenarbeit mit den NutzerInnen weiterentwickelt werden. Für das Projektteam ist es etwa wichtig zu erfragen, welche unterschiedlichen Sichtweisen es seitens der verschiedenen Disziplinen auf den Text gibt, welche Aspekte bei der Erschließung von besonderer Relevanz sind oder ob es so etwas wie einen “kleinsten, gemeinsamen Nenner” aller Annotationskonzepte geben kann. Die Einbeziehung von ExpertInnenen im Annotationsprozess ist dem Projektteam ein besonderes Anliegen: Es begreift das Annotieren als höchst anspruchsvolle Forschungsleistung, die ein profundes historisch-kulturelles Wissen bei der Beurteilung erfordert und für eine Quelle wie dem “Wiennerischen Diarium” bei genauerer Betrachtung eigentlich nur gemeinsam erbracht werden kann. Ein kollaboratives Annotieren technisch vorzusehen und zu ermöglichen, ist hierbei die besondere Herausforderung. Die dafür entstehende benutzerfreundliche Präsentations- wie auch die Annotationsumgebung wird unter einer freien Lizenz zur Nachnutzung zur Verfügung gestellt werden.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/sup&gt; Vgl. https://www.suub.uni-bremen.de/ueber-uns/projekte/alte-zeitungen/&lt;u&gt;&lt;/u&gt;[letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/sup&gt; Vgl. http://zefys.staatsbibliothek-berlin.de/&lt;u&gt;&lt;/u&gt;[letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/sup&gt; Vgl. http://digitale-sammlungen.ulb.uni-bonn.de/ulbbnz/date/list/229854&lt;u&gt;&lt;/u&gt;[letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/sup&gt; Vgl. http://anno.onb.ac.at/&lt;u&gt;&lt;/u&gt;[letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/sup&gt; Vgl. http://www.europeana-newspapers.eu/&lt;u&gt;&lt;/u&gt;[letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[6]&lt;/sup&gt;&lt;/sup&gt; Vgl. https://kw.uni-paderborn.de/institut-fuer-germanistik-und-vergleichende-literaturwissenschaft/germanistische-und-allgemeine-sprachwissenschaft/schuster/forschung/projekte/der-hamburgische-unpartheyische-correspondent-volltextdigitalisierung/&lt;u&gt;&lt;/u&gt;[letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[7]&lt;/sup&gt;&lt;/sup&gt; Vgl. http://repos.ids-mannheim.de/mkhz-beschreibung.html&lt;u&gt;&lt;/u&gt;[letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[8]&lt;/sup&gt;&lt;/sup&gt; Vgl. etwa die Einrichtung einer “Special Interest Group Newspapers” bei der TEI-Konferenz 2016 in Wien, das CLARIN-Vernetzungstreffen "Working with Digital Collections of Newspapers" 2016 in Leuven oder das “Transatlantic Digitised Newspaper Symposion” 2017 in London.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[9]&lt;/sup&gt;&lt;/sup&gt; Das “Wien[n]erische Diarium” ist am 8. August 1703 erstmals erschienen, als “Wiener Zeitung” bis heute erhältlich und somit die älteste, noch existierende Tageszeitung der Welt. Insbesondere im 18. Jahrhundert lässt sich die Entwicklung der Zeitung von den Anfängen des modernen Journalismus in einer spannenden Zeit und unter sich verändernden politischen, sozialen und künstlerischen Bedingungen gut nachverfolgen und mitvollziehen.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[10]&lt;/sup&gt;&lt;/sup&gt; Vgl. http://anno.onb.ac.at/cgi-content/anno?aid=wrz [letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[11]&lt;/sup&gt;&lt;/sup&gt; Vgl. https://transkribus.eu/Transkribus/ [letzter Zugriff 22. September 2017]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[12]&lt;/sup&gt;&lt;/sup&gt; Vgl. https://www.oeaw.ac.at/acdh/projects/wienerisches-diarium-digital/ [letzter Zugriff 22. September 2017]&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>129</session_ID>
  <session_short>VP_2d</session_short>
  <session_title>Visualisierung II</session_title>
  <session_start>2018-02-28 11:00</session_start>
  <session_end>2018-02-28 12:30</session_end>
  <session_room_ID>1</session_room_ID>
  <session_room>Hörsaal B, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Münster, Sander</chair1>
  <attendee_count>6</attendee_count>
  <chair1_name>Sander Münster</chair1_name>
  <chair1_organisation>Medienzentrum/TU Dresden</chair1_organisation>
  <chair1_email>sander.muenster@tu-dresden.de</chair1_email>
  <chair1_ID>1402</chair1_ID>
  <sessionID>129</sessionID>
  <presentations>3</presentations>
  <p1_paperID>186</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Frank, Ingo</p1_authors>
  <p1_organisations>Leibniz-Institut für Ost- und Südosteuropaforschung (IOS)</p1_organisations>
  <p1_emails>frank@ios-regensburg.de</p1_emails>
  <p1_presenting_author>Frank, Ingo</p1_presenting_author>
  <p1_title>Digitale Vernunft zwischen Text und Diagramm</p1_title>
  <p1_abstract>&lt;p&gt;Mit meinem Vortrag möchte ich zu einer Kritik der digitalen Methoden beitragen. Gegenwärtige Forschung im Bereich der Digital Humanities konzentriert sich vorwiegend auf den Einsatz quantitativer digitaler Methoden. Ich betrachte diesen Schwerpunkt auf ‘Big Data’, ‘Distant Reading’ und die Beschränkung auf statistische Verfahren zur Erklärung geisteswissenschaftlicher Phänomene kritisch. In einer methodologischen Kritik der digitalen Geisteswissenschaften werde ich dazu aus wissenschaftstheoretischer Perspektive anhand Beispielen historischer Forschung die Grenzen der quantitativen Ansätze aufzeigen, um dann Möglichkeiten vorzustellen, wie qualitative Methoden im Sinne eines &lt;em&gt;Augmenting Human(ist) Intellect&lt;/em&gt;-Ansatzes (Engelbart, 1962) digital unterstützt und erweitert werden sollten.&lt;/p&gt;
&lt;p&gt;Als erstes werde ich die Methoden der Digital Humanities mit Hilfe der NeMO (NeDiMAH Methods Ontology) wissenschaftstheoretisch verorten (Abb. 1). NeMO dient als Rahmen zum Aufbau einer Taxonomie digitaler Methoden und Werkzeuge. Ein erster Schritt dazu ist die Unterscheidung zwischen quantitativen und qualitativen Methoden. Da diese Unterscheidung auf methodischer Ebene und nicht etwa auf epistemologischer oder theoretischer Ebene erfolgt, erscheint die gängige scharfe Trennung in quantitative und qualitative Forschung nicht gerechtfertigt. Auf Ebene der Epistemologie muß man bei einer bestimmten erkenntnistheoretischen Position bleiben und kann dabei qualitative und quantitative Methoden kombinieren, ohne daß das methodologisch problematisch wäre (Crotty, 1998). Bei digitaler Forschung kommt es auf den Ansatz der formalen Modellierung von Forschungsgegenständen an. Formale Modellierung bildet den Kern der Digital Humanities und ist Voraussetzung um überhaupt digital transformierte Foschungsmethoden einsetzen zu können – egal ob quantitativ oder qualitativ. Man könnte daher anstatt von &lt;em&gt;digitalen&lt;/em&gt; Methoden besser von &lt;em&gt;formalen&lt;/em&gt; bzw. &lt;em&gt;formalisierbaren&lt;/em&gt; Methoden sprechen. Im Vortrag werde ich mich auf ‘Digital Mapmaking’ als Methode konzentrierten, die den Einsatz von Diagrammen zur qualitativer Forschung umfaßt. Um den Mehrwert von diagrammatischer Darstellung zu zeigen, werde ich auf Beispiele aus der Sozialgeschichte zurückgreifen. Skocpols “States and Revolutions: A Comparative Analysis of France, Russia, and China” (Skocpol, 1979) eignet sich sehr gut, um zu zeigen, daß Text nicht ausreicht, um die komplexen kausalen Narrative, die zu sozialen Revolutionen führen, gut nachvollziehbar zu repräsentieren. In der methodologischen Forschung dazu wird sogar argumentiert (Mahoney, 1999; George und Bennett, 2005), daß die Darstellung der kausalen Prozesse in diagrammatischer Form notwendig ist, um die kausalen Zusammenhänge in der Präsentation der Forschungsergebnisse explizit zu machen und darüber hinaus den Forschungsprozess selbst durch den Zwang zur Formalisierung zu unterstützen. Am Beispiel Skocpols vergleichender Studie werde ich demonstrieren, wie die Methoden QCA (Qualitative Comparative Analysis) und Process Tracing in den Digital Humanities durch den Einsatz von geeigneten diagrammatischen ‚Denkwerkzeugen‘ wie Hypertext-Karten, Fuzzy Cognitive Maps und Dynamischer Netzwerkanalyse unterstützt und erweitert werden können.&lt;/p&gt;
&lt;p&gt;Um überhaupt sinnvoll von ‚digitalen‘ Methoden reden zu können, muß der Aspekt der formalen Modellierung als zentral erachtet werden. Formalisierung ist die Voraussetzung für die „Mechanisierung angeblich geistiger Tätigkeiten“ (Brauer et al., 1989). Piotrowski (2016) definiert Digital Humanities wie folgt: “The digital humanities study the means and methods of constructing formal models in the humanities.” Ein Modell ist dabei als Repräsentation eines geisteswissenschaftlichen Untersuchungsgegenstands zu verstehen und ‚formal‘ bedeutet &lt;em&gt;logisch kohärent&lt;/em&gt;, &lt;em&gt;nicht mehrdeutig&lt;/em&gt; und &lt;em&gt;explizit&lt;/em&gt; (Piotrowski, 2016).&lt;/p&gt;
&lt;p&gt;Mir geht es im folgenden um digitale Methoden des ‘Mapmaking’:&lt;/p&gt;
&lt;blockquote&gt;[Mapmaking] is combinatorial method, for it involves mapping information gathered from other sources: generally secondary data or surveys, though texts are another possibility. It must thus reflect the strengths and weaknesses of its sources of data. As with hermeneutics, mapmaking involves a set of techniques beyond those involved in the methods it draws upon. Its obvious strength lies in terms of spatiality. There are, quite simply, some spatial movements that cannot be properly visualized nor comprehended without recourse to maps […] (Szostak, 2004)&lt;/blockquote&gt;
&lt;p&gt;Dazu kann mit NeMO Quellen- und Archivmaterial, das zur Erstellung von Karten herangezogen wird, formal erfaßt werden, was im Kontext von Digital Libraries und Forschungsdatenmanagement relevant ist.&lt;/p&gt;
&lt;p&gt;Einführen werde ich &lt;em&gt;Digital Mapmaking&lt;/em&gt; mit einem für die Geschichtswissenschaft naheliegenden Diagrammtyp: Zeitleisten. Synchronoptische Visualisierungen (Abb. 2) dienen der Kontextualisierung großer Datenmengen aus verschiedenen Quellen und ermöglichen die Generierung von Hypothesen durch Abduktion (Frank, 2018). Eine Erklärung – z. B. in Form der Beschreibung eines sozialen Mechanismus, der ein historisches Ereignis hervorbringt – kann jedoch nur durch weiterführende Forschung gefunden werden.&lt;/p&gt;

&lt;p&gt;Im weiteren Verlauf werde ich zeigen, wie Visualisierungsansätze nicht nur &lt;em&gt;explorative &lt;/em&gt;Analyse unterstützen können, sondern wie sie darüber hinaus auch die Möglichkeiten zur &lt;em&gt;Erklärung&lt;/em&gt; historischer Ereignisse erweitern können. Sehr gut veranschaulicht werden kann ein solcher Ansatz mit den diagrammatischen Darstellungen der kausalen Narrative in Skocpols Theorie sozialer Revolutionen (Skocpol, 1979).&lt;/p&gt;
&lt;p&gt;Goertz und Mahoney (2005) argumentieren, daß “a failure to appropriately conceptualize levels and relationships between levels” zu vielen Fehlinterpretationen von Skocpols Theorie geführt haben. George und Bennett (2005) empfehlen tatsächlich “diagrams to present clearly the argument of causal narratives, to make the causal claim more explicit”. Um die laut Goertz und Mahoney (2005) oft in den Interpretationen vorkommende Verwechslung von Kausalität und Konstitution zu vermeiden, eignet sich zur Darstellung von &lt;em&gt;Two-Level Theories&lt;/em&gt; der Einsatz von Diagrammen (Abb. 3):&lt;/p&gt;
&lt;blockquote&gt;For example, the examination of an ontological relationship between levels allows the analyst to explore the specific defining properties of the basic-level concepts that actually affect the outcome of interest. In this case of an ontological relationship, the specific properties identified in the secondary level are “mechanisms” that explain why the basic-level variables have the effects they do. (Goertz und Mahoney, 2005)&lt;/blockquote&gt;
&lt;p&gt;Im Rückentext des Geschichtstheorie-Lehrbuchs von Kolmer (2008) steht dazu treffend: „Wer sich nicht von der Beredsamkeit der Historiker blenden lassen will, muss das Gerüst entdecken können, das ihre Erzählungen trägt.“ Meine Idee ist nun, die kausalen Narrative nicht nur als Diagramme zu visualisieren, sondern die historischen Narrative der komplexen kausalen Zusammenhänge als Hypertext zu modellieren. Der Mehrwert dieses &lt;em&gt;Hypertext Mapping&lt;/em&gt;-Ansatzes wird durch einen weiteren Verweis auf Szostak (2004) deutlich: “Mapmaking can give unique insight into spatial and temporal aspects of causal systems (among other things).”&lt;/p&gt;
&lt;p&gt;Offensichtlich bietet die diagrammatische Visualisierung (Abb. 4) von Mahoney (1999) deutliche Vorteile bei der Darstellung der komplexen kausalen Zusammenhänge und erleichtert dadurch das genaue Nachvollziehen und Verstehen der historischen Vorgänge, die zum Zusammenbruch des französischen Staates führten, erheblich.&lt;/p&gt;
&lt;p&gt;Allerdings berücksichtigt das Diagramm nach Mahoney (1999) keine komplexen Rückkopplungsschleifen zwischen Ereignissen und außerdem fehlt die Darstellung der Gewichtung einzelner Ereignisse und die Möglichkeit, die Details der kausalen Prozesse zu erfassen. Diese vernachlässigten Aspekte könnten durch Causal Loop Diagrams (siehe Beispiel in Abb. 5) oder durch Fuzzy Cognitive Maps oder auch Dynamische Netzwerkanalyse modelliert werden.&lt;/p&gt;
&lt;p&gt;Die ersten fünf von Mahoney (1999) aus Skocpol (1979) extrahierten Bedingungen, sollen zur Veranschaulichung einer zusätzlichen Möglichkeit von Hypertext Maps ausreichen:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;
&lt;p&gt;Property relations prevent introduction of new agricultural techniques (S. 55)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tax system discourages agricultural innovation (S. 55)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sustained growth discourages agricultural innovation (S. 55)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backwardness of French agriculture (esp. vis-à-vis England) (S. 56)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weak domestic market for industrial goods (S. 55–56)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Die Kausalkette von 4 nach 5 wird von Skocpol (1979) z. B. mit einer Reihe von historischen Studien belegt. Diese Verweise können in einem Hypertext-Narrativ mit typisierten Links explizit gemacht werden.&lt;/p&gt;
&lt;p&gt;Wie ein kritischer Rückblick in die Geschichte der Hypertext-Forschung zeigt, sind wesentliche Anforderungen an Hypertext-Systeme bisher immer noch nicht zufriedenstellend erfüllt. Ich werde zeigen, wie die sieben offenen Punkte/Issues von Halasz (2001) mit aktueller Semantic Web- und Informationsvisualisierungstechnologie angegangen werden können. Suche unter Berücksichtigung der Hypertextstruktur (Issue 1) kann auf Basis von Ontologie-basiertem Hypertext im RDF-Datenmodell realisiert werden. Berechnungen aufgrund der Hypertextstruktur (Issue 4) könnten durch den Einsatz von Dynamischer Netzwerkanalyse und Fuzzy Cognitive Maps (Carvalho, 2012) umgesetzt werden.&lt;/p&gt;
&lt;p&gt;Die ‚digitale Vernunft‘ erfordert es schließlich, daß geisteswissenschaftliches &lt;em&gt;Erklären&lt;/em&gt; bzw. &lt;em&gt;Verstehen&lt;/em&gt; in einem angemessen methodologischen und ontologischen Rahmen abläuft. Mechanistische Erklärung bietet ein explanatorischen Rahmenwerk, um z. B. die verschiedenen Ismen der Politikwissenschaft zu vereinen (Bennett, 2013).&lt;/p&gt;
&lt;p&gt;Die Relevanz von Digital Mapmaking als Hilfsmittel zur Erklärung historischer Ereignisse zeigt sich insbesondere beim Einsatz von Hypertext Maps zur Strukturierung von historischen Narrativen. Historische Narrative können dabei durchaus als mechanistische Erklärungen historischer Ereignisse aufgefaßt werden (Glennan, 2010). Analog zum Periodensystem chemischer Elemente kann ein diagrammatischer Hypertext auf Lücken im theoretisch fundierten Gerüst eines kausalen Narrativs hinweisen, um diese durch Methoden wie Process Tracing zu füllen. Zusätzlich zu mechanistischer Erklärung unterstützt Hypertext multiperspektivische Erklärung historischer Ereignisse (Shaw, 2013; Jensen, 2013).&lt;/p&gt;
&lt;p&gt;Die Ergebnisse bringt schließlich ein erneuter Blick in die Hypertext-Geschichte auf den Punkt: “Linearity was never an option for historical writing; hypertextuality can make complex structure concrete, clear and responsive to both the presentationAuthor and the reader.” (Eastgate Systems, 2005)&lt;/p&gt;
&lt;p&gt;Etablierte qualitative Methoden wie QCA, Process Tracing und Netzwerkanalyse (Haug, 2008) werden in den Digital Humanities vernachlässigt oder, wie Netzwerkanalyse, nicht kritisch genug eingesetzt – im Sinne von zu wenig formaler Modellierung (Lemercier, 2015). Der Einsatz dieser Methoden im Bereich der Digital Humanities setzt die formale Modellierung der geisteswissenschaftlichen Untersuchungsgegenstände voraus.&lt;/p&gt;
&lt;p&gt;Die Rolle von formaler Ontologie für die Digital Humanities beim Aufbau von Regionalontologien für die Geisteswissenschaften (Gnoli, 2008) und der Repräsentation von Wissen aus verschiedenen diziplinären Perspektiven und Kontexten wird an dieser Stelle deutlich (Frank, 2017). Bisher gibt es nur wenig Ansätze, die die Komplexität der geisteswissenschaftlichen Realität dabei angemessen berücksichtigen (Grossner, 2010; Fokkens et al., 2016).&lt;/p&gt;
&lt;p&gt;Im Vortrag werden abschließend erste eigene formal-ontologische Modellierungsbeispiele und diagrammatische Visualisierungen kausaler Narrative historischer Ereignisse gemäß dem vorgestellten Ansatz präsentiert und offene Probleme und Fragen zur Diskussion gestellt.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>283</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Dörk, Marian
Glinka, Katrin</p2_authors>
  <p2_organisations>Fachhochschule Potsdam, Deutschland
Stiftung Preußischer Kulturbesitz, Deutschland</p2_organisations>
  <p2_emails>doerk@fh-potsdam.de
k.glinka@smb.spk-berlin.de</p2_emails>
  <p2_presenting_author>Dörk, Marian
Glinka, Katrin</p2_presenting_author>
  <p2_title>Der Sammlung gerecht werden: Kritisch-generative Methoden zur Konzeption experimenteller Visualisierungen</p2_title>
  <p2_abstract>&lt;p&gt;Einleitung&lt;/p&gt;
&lt;p dir="ltr"&gt;Das Versprechen hinter Digitalisierungsprojekten in sammelnden Institutionen ist oft die Erweiterung des Zugangs zum kulturellen Erbe, sei es für die Forschung oder im Sinne der Vermittlung. Bei kritischer Betrachtung fast aller Benutzerschnittstellen für Sammlungen scheint es aber an Ansätzen zu fehlen, reichhaltige Informationsräume einladend bereitzustellen. Diese Diskrepanz zwischen Digitalisierung von Sammlungen und ihrer digitalen Verfügbarmachung lässt sich dadurch begründen, dass Kultureinrichtungen selten über die nötige Kapazität verfügen, eigenständig Benutzerschnittstellen zu konzipieren und umzusetzen. Die Zielstellung des dreijährigen Forschungsprojektes Visualisierung kultureller Sammlungen (VIKUS) an der Fachhochschule Potsdam war die Erforschung graphischer Benutzerschnittstellen zur explorativen Sichtung von Kulturobjekten. Im Projekt wurden in Kooperation mit Kultur- und Technologiepartnern Erkenntnisse zur visuellen Exploration digitalisierter Sammlungen gewonnen (Glinka et al. 2017a), die Entwicklung nachhaltiger Technologielösungen behandelt (Glinka et al. 2017b) und in experimentellen Settings kritisch-generative Methoden erprobt. Zu letzterem zählt die Übertragung der Forschungsfragen in den Kontext eines interdisziplinären Lehrformats, welches wir in unserem Beitrag diskutieren. Wir gehen insbesondere auf den produktiven Zusammenhang zwischen Kritik und Konzeption von Informationsvisualisierungen ein und zeigen auf, welche Potenziale ein bewusster Bruch mit disziplinären Konventionen und Sehgewohnheiten mit sich bringt.&lt;/p&gt;
&lt;p&gt;Herangehensweise&lt;/p&gt;
&lt;p dir="ltr"&gt;Mit der Digitalisierung von Sammlungen erhält eine breite Öffentlichkeit Zugriff auf zahlreiche Kulturobjekte, welche zuvor hauptsächlich für Wissenschaftler*innen zugänglich waren. Dieser Zugriff basiert häufig auf digitalisierten Museumskatalogen oder Archivsystemen, welche ursprünglich der Ortung physischer Originale und nicht als eigenständige “Repräsentation” der Sammlungsobjekte dienten. Das Forschungs- und Lehrprojekt VIKUS stellte sich der Frage, wie die digitale Repräsentation als eine für sich stehende Perspektive auf Sammlungen zu begreifen sein könnte und welche Interfacekonzepte diese unterstützen würden. Hier eröffnet sich die Gelegenheit, die Stärken des Digitalen bei der Bereitstellung kultureller Sammlungen zu berücksichtigen. So kann zum Beispiel die vergleichsweise statische Anordnung in Ausstellungen in digitalen Benutzeroberfllächen mittels dynamischer Arrangements durchbrochen werden. Obwohl die Auswahl und Anordnung von Objekten bei der Gestaltung von Ausstellungsräumen große Aufmerksamkeit erfährt, werden diese Überlegungen im digitalen Kontext noch vernachlässigt. Vor kultureller Intention stehen zumeist technische Konventionen, die der Bedeutung der Sammlung kaum entsprechen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Der Ansatz dieses Projekts liegt in der Verknüpfung technologischer Möglichkeiten mit kulturwissenschaftlichen Überlegungen, um kritisch-generative Methoden zur Visualisierung zu entwickeln, welche alternative Perspektiven auf Kultursammlungen eröffnen. Dabei sind solche Visualisierungen ebenso als Kulturartefakte zu betrachten, die es in ihrer Funktion nicht nur zu konstruieren, sondern ebenso zu kritisieren gilt, wobei der »performative Charakter der Interpretation« (Drucker 2013) insbesondere bei Humanities Interfaces relevant wird. Entlang der für das Projekt zentralen disziplinären Perspektiven—Interface Design, Information Retrieval und Digital Humanities—haben sich drei Fragestellungen herausgebildet:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;1.) Wie können Visualisierungen der kulturellen Signifikanz einer Sammlung gerecht werden? Bislang orientieren sich digitale Zugänge zu Kultursammlungen eher an technischen Gegebenheiten von Datenbanklösungen als an der kulturellen Bedeutung der Objekte. So werden diese herkömmlicherweise in einer tabellarischen Auflistung in Anlehnung an einen Leuchttisch gezeigt. Statt solche Standardlösungen auf alle Arten von Sammlungen anzuwenden, untersuchen wir, inwiefern die spezifischen Eigenschaften einer Sammlung reflektiert werden können.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;2.) Wie kann offenes Stöbern in komplexen Informationsräumen angeregt werden? Herkömmliche Sammlungsinterfaces sind auf gezielte Suche mit expliziter Anfrageformulierung ausgerichtet, was eine von Neugier getriebene Exploration erschwert. Als Gegenentwurf zu solch „geizigen” Zugängen sind „freigebige” Oberflächen vonnöten (Whitelaw 2015), die durch visuelle Arrangements der Sammlungsobjekte entlang ihrer Facetten die Benutzer*innen zum Stöbern einladen. Diesem Anspruch folgend entwickelt das VIKUS-Projekt Szenarien für die offene Exploration digitaler Sammlungen weiter.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;3.) Wie können digitale Methoden die Analyse visueller Sammlungen unterstützen? Als Ergänzung zur Forschung an textbasierten Quellen in den Digital Humanities widmen wir uns reichhaltigen Sammlungen, in denen bildliche Aspekte einen wichtigen Stellenwert einnehmen und erproben die Übertragung des Konzepts des “Distant Readings” (Moretti 2005) auf andere Kulturwissenschaften. Über das interessierte Stöbern hinaus kann Visualisierung auch für Expert*innen die Sichtung von Kulturobjekten und deren Analyse entlang verschiedener Fragestellungen unterstützen (Yamaoka et al. 2011).&lt;/p&gt;
&lt;p&gt;Interdisziplinäre Projektkurse&lt;/p&gt;
&lt;p dir="ltr"&gt;Um diesen Fragestellungen nachzugehen, wurde im Laufe des Forschungsprojekts die Herangehensweise des iterativen Designs verfolgt. Dazu zählen der Einsatz von Co-Creation, stufenweise Ideenentwicklung im interdisziplinären Austausch und das Durchführen von Nutzerstudien (Glinka et al. 2017a). Insbesondere durch die Einbindung von Studierenden und Projektpartnern im Rahmen von Workshops (Chen et al. 2014) und Projektkursen hat sich das Spektrum an Erkenntnissen signifikant erweitert. Im Folgenden gehen wir auf das Kursformat ein und umreißen die Ergebnisse.&lt;/p&gt;
&lt;p&gt;Methodik&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Zielstellung des interdisziplinären Projektkurses, welcher seit 2014 bereits drei mal stattfand, folgt der umrissenen Herangehensweise des Forschungsprojektes. In interdisziplinären Teams erforschen fortgeschrittene Studierende aus Design, Kulturarbeit, Europäischer Medienwissenschaft und Informationswissenschaften innovative Darstellungsformen zur explorativen Sichtung von Sammlungen. Kursteilnehmer*innen verfügen über Grundlagen und Erfahrungen in mindestens einem der Kernthemen des Kurses—Informationsvisualisierung und Kultursammlungen—und haben Interesse an dem jeweilig anderen. Der Kurs verfolgt die Ideen des fächerübergreifenden »Forschenden Lernens«, indem aktuelle Forschungsfragen das Kursthema motivieren und Interdisziplinarität neue Erkenntnisse verspricht.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Die Studierenden nähern sich dem Kursthema aus ihrem disziplinären Hintergrund, entwickeln selbständig Fragestellungen und bringen ihre fachliche Perspektive ein. Die Kursprojekte werden in Teams von 2-4 Studierenden bearbeitet, welche jeweils aus verschiedenen Studiengängen kommen müssen. Es findet anfangs eine kritische Auseinandersetzung mit existierenden Sammlungszugängen statt: Durch die Analyse von Sammlungsinterfaces wird ein kritisches Bewusstsein geschult, welches den Studierenden bei ihrer eigenen Projektentwicklung zu Gute kommt. Als Datengrundlage stehen dem Kurs eine Auswahl an digitalisierten Beständen zur Verfügung, welche die Kooperationspartner zur Verfügung stellen. Zu den Datensätzen zählten bislang u.a. Sammlungen der Stiftung Preußische Schlösser und Gärten (SPSG), der Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW), des Syrian Heritage Archives, des Deutschen Forums für Kunstgeschichte Paris (DFK) und des Münzkabinetts der SMB (siehe https://uclab.fh-potsdam.de/vikus/). &lt;/p&gt;
&lt;p dir="ltr"&gt;Die Wahl der Themen, Fragestellungen und Methoden erfolgt durch die Teilnehmer*innen in Absprache mit dem Projektteam. Anstelle von frontaler Wissensvermittlung wird versucht, einen konstruktiven und kritischen Möglichkeitsraum für Forschung und Studium aufzuspannen. Dafür werden auch Professor*innen aus den Fachbereichen in den Kurs eingeladen, um Feedback bei Zwischen- und Endpräsentationen zu geben.&lt;/p&gt;
&lt;p&gt;Ergebnisse&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Kursergebnisse reichen von Visualisierungskonzepten über Studien zur Wireframe-Analyse von Sammlungswebseiten (Kreiseler et al. 2017) bis hin zu funktionstüchtigen Webprototypen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Eines der hervorzuhebenden Kursergebnisse ist ein Konzept zu einem Datenbestand der BBAW, welches mit seiner reduzierten Gestaltung und seiner Fokussierung auf Zeitgenossenschaft bewusst mit klassischen Archiv-Interfaces bricht (siehe Abb. 1). Anstatt die einzelnen Lebensdaten der Personen als Liste darzustellen, wird ein zeitlich geordneter Überblick geboten, der die Beziehungen zwischen Personen der »Berliner Klassik« in den Vordergrund rückt.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 1: Biographische Textdaten der Berliner Klassik (Sebastian Schuth, Tatjana Tšernõhh, Andreas Waleczek, Alexander Zöller).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Ein Projekt zur ostasiatischen Porzellansammlung der SPSG hatte zum Ziel, sowohl die Exploration der Bestände aus wissenschaftlichem Interesse zu unterstützen als auch kontextualisierende Narrative über die Entstehung der Objekte und der Sammlung anzubieten (siehe Abb. 2). Statt ausschließlich Informationen zu den Objekten in Form von Bild- und Metadaten zugrunde zu legen, haben die Studierenden gemeinsam mit den Kuratorinnen der Sammlung illustrierende Inhalte textuell und visuell erarbeitet, wodurch sie eine Vermittlungsebene in die wissenschaftlich erschlossenen Sammlungsdaten integrierten.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 2: Narrative und explorative Visualisierung einer Porzellansammlung (Jana Klausberger, Mark-Jan Bludau, Swann Nowak, Constantin Eichstaedt).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Eine interaktive Visualisierung des Münzkabinetts zeigt, wie das Loslösen von Darstellungskonventionen dazu beitragen kann, auf spielerische Art Erkenntnisse aus einem für Laien eher schwer zugänglichen Bestand zu gewinnen (siehe Abb. 3). Sowohl im physischen Ausstellungskontext als auch im Datenbankinterface werden Münzen zumeist in rigiden Tableaus dargestellt. Die Projektgruppe näherte sich dem numismatischen Bestand über ihren alltäglichen Blick auf das Material, nämlich über physische Haufen unterschiedlicher Münzen, in denen das Einzelstück zunächst untergeht. Die daraus resultierende Visualisierungsumgebung erlaubt es, aus haptisch anmutenden Münzhaufen über verschiedene Darstellungsmodi die Sammlung nach verschiedenen Merkmalen zu filtern und zu sortieren. Es können Zusammenhänge zwischen Prägungsort, Material, zeitlichen Verläufen und anderen Facetten in verschiedenen organischen Arrangements untersucht werden.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 3: Visualisierung der numismatischen Sammlung des Berliner Münzkabinetts (Flavio Gortana, Daniela Guhlmann, Franziska von Tenspolde).&lt;/p&gt;
&lt;p&gt;Diskussion&lt;/p&gt;
&lt;p dir="ltr"&gt;Disziplinäre Konventionen und die “digitale Vernunft” in Sammlungsinstitutionen legen bei Digitalisierungs- und Erschließungsprojekten häufig den Fokus auf die strukturierte und möglichst vollständige Erfassung von Metadaten, welche meist für interne Prozesse optimiert wird. Obwohl dieses Vorgehen im Sinne einer infrastrukturellen Entwicklung von digitalen Forschungsumgebungen zu Recht weite Verbreitung findet, werden auf diesem Wege kritische gestalterische Ansätze und Innovationen nicht begünstigt. In den ersten Beschäftigungen mit den in den Kurs eingebrachten Datensätzen wird angeregt, dass sich die Studierenden einerseits “sensibel” mit den Spezifika der Sammlungen auseinandersetzen, gleichzeitig aber im Sinne eines kritisch-generativen Annäherns auch neue und möglicherweise “ungewöhnliche” Sichten auf die Sammlungen in Betracht ziehen. In den begleitenden Sitzungen finden daher ebenso Impulsvorträge zur musealen Praxis, zur Infragestellung disziplinärer Deutungshoheit und zur Diskrepanz zwischen Materialität von Sammlungen und deren Distanz schaffenden Präsentationsformen in Vitrinen, Schaukästen oder gesicherten Displays statt. Nach teilweiser Skepsis vonseiten der Institutionen am Anfang der Zusammenarbeit mit den Projektgruppen führt diese kritisch-generative Annäherung jedoch immer dazu, dass auch die Expert*innen neue Blicke auf “ihre” Sammlungen gewinnen können. Einige der Projekte werden, auch auf Wunsch der Sammlungsinstitutionen, über das Ende des Kurses hinweg weiterentwickelt und zeugen somit davon, dass alternative Entwürfe zur gängigen Darstellungspraxis und das Hinterfragen des Status Quo für alle Seiten ein Zugewinn bieten kann: für die Expert*innen in den Sammlungsinstitutionen, den mit den Sammlungen arbeitenden Wissenschafter*innen, der breiteren Öffentlichkeit, welche über anregende Visualisierungen Zugang zu den Sammlungen erhält und für die Studierenden, die selbständig neuartige Zugänge zu spannenden Beständen entwickeln und erforschen.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>300</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Windhager, Florian
Glinka, Katrin
Mayr, Eva
Schreder, Günther
Dörk, Marian</p3_authors>
  <p3_organisations>Donau-Universität Krems, Österreich
Stiftung Preußischer Kulturbesitz, Deutschland
Donau-Universität Krems, Österreich
Donau-Universität Krems, Österreich
Fachhochschule Potsdam</p3_organisations>
  <p3_emails>florian.windhager@donau-uni.ac.at
k.glinka@smb.spk-berlin.de
eva.mayr@donau-uni.ac.at
Guenther.Schreder@donau-uni.ac.at
doerk@fh-potsdam.de</p3_emails>
  <p3_presenting_author>Windhager, Florian</p3_presenting_author>
  <p3_title>Zur Weiterentwicklung des “cognition support”: Sammlungsvisualisierungen als Austragungsort kritisch-kulturwissenschaftlicher Forschung</p3_title>
  <p3_abstract>&lt;p dir="ltr"&gt;1. Einleitung&lt;/p&gt;
&lt;p dir="ltr"&gt;Interfaces und Methoden der Informationsvisualisierung  dienen insbesondere in Bezug auf abstrakte und komplexe Gegenstände der Unterstützung, Verstärkung und Augmentierung der menschlichen Kognition (Arias-Hernandez et al., 2012). Sammlungen des kulturellen Erbes (Galerien, Bibliotheken, Archive und Museen) sind Paradebeispiele für solche komplexen Gegenstände: sie organisieren und bereiten tausende Objekte auf und stellen diese gemeinsam mit assoziierten Informationen für Forschung und Öffentlichkeit bereit. Viele dieser Sammlungen sind mittlerweile digitalisiert im Netz zugänglich, womit lokale Sammlungs-Interfaces und große Aggregatoren zu Portalen von neuen Informationsräumen werden, in denen Kultur erlebbar und verhandelbar wird.&lt;/p&gt;
&lt;p dir="ltr"&gt;Ausgangspunkt unseres Vortrags ist eine aktuelle Studie zu Sammlungsinterfaces, die Methoden der Informationsvisualisierung nutzen um kognitive Operationen wie Exploration, Navigation und Analyse auf verschiedenen Ebenen einer Sammlung zu unterstützen. Im Vortrag werden wir einige der durch die Studie gewonnenen Erkenntnisse vertiefen und die Frage ins Zentrum stellen, wie Visualisierungsinterfaces auch jene kognitiven Operationen fördern können, die im kulturwissenschaftlichen Kontext als “kritische” tradiert werden. Analog zu existierenden Definitionen (vgl. Jaeggie &amp; Wesche, 2009; Foucault, 1990; und Butler, 2001) verstehen wir Kritik als jene Form der Kognition, die einen Gegenstand - und das ihn konstituierende Forschungssystem - in seiner Umwelt kontextualisiert und einer Bewertung unterzieht. Dabei bündelt die Operation i) analytisches und exploratives Wissen über Struktur und Dynamik ihres Gegenstands, ii) eine Bewertung im Sinne einer differenzierten Vermessung von Aktualität und Potentialität des Gegenstands, iii) eine Offenlegung und Argumentierung der instrumentalisierten Maßstäben und Normen, sowie oftmals eine iv) Ableitung von Handlungsoptionen zur (Selbst)berichtigung und (Selbst)Steuerung des fokussierten und des fokussierenden Systems.&lt;/p&gt;
&lt;p dir="ltr"&gt;Mit direktem Bezug auf Fragestellungen des Calls entwickeln wir ein Analyseschema, um dieses Potenzial für Visualisierungssysteme zu kulturellen Sammlungen zu erkunden und diskutieren Designstrategien, um entsprechende Funktionen zu stärken.&lt;/p&gt;
&lt;p dir="ltr"&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;2. Visuelle Sammlungsinterfaces&lt;/p&gt;
&lt;p dir="ltr"&gt;Interfaces zu kulturellen Sammlungen vermitteln zwischen großen Mengen von digitalen Objekten und den darauf bezogenen Absichten und Interessen von ExpertInnen oder BesucherInnen. Abbildung 1 zeigt eine schematische Aufreihung der wichtigsten Komponenten eines entsprechenden Systems, in dem das visuelle Interface (Mitte) einen kognitiven Mehrwert auf BenutzerInnenseite (rechts) schaffen soll (vgl. Card et al., 1999, S. 17). Dieser kognitive Mehrwert umfasst beispielsweise einen Erkenntnisgewinn bezüglich einer digitalen Sammlung, welche oftmals auf physische Objektsammlungen und letztlich auf Ursprungskulturen oder Sammlungskontexte verweisen (links).&lt;/p&gt;
&lt;p dir="ltr"&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Abb.1: Komponenten eines visualisierungs-gestützten HCI-Systems, das Daten von kulturellen Sammlungen (links) in visuellen Interfaces repräsentiert (Mitte), um damit diverse kognitive Operationen (wie Exploration, Navigation, Analyse, und mentale Repräsentation) zu unterstützen (rechts) .&lt;/p&gt;
&lt;p dir="ltr"&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Interfacesysteme - als zunächst meist opake Ensembles von Datenbank, Algorithmik und Benutzeroberfläche - entscheiden in solchen Konstellationen über eine zentrale Passage der Informationsverarbeitung und haben nicht zuletzt deshalb kritische Aufmerksamkeit verdient. Dies betrifft sowohl die Skepsis, ob quantitative und algorithmische Visualisierungsverfahren überhaupt nicht-verfremdend oder epistemologisch “a-trojanisch” (Drucker, 2011) auf Feldern von geistes- und kulturwissenschaftlichen Fragestellungen eingeführt und genutzt werden können, wie auch den konstanten internen Diskurs der Methodenkritik, der die Konferenzen und Reader der Digital Humanities bestimmt. So wie alle mediale Technologien beeinflussen Interfacesysteme den Aufbau der mentalen Repräsentationen ihrer Gegenstände über multiple Stationen der Übertragung und Übersetzung (Vektoren in blau, Abb. 1) und begünstigen oder erschweren in der Folge gewisse interpretative oder praktische Anschlussoperationen (Vektoren in orange).&lt;/p&gt;
&lt;p dir="ltr"&gt;In der diesem Beitrag zugrunde liegenden Studie erfassten wir den Gestaltungsraum solcher medialen Systeme im Sammlungskontext und leiteten Anforderungen an zukünftige Interfaces, darunter Generosität (Dörk, 2011; Whitelaw, 2015), Serendipität (Thudt et al., 2012), Narrativität (Davis et al., 2016) und kontextuelle Konnektivität (Hooland et al., 2014), als wichtige Gestaltungsprinzipien ab.&lt;/p&gt;
&lt;p dir="ltr"&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;3. Strategien des “Critical Cognition Support” durch Sammlungsinterfaces&lt;/p&gt;
&lt;p dir="ltr"&gt;Eine weitere der identifizierten Anforderung beschreibt den Aspekt des “Critical Cognition Support”, welcher im Kontext dieses Calls eine analytische Vertiefung verdient. Darunter verstehen wir “kritik-unterstützende” Funktionen von visuellen Interfaces, die über Exploration und Analyse hinaus eine differenzierte Beurteilung und gegebenenfalls eine (Selbst)berichtigung des visuell gestützten Forschungssystems ermöglichen - inklusive einer Kritik des kulturellen Gegenstands selbst. Auf medientheoretischer Basis ist bereits evident, dass die Gegenstände der Digital Humanities durch Interface- und Forschungssysteme entscheidend mit konstituiert werden. Abgesehen von der Diskussion der medialen und technologischen Bedingtheit von digitalen Gegenständen besteht jedoch ebenso die Notwendigkeit, sich kritisch-reflektierend mit den physischen Gegenständen - den Sammlungen, ihren Objekten, den Kulturen und Kontexten - und ihren Bedingtheiten auseinanderzusetzen. Die Ausbalancierung von medientheoretischen Axiomen mit einer kritisch-realistischen Hinwendung zur materiellen Welt und ihrer Konstruiertheit außerhalb von medialen Repräsentation eröffnet eine produktive Strategie, um ein mögliches Abdriften in Selbstbezüglichkeit des digitalen Methodendiskurses zu verhindern (cf. Latour, 2004). Sammlungsvisualisierungen sollten auch aus diesem Grund über die scheinbar “objektive” Abbildungen von deskriptiven Modellierungen hinausgehen, da die ihr zugrunde liegenden Gegenstände immer produktiv bezüglich ihrer Position und Funktion in gesellschaftlichen, epistemischen, oder normativen Kontexten befragt werden können (und müssen) (vgl. Calhoun, 1995; Swartz, 2012).&lt;/p&gt;
&lt;p dir="ltr"&gt;Der Aspekt einer solchen kritischen Kognition innerhalb eines Forschungssystems lässt sich in diesem Kontext in mehrfacher Hinsicht durch multiple Vektoren der Systemkritik fokussieren (siehe Abb. 2 oben): a) Kritik und Evaluation der Kultur und des Kontexts einer Sammlung, b) Kritik der Sammlungs und Kuratierungspraxis, c) Kritik der digitalen Modellierung, sowie d) Kritik der Informationsvisualisierung.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb.2. Kritische Kognition kann durch das Zusammenspiel von Designstrategien für visuelle Interfaces mit Bezug auf multiple Systemkomponenten gefördert werden. Wir unterscheiden in der Folge pro Komponente Aspekte der Kritik, Techniken der Kritik-Unterstützung, praktische Relevanz und individuelle “Critical Literacies”.&lt;/p&gt;
&lt;p dir="ltr"&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Aus multipler Motivation (als AnwenderInnen, EntwicklerInnen und AnalytikerInnen von Interfacesystemen) gehen wir davon aus, dass sich die Kritik und Transparenz dieser Systemkomponenten durch Designprinzipien von Interfaces selbst fördern lassen, und dass sich die kritische Interpretation der Komponenten oft nur schrittweise und “rückwärts” (d.h. mit steigender kulturwissenschaftlicher Relevanz von der Interface- bis zur Kuratierungs- und Kulturkritik) entfalten lässt. Wir summieren Aspekte und mögliche Gestaltungsoptionen zur Unterstützung dieser Komponenten in Abbildung 2 und beleuchten ein paar Facetten in der abschließenden Diskussion.&lt;/p&gt;
&lt;p dir="ltr"&gt;a) Kritik der Informationsvisualisierung: Sammlungsinterfaces können ihre eigenen technologischen Funktionsprinzipien zur kritischen Ermächtigung ihrer NutzerInnen via onboarding-Techniken, Tutorials, und Offenlegung von design choices transparent machen (vgl. Dörk et al., 2013). Akteure hinter Interfaceprojekten können gemeinsam mit ihren Strategien, Interessen und technologischen Präferenzen (Bubenhofer, 2016) über Kontextinformation kenntlich gemacht werden. Weiterhin kann die Kritik von Visualisierungen durch die Implementierung von multiple views gefördert werden, die Pluralitäten betonen und den Vergleich von Stärken und Schwächen der jeweiligen Darstellungen ermöglichen. Transparenz kann weiterhin durch Code Sharing und Open Source Dissemination unterstützt werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;b) Kritik der digitalen Modellierung: Methoden und Prozesse der Datengenerierung, -aufbereitung und -modellierung spielen eine konstitutive Rolle für die spezifische Medialität des Systems, seine analytischen Möglichkeiten, Grenzen, aber auch Unbestimmtheiten und Unschärfen. Dies gilt für die Modellierung von quantitativen und kategorialen Metadaten aus bestehenden analogen Beständen, sowie umso mehr für Systeme die Verfahren des Natural Language Processings und des Computer Visions in die Datengenerierung einbeziehen. Solche Informationen zur Provenienz von Daten sollten in visuelle Repräsentation ebenso einfließen wie Informationen zu Akteuren, Institutionen und Konventionen der digitalen Modellierung.&lt;/p&gt;
&lt;p dir="ltr"&gt;c) Kritik der Kurations- und Sammlungspraxis: Datenbanken und Metadaten beruhen häufig auf historischen Sammlungskatalogen und Erfassungssystemen, deren Selektivität bei der Entwicklung von kritisch informierten Sammlungsrepräsentationen reflektiert werden muss. Dies kann beispielsweise in einer Fokussierung von Visualisierungen auf Themen, Objekte oder Akteure einer Sammlung geschehen, die aufgrund von historischen Kanonisierungen, kuratorischer Selektion, soziokulturell und historisch bedingten Strukturen von Exklusion oder institutionellem Bias nicht (oder wenig) repräsentiert sind (Glinka et al., 2015). Im Sinne einer Diversifizierung von Narrativen im Anschluss an institutionskritische Interventionen ermöglichen Sammlungsvisualisierungen die Verhandlung von kritischen Interpretationen und Analysen der institutionellen Selbstbeschreibungen. Zu den kritikunterstützdenden Funktionen von Interfaces zählen beispielsweise auch Designs, welche institutionelle Wissensbestände und Interpretationen in Form von außer-institutioneller Teilhabe über Folksonomy, Crowd-Curation oder Community Co-Creation anreichern, in Frage stellen oder ergänzen.&lt;/p&gt;
&lt;p dir="ltr"&gt;d) Kulturkritik von Sammlungen: Sammlungen sind in der Regel reichhaltige Quellen der Information über assoziierte Kulturen, deren heterogene und herrschende Intentionen, Interessen, Epistemiken, Ideologien und sozio-ökonomischen Praktiken zum Kerngebiet kulturwissenschaftlicher Forschung gehören. Wir gehen zudem davon aus, dass die kritische Interpretation von Sammlungen teilweise nur über eine differenzierte Bewertung ihres Kontextes gewonnen werden kann. Wir sehen Möglichkeiten, solche Kontextualisierungen schon auf Ebene des Interfacedesigns herzustellen und ihre Interpretation mit Axiomen aus dem Bestand der kritischen Kulturtheorien zu verknüpfen. Die Veranschaulichung von historischen kulturellen Soll-Werten oder normativen Bewertungskategorien kann bei deren Neubewertung unterstützen. Die direkte Aktivierung von kritischem Engagement im gesellschaftlichen Kontext kann nicht zuletzt bei Interfaces zu zeitgenössischen Sammlungen eine zentrale Rolle spielen.&lt;/p&gt;
&lt;p dir="ltr"&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;4. Resümee und Ausblick&lt;/p&gt;
&lt;p dir="ltr"&gt;Wir präsentieren ein modulares und multifokales Schema für die Beurteilung und Entwicklung von Designstrategien, die kritische Kognition in digitalen Systemen der visuellen Analyse von Kulturdaten unterstützen. Über die oftmals technikgeleitete Entwicklung von analytischen und explorativen Systemen hinaus soll dies dabei helfen, den Anschluss der digitalen Tool-Entwicklung an kultur- und geisteswissenschaftliche Grundströmungen zu suchen. Auf Seiten des Interfacedesigns ergibt diese Skizze ein breites Arbeits- und Entwicklungsprogramm, das auch schon durch partielle Implementierungen neue Akzente gesetzt hat - sowohl im Bereich der Evaluierung von Sammlungsinterfaces, als auch in der Gestaltung und Umsetzung. Wir gehen davon aus, dass Aspekte dieser Designprinzipien auch auf andere Forschungssysteme der Digital Humanities übertragbar sind, jedoch von kritischen Kompetenzen (Literacies) der Akteure, sowie durch ihre konstitutive Selbstkritik als ForscherInnen oder BeobachterInnen ergänzt werden müssen, um ein transparent ineinandergreifendes und sich selbst korrigierendes und entwickelndes Ensemble von Vermittlungen zu erreichen.&lt;/p&gt;
&lt;p dir="ltr"&gt;  &lt;/p&gt;
&lt;p dir="ltr"&gt;Danksagung&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Arbeit wurde zum Teil durch den Wissenschaftsfonds FWF P.Nr. P28363 gefördert.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>76</session_ID>
  <session_title>Mittagspause</session_title>
  <session_start>2018-02-28 12:30</session_start>
  <session_end>2018-02-28 14:00</session_end>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>161</session_ID>
  <session_short>Vernetzungstreffen 2</session_short>
  <session_title>Digitale Philologien</session_title>
  <session_start>2018-02-28 12:30</session_start>
  <session_end>2018-02-28 14:00</session_end>
  <session_room_ID>14</session_room_ID>
  <session_room>S 21, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>179</session_ID>
  <session_short>Vernetzungstreffen 3</session_short>
  <session_title>Digitale Kunstgeschichte</session_title>
  <session_start>2018-02-28 12:30</session_start>
  <session_end>2018-02-28 15:00</session_end>
  <session_room_ID>16</session_room_ID>
  <session_room>S 23, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>173</session_ID>
  <session_title>Führung durch das VR-Labor des Regionalen Rechenzentrums Köln</session_title>
  <session_start>2018-02-28 13:00</session_start>
  <session_end>2018-02-28 13:30</session_end>
  <session_info>maximale Teilnehmerzahl: 10 &lt;br&gt;
Anmeldung nur vor Ort im Konferenzsekretariat. </session_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>174</session_ID>
  <session_title>Führung durch das VR-Labor des Regionalen Rechenzentrums Köln</session_title>
  <session_start>2018-02-28 13:30</session_start>
  <session_end>2018-02-28 14:00</session_end>
  <session_info>maximale Teilnehmerzahl: 10 &lt;br&gt;
Anmeldung nur vor Ort im Konferenzsekretariat. </session_info>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>50</session_ID>
  <session_short>Panel_3a</session_short>
  <session_title>Historische Grundwissenschaften</session_title>
  <session_start>2018-02-28 14:00</session_start>
  <session_end>2018-02-28 15:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Schulz, Daniela</chair1>
  <attendee_count>6</attendee_count>
  <chair1_name>Daniela Schulz</chair1_name>
  <chair1_organisation>Bergische Universität Wuppertal, Universität zu Köln</chair1_organisation>
  <chair1_email>schulzd1@uni-koeln.de</chair1_email>
  <chair1_email2>dschulz@uni-wuppertal.de</chair1_email2>
  <chair1_ID>1210</chair1_ID>
  <sessionID>50</sessionID>
  <presentations>1</presentations>
  <p1_paperID>295</p1_paperID>
  <p1_contribution_type>Panel</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Schulz, Daniela
Vogeler, Georg</p1_authors>
  <p1_organisations>Bergische Universität Wuppertal
Universität Graz, Österreich</p1_organisations>
  <p1_emails>dschulz@uni-wuppertal.de
georg.vogeler@uni-graz.at</p1_emails>
  <p1_presenting_author>Schulz, Daniela</p1_presenting_author>
  <p1_title>Abgrenzung oder Entgrenzung? Zum Spannungsverhältnis zwischen Historischen Hilfswissenschaften und Digital Humanities</p1_title>
  <p1_abstract>&lt;p&gt;Das Panel möchte Vertreter|inn|en der traditionellen Historischen Hilfs- bzw. Grundwissenschaften und Digital Humanists als dezidierte „Grenzgänger“ an einen Tisch holen, um so einen möglichst offenen und konstruktiven Austausch zum Verhältnis der Disziplinen und gegenseitigen (Un-)Nutzen der jeweiligen Methoden zu führen, sowie erfolgreiche, aber auch problematische Kooperationen und Projekte vorzustellen und zu diskutieren.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>49</session_ID>
  <session_short>Panel_3c</session_short>
  <session_title>Wissenschaftsorganisation I</session_title>
  <session_start>2018-02-28 14:00</session_start>
  <session_end>2018-02-28 15:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Henny-Krahmer, Ulrike</chair1>
  <attendee_count>3</attendee_count>
  <chair1_name>Ulrike Henny-Krahmer</chair1_name>
  <chair1_organisation>Universität Würzburg</chair1_organisation>
  <chair1_email>ulrike.henny@uni-wuerzburg.de</chair1_email>
  <chair1_email2>ulrike.henny@web.de</chair1_email2>
  <chair1_ID>1434</chair1_ID>
  <sessionID>49</sessionID>
  <presentations>1</presentations>
  <p1_paperID>198</p1_paperID>
  <p1_contribution_type>Panel</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Neuber, Frederike
Henny-Krahmer, Ulrike
Sahle, Patrick
Fischer, Franz</p1_authors>
  <p1_organisations>Institut für Dokumentologie und Editorik e.V.; Berlin-Brandenburgische Akademie der Wissenschaften
Institut für Dokumentologie und Editorik e.V.; Universität Würzburg
Institut für Dokumentologie und Editorik e.V.; Universität zu Köln/CCeH
Institut für Dokumentologie und Editorik e.V.; Universität zu Köln/CCeH</p1_organisations>
  <p1_emails>neuber.frederike@gmail.com
ulrike.henny@uni-wuerzburg.de
sahle@uni-koeln.de
franz.fischer@uni-koeln.de</p1_emails>
  <p1_presenting_author>Neuber, Frederike
Sahle, Patrick</p1_presenting_author>
  <p1_title>Alles ist im Fluss - Ressourcen und Rezensionen in den Digital Humanities.</p1_title>
  <p1_abstract>&lt;p&gt;Eine Kritik der digitalen Vernunft muss auch eine Kritik der digitalen Ressourcen umfassen. Die traditionelle Form der Kritik ist die wissenschaftliche Besprechung oder Rezension. Die kritische Instanz eines Rezensionswesens fehlt den Digital Humanities bisher fast gänzlich. Digitale Ressourcen wie etwa wissenschaftliche Editionen, Textkorpora, Bilddatenbanken oder auch Software werden selten systematisch rezensiert.&lt;/p&gt;
&lt;p dir="ltr"&gt;Im Gegensatz zu traditionellen Forschungsergebnissen der Geisteswissenschaften sind digitale Ressourcen nicht statisch, sondern wandelbar, oft prozesshaft und nicht abgeschlossen. Die Kritik der Ressourcen muss die besonderen Bedingungen, Eigenschaften und Folgephänomene digitaler Daten berücksichtigen und eine eigene Form finden. Wie sich die Ressourcen wandeln, so muss sich auch die Kritik wandeln, denn panta rhei - “alles fließt”, sagt Heraklit.&lt;/p&gt;
&lt;p dir="ltr"&gt;Vor diesem Hintergrund widmet sich das Panel u.a. folgenden Fragen: Wie können traditionelle Rezensionsorgane die Besprechung digitaler Ressourcen fördern? Brauchen die Digital Humanities ein eigenes Rezensionswesen, um den Besonderheiten digitaler Ressourcen gerecht zu werden? Welche Herausforderungen stellen Publikationen von Daten oder Algorithmen an die RezensentInnen? Inwiefern steigen mit dem Zuwachs an technischen Möglichkeiten auch die Ansprüche an digitale Ressourcen und wie lassen sich diese als Standards und Evaluationskriterien verhandeln, dokumentieren und weiterentwickeln? Wie kann man die Digitalität des Rezensionswesens nutzen, um die Prozesshaftigkeit der zu rezensierenden Objekte zu berücksichtigen, z. B. auch durch dynamischere Formen der Kritik jenseits der “klassischen” Rezension?&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>51</session_ID>
  <session_short>VP_3b</session_short>
  <session_title>Sprachanalyse</session_title>
  <session_start>2018-02-28 14:00</session_start>
  <session_end>2018-02-28 15:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Burr, Elisabeth</chair1>
  <attendee_count>4</attendee_count>
  <chair1_name>Elisabeth Burr</chair1_name>
  <chair1_organisation>Universität Leipzig</chair1_organisation>
  <chair1_email>elisabeth.burr@uni-leipzig.de</chair1_email>
  <chair1_ID>1000</chair1_ID>
  <sessionID>51</sessionID>
  <presentations>3</presentations>
  <p1_paperID>105</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Hoenen, Armin
Samushia, Lela</p1_authors>
  <p1_organisations>Goethe Universität Frankfurt, Germany
Goethe Universität Frankfurt, Germany</p1_organisations>
  <p1_emails>hoenen@em.uni-frankfurt.de
samushia@em.uni-frankfurt.de</p1_emails>
  <p1_presenting_author>Hoenen, Armin</p1_presenting_author>
  <p1_title>Principles Aiding in Reading Abbreviations in OldGeorgian and Latin</p1_title>
  <p1_abstract>&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In Old Georgian, abbreviations are very prominent and yet very heterogeneous. That is more so than in other (historical) languages. One finds many&lt;br /&gt;abbreviations for the same word, compare Boeder (1987). In this article, we&lt;br /&gt;show that some properties of Old Georgian abbreviations hold nonetheless&lt;br /&gt;and which can be used as an aid to reading historical text. We use Old&lt;br /&gt;Georgian abbreviations in inscriptions and manuscripts from the Titus website, Gippert (1995) and for contrast Latin, for which we use data from the&lt;br /&gt;Epigraphic database Heidelberg, http://edh-www.adw.uni-heidelberg.&lt;br /&gt;de/home. There are several partly overlapping typologies of abbreviations, com-&lt;br /&gt;pare Marchand (1969); Kreidler (1979); McArthur (1988); McArthur and&lt;br /&gt;McArthur (1992); Rúa (2004); Driscoll (2009). (Carroll, 2003, p.205) remarks: ”Unfortunately, universally accepted standards for many abbreviations and acronyms do not exist”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Position&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first investigation concerns the position of the letters in the extension,&lt;br /&gt;which are maintained in the abbreviation. That is &lt;precip&gt; as an abbre-&lt;br /&gt;viation for &lt;precipitation&gt;, will be converted into 1-2-3-4-5-6. We look at&lt;br /&gt;the type and token levels for Old Georgian and Latin, see Table 1.&lt;br /&gt;We find that abbreviations usually start in the first letter although this&lt;br /&gt;might be part of a prefix in both Latin and Old Georgian. This allows&lt;br /&gt;for keeping parafoveal preview information intact, see for instance Slattery&lt;br /&gt;et al. (2011); Rayner et al. (2012). Chanceaux et al. (2013) summarizing&lt;br /&gt;Dandurand et al. (2011) find that ”initial letters provide more information with respect to word identity than any other letter position”. Initital letters are together with the last letter the ”most visible” letters of a word,&lt;br /&gt;Dandurand et al. (2011), which means under more that due to the adjacent&lt;br /&gt;spaces, they can more easily be recognized.&lt;/p&gt;
&lt;p&gt;Secondly, Old Georgian uses more contractions (abbreviations by first&lt;br /&gt;and lasdt letter, a Christian abbreviation tradition), Latin suspensions (ab-&lt;br /&gt;breviation by the first n letters. Figure 1 shows the patterns of occurrence&lt;br /&gt;of positions, given a certain word length. The x-axis represents the percent-&lt;br /&gt;age of abbreviations (regardless of their lengths) that contain the position&lt;br /&gt;specified on the y-axis. In a normalized plot, all wordlengths from 4 to 11&lt;br /&gt;are plotted together. For Old Georgian there is a gap after some first letters.&lt;br /&gt;We conjecture that the end of the word stem is most unlikely to occur in&lt;br /&gt;abbreviations. While in Latin, suffixes are often left out, in Georgian, which&lt;br /&gt;has an agglutinative morphology, this would lead to considerable difficulties&lt;br /&gt;in relating the actual word to the context since some suffixes carry infor-&lt;br /&gt;mation which in Latin are expressed by independent pre- or postpositions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vowels &amp; Consonants&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, we look at how many vowels and consonants are retained in abbrevia-&lt;br /&gt;tions, see Table 2. As one can see, in all contexts the ratio of vowels in the&lt;br /&gt;abbreviations is clearly lower than in the extensions. Information theory,&lt;br /&gt;see for instance Shannon (1948) provides a stable basis for the interpretation&lt;br /&gt;of these results. a) the class of vowels is smaller than the class of conso-&lt;br /&gt;nants and b) vowels are much more frequent not only because of a) but also&lt;br /&gt;because they occur in syllable nuclei and consequently a single vowel, yet not a single consonant can constitute a syllable. 1 2 Thus, vowels are less&lt;br /&gt;informative and hence more easily guessable than consonants. An attempt&lt;br /&gt;on a cross linuistic study involving 20 languages confirmed this result and&lt;br /&gt;makes it plausible for a general setting.&lt;br /&gt;Finally, we analysed our larger Latin data in more detail. We extract the&lt;br /&gt;frequency based rankings of letters and test their correlation with the rank-&lt;br /&gt;ing through probability of being retained in an abbreviation. The Spearman&lt;br /&gt;rank correlation suggests, that there is a significant correlation between the&lt;br /&gt;frequency of a consonant and its probability to be retained in an abbrevi-&lt;br /&gt;ation (p-value = 0.002655, ρ 0.622807). For vowels p is above 0.01, and ρ&lt;br /&gt;is negative at −0.2 suggesting that for vowels, which are all quite frequent,&lt;br /&gt;there is no clear effect. Carreiras et al. (2009) find that vowels do not yield&lt;br /&gt;priming effects for recognition in opposition to consonants, which would&lt;br /&gt;nicely align with this finding. Thus, an infrequent consonant grapheme is&lt;br /&gt;more probably retained.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ordering abbreviations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally, one can ask how an abbreviation is being generated, where one&lt;br /&gt;hypothesis assumes, that someone, who wants to abbreviate a word, holds in&lt;br /&gt;mind all possible abbreviations and chooses from them. We try to validate&lt;br /&gt;this counting all possible abbreviations per word length given suspension&lt;br /&gt;and contraction. This can also help estimate algorithmic complexity in&lt;br /&gt;abbreviating.&lt;br /&gt;As we have empirically observed but not explicitly stated so far and&lt;br /&gt;implicitly assumed, a condition for a valid abbreviation may be that the&lt;br /&gt;indices of the letters must maintain ascending order, or, in other words, the&lt;br /&gt;original sequence of the letters is not ( permuted.&lt;br /&gt;If so, for each word length w) w and abbreviation length a there are a possible abbreviations, since for each&lt;br /&gt;distinct combination only one can maintain the ascending order of elements.&lt;br /&gt;Now in order for understanding how many possible abbreviations there are&lt;br /&gt;for each word we need to add up values for all different a, where a &lt; w.&lt;/p&gt;
&lt;p&gt;By simply using this binomial coefficient one can compute the numbers of&lt;br /&gt;possible combinations (for numbers unitl 10, see Table 3 leftmost numbers in&lt;br /&gt;cells) which gives the inner portion of Pascal’s triangle. However, the outer&lt;br /&gt;1s are missing, since the extension itself is no abbreviation and neither is&lt;br /&gt;a sequence of zero letters, the sum is thus 2 w − 2. Thus, the increase in&lt;br /&gt;possibilities is linear and quick, for a word of length 15, there are 32, 766&lt;br /&gt;possibilities how it could be abbreviated.&lt;br /&gt;If we additionally fix the first letter, we ( count&lt;br /&gt;) only all combinations w−1 containing element ’1’,which must then be a−1 , since we have fixed the&lt;br /&gt;first letter and from the remaining w − 1 letters, we can choose any a − 1&lt;br /&gt;remaining elements of the abbreviation. This restricts the possible numbers&lt;br /&gt;already considerably. Overall, we halve possibilities, so 2 w−1 − 1 becomes&lt;br /&gt;the sum formula, which would still leave us with 16, 383 possibilities for a&lt;br /&gt;word of 15 letters.&lt;br /&gt;In case of a contraction, one fixes the first and the last letter. Then&lt;br /&gt;again, results are halved with one letter long abbreviations excluded, hence&lt;br /&gt;the sum relates to w by 2 w−2 − 1. For a word of length 15 still 8, 191&lt;br /&gt;possibilities would be left.&lt;br /&gt;Numbers remain so high towards the end of the scale that it seems&lt;br /&gt;improbable that someone who abbreviates a word be aware of all of the&lt;br /&gt;possibilities simulateneously at decision time. One also sees that contrac-&lt;br /&gt;tion is quite effective in restricting possible abbreviations and might thus&lt;br /&gt;considerably speed-up abbreviating.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discussion and Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Various analyses (not only the presented) conducted have shown two prop-&lt;br /&gt;erties of abbreviations which can help understand and read Old Georgian and Latin abbreviations.&lt;br /&gt;1. the overwhelming majority of abbreviations contains the first letter of&lt;br /&gt;the extension&lt;br /&gt;2. morphological type is an important factor for abbreviating, for Old&lt;br /&gt;Georgian the end of the word stem is likely to disappear whereas suf-&lt;br /&gt;fixes tend to be presented, the last letter being likely for the contrac-&lt;br /&gt;tion principle&lt;br /&gt;3. for Latin we found that consonants are more likely to occur in an&lt;br /&gt;abbeviation than vowels, less frequent consonants more than frequent&lt;br /&gt;ones&lt;br /&gt;Combinatorial evidence suggests that keeping in mind all possible ab-&lt;br /&gt;breviations even in case of contraction is unlikely for longer words. Consid-&lt;br /&gt;ering that Old Georgian as an agglutinative language produces long words&lt;br /&gt;this may partly explain why there is larger variety in the abbreviation land-&lt;br /&gt;scape, since in each abbreviation process another possible abbreviation may&lt;br /&gt;have surfaced.&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Boeder, W. (1987). Versuch einer sprachwissenschaftlichen Interpretation&lt;br /&gt;der altgeorgischen Abkürzungen. Revue des études géorgiennes et cau-&lt;br /&gt;casiennes, 3:33–81.&lt;/p&gt;
&lt;p&gt;Carreiras, M., Duñabeitia, J. A., and Molinaro, N. (2009). Consonants and&lt;br /&gt;vowels contribute differently to visual word recognition: Erps of relative&lt;br /&gt;position priming. Cerebral Cortex, 19(11):2659–2670.&lt;/p&gt;
&lt;p&gt;Carroll, J. (2003). Oxford handbook of computational linguistics. Edited&lt;br /&gt;by R. Mitkov.&lt;/p&gt;
&lt;p&gt;Chanceaux, M., Mathôt, S., and Grainger, J. (2013). Flank to the left,&lt;br /&gt;flank to the right: Testing the modified receptive field hypothesis of letter-&lt;br /&gt;specific crowding. Journal of Cognitive Psychology, 25(6):774–780.&lt;/p&gt;
&lt;p&gt;Dandurand, F., Grainger, J., Duñabeitia, J. A., and Granier, J.-P. (2011).&lt;br /&gt;On coding non-contiguous letter combinations. Frontiers in psychology,&lt;br /&gt;2:136.&lt;/p&gt;
&lt;p&gt;Driscoll, M. (2009). Marking up abbreviations in old norse-icelandic&lt;br /&gt;manuscripts. In Medieval Texts–Contemporary Media. Ibis.&lt;/p&gt;
&lt;p&gt;Dryer, M. S. and Haspelmath, M., editors (2013). WALS Online. Max&lt;br /&gt;Planck Institute for Evolutionary Anthropology, Leipzig.&lt;/p&gt;
&lt;p&gt;Gippert, J. (1995). TITUS. Das Projekt eines indogermanistischen The-&lt;br /&gt;saurus (”TITUS. The project of an Indo-European thesaurus”). LDV-&lt;br /&gt;Forum, 12(2):35–47.&lt;/p&gt;
&lt;p&gt;Guthrie, D., Allison, B., Liu, W., Guthrie, L., and Wilks, Y. (2006). A&lt;br /&gt;closer look at skip-gram modelling. In Proceedings of the 5th international&lt;br /&gt;Conference on Language Resources and Evaluation (LREC-2006), pages&lt;br /&gt;1–4.&lt;/p&gt;
&lt;p&gt;Kreidler, C. W. (1979). Creating new words by shortening. Journal of&lt;br /&gt;English Linguistics, 13(1):24–36.&lt;/p&gt;
&lt;p&gt;Maddieson, I. (2013a). Consonant-Vowel Ratio. Max Planck Institute for&lt;br /&gt;Evolutionary Anthropology, Leipzig.&lt;/p&gt;
&lt;p&gt;Maddieson, I. (2013b). Vowel Quality Inventories. Max Planck Institute for&lt;br /&gt;Evolutionary Anthropology, Leipzig.&lt;/p&gt;
&lt;p&gt;Marchand, H. (1969). The categories and types of present-day English word-&lt;br /&gt;formation: A synchronic-diachronic approach. Beck.&lt;/p&gt;
&lt;p&gt;McArthur, T. (1988). The cult of abbreviation. English Today, 4(03):36–42.&lt;/p&gt;
&lt;p&gt;McArthur, T. B. and McArthur, F. (1992). The Oxford companion to the&lt;br /&gt;English language. Oxford University Press.&lt;/p&gt;
&lt;p&gt;Rayner, K., Pollatsek, A., Ashby, J., and jr. Clifton, C. (2012). Psychology&lt;br /&gt;of Reading. Psychology Press, New York/Hove.&lt;/p&gt;
&lt;p&gt;Rúa, P. L. (2004). Acronyms &amp; co.: A typology of typologies= acrónimos&lt;br /&gt;y cía: una tipología de tipologías. Estudios Ingleses de la Universidad&lt;br /&gt;Complutense, 12:109–129.&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
</p1_abstract>
  <p2_paperID>210</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Dimpel, Friedrich Michael</p2_authors>
  <p2_organisations>FAU Erlangen-Nürnberg, Deutschland, Germanistik, und TU Darmstadt, Computerphilologie und Mediävistik</p2_organisations>
  <p2_emails>mail@dimpel.de</p2_emails>
  <p2_presenting_author>Dimpel, Friedrich Michael</p2_presenting_author>
  <p2_title>Die guten ins Töpfchen: Zur Anwendbarkeit von Burrows’ Delta bei kurzen mittelhochdeutschen Texten nebst eines Attributionstests zu Konrads ‚Halber Birne‘</p2_title>
  <p2_abstract>&lt;p&gt;Die Anwendbarkeit von Burrows’ Delta (Burrows 2002) als Autorschaftstest für das Deutsche ist in Validierungstestreihen wiederholt eindrucksvoll demonstriert worden (Büttner et alia 2017, Eder 2013a/b, Evert et alia 2015. Evert et alia 2016); auch im Mittelhochdeutschen ist Delta anwendbar (Dimpel 2016/2018). Die Stabilität des Verfahrens wurde in Noise-Tests belegt: Wenn man etwa 12% aller Wörter durch Fremdmaterial austauscht, sinkt die Erkennungsquote kaum (Dimpel 2017a/2018).&lt;/p&gt;
&lt;p&gt;Bei nicht-normalisierten mittelhochdeutschen Texten steigt die Erkennungsquote in einem Validierungstest von 80% auf 91%, wenn man die bei Evert et alia (2016) entwickelte Methode der Z-Wert-Begrenzung mit einem von mir zusammengestellten Normalisierungswörterbuch kombiniert (Dimpel 2017a). Kontraintuitiv ist, dass nur die Kombination dieser Optimierungsverfahren zu einer Verbesserung um 11% führt, während in diesem Setting nur der Einsatz der Z-Wert-Begrenzung zu einer minimalen Verschlechterung führt; der Einsatz nur des Normalisierungswörterbuchs führt nur zu einer Verbesserung um 5,6%. Dieser Befund wird unter dem Stichwort „Delta-Rätsel“ in einem Dariah-de-Working-Paper (Dimpel 2017b) ausführlich analysiert. Bei der Rätsel-Analyse wurde – ein Serendipitätseffekt – eine Möglichkeit entdeckt, wie man bei einem konkreten Vergleich von drei Texten die Wortformen identifizieren kann, die eine korrekte Autorschaftserkennung begünstigen oder behindern – dazu im Weiteren.&lt;/p&gt;
&lt;p&gt;1         Gute und schlechte Wortformen&lt;/p&gt;
&lt;p&gt;Beim Delta-Test berechnet man aus den Wortfrequenzen für ein Korpus jeweils die zugehörigen Z-Werte. Beim Vergleich von zwei Wortformen aus zwei Texten wird die Differenz der jeweiligen Z-Werte gebildet und der Betrag dieser Differenz genommen. Delta ist schließlich der Mittelwert der absoluten Z-Wert-Differenzen für alle Wortformen.&lt;/p&gt;

&lt;p&gt;Abb. 1: Ratetext-Z-Werte (blau) sowie Z-Wert-Differenzen Ratetext–Autor-Trainingstext (orange)&lt;/p&gt;
&lt;p&gt;Abb. 1. zeigt oben die Z-Werte der Handschrift M von Wolframs ‚Parzival‘ in einem Test gegen Wolframs ‚Willehalm‘ bei einem Trainingskorpus mit 19 weiteren Texten (vgl. Dimpel 2017b). Der ‚Parzival‘ soll dem Autor-Trainingstext (Wolframs ‚Willehalm‘) zugeordnet werden und nicht etwa Konrads ‚Partonopier‘.&lt;/p&gt;
&lt;p&gt;Im oberen linken Viertel sind positive Z-Werte blau aufgetragen und nach der Höhe der Z-Werte sortiert. Ab der Stelle, an der die blauen Balken auf 0 zurückgehen, folgt rechts der Betrag der negativen Z-Werte (blau). Unten stehen (orange) die absoluten Z-Wert-Differenzen zwischen dem Ratetext und dem Autor-Trainingstext (Differenzen der Z-Werte von Wolframs ‚Parzival‘ und Wolframs ‚Willehalm‘).&lt;/p&gt;
&lt;p&gt;Man könnte A) den Verdacht haben, dass Wortformen bei hohen blauen Balken „gut“ sind, um einen Text von Distraktortexten zu unterscheiden, da hohe Z-Werte auf erhebliche Abweichung von den übrigen Korpusfrequenzen hindeuten. Man könnte auch B) den Verdacht haben, dass Wortformen bei hohen orangen Balken „schlecht“ für die Autorerkennung sind: Unterschiede zwischen dem Ratetext und Autor-Trainingstext (beide vom gleichen Autor) sollten eher niedrig sein, damit die Erkennung funktioniert. Allerdings sind bei hohen blauen Balken relativ oft auch hohe orange Balken vorhanden – auch in anderen Tests (Dimpel 2017b). Dieses Diagramm erlaubt also keine Aussage darüber, welche Wortformen gut für die Autorerkennung sind; hohe Z-Werte allein erlauben noch keine Aussage darüber, ob ein Wort hier gut geeignet ist, um einen Autor zu charakterisieren.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Abb. 2: Z-Wert-Differenzen Ratetext–Autor-Trainingstext (orange: Wolframs ‚Parzival‘– Wolframs ‚Willehalm‘) und Z-Wert-Differenzen Ratetext–Distraktortext (grau: Wolframs ‚Parzival‘ – Konrads ‚Partonopier‘)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Neu ist in Abb. 2 nur die obere Hälfte: Sie enthält Z-Wert-Differenzen des Ratetexts zum Distraktortext (‚Partonopier‘). Diese grauen Unterschiede sollten bei funktionierender Autorerkennung eher groß sein; gleichzeitig sollten die orangen Unterschiede der Texte vom gleichen Autor niedriger sein als die grauen. Dort, wo die grauen Balken genauso hoch sind wie die orangen, hilft das Wort nicht bei der Autorerkennung – dies ist bei sehr hohen positiven Z-Werten der Fall. Sind die orangen Balken höher als die grauen, stört die Wortform die Autorerkennung.&lt;/p&gt;
&lt;p&gt;Die Differenz zwischen orange und grau sei ‚Level-2-Differenz‘ genannt: „Differenz aus der Z-Wert-Differenz zwischen Ratetext und Autor-Trainingstext einerseits und der Z-Wert-Differenz zwischen Ratetext und Distraktortext andererseits“. Bei positiven Level-2-Differenzen ist eine Wortform vorteilhaft für die Autorerkennung – mit Blick auf den einen untersuchten Distraktortext. Bei negativen Level-2-Differenzen ist die Wortform schlecht für die Autorerkennung. Über diese Differenz kann man „gute“ und „schlechte“ Wortformen einzeln identifizieren.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;2          Use-Case-Szenario ‚Halbe Birne‘&lt;/p&gt;
&lt;p&gt;Konrads Autorschaft wurde der ‚Halben Birne‘ trotz Selbstnennung im Epilog (&lt;em&gt;von Wirzburc maister Kuonrat&lt;/em&gt;) abgesprochen (Lachmann 1820, Laudan 1906, de Boor 1973, de Boor / Janota 1997; ‚Konrad‘ mit Fragezeichen bei Grubmüller 1996)– aufgrund des „obszönen“ Inhalts und sprachlicher Merkmale; anders Feistner 2000.&lt;/p&gt;
&lt;p&gt;Die stilometrische Analyse ist in mehrfacher Hinsicht eine Herausforderung: Eine gattungsübergreifende Attribution ist mangels anderer Vergleichstexte nötig (nach Schöch 2014 wäre eine Gattungsmischung möglichst zu meiden). In Konrads Oevre herrscht eine Vielfalt an Themen, Frivoles wie in der ‚Halben Birne‘ ist eher selten – auch im einzigen anderen Märentext Konrads: im ‚Herzmäre‘ bleibt die Liebe unerfüllt, es kommt zum doppelten Minnetod. Zudem ist die ‚Halbe Birne‘ recht kurz: sehr gute Quoten erreicht Delta ab 5.000 Wortformen in einer Bag-of-Words (vgl. Abb. 3 sowie Eder 2013a und Eder 2013b).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Abb. 3; zum Setting vgl. Dimpel 2018.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Die ‚Halbe Birne‘ enthält jedoch nur 2.469 Wortformen. Wenn man nun die ‚Birne‘ gegen ein Konrad-Korpus testet, kann man entweder die Wörter mit hoher Level-2-Differenz, die einer Erkennung von Konrad entgegenstehen, aus der Liste der untersuchten Most-Frequent-Words (MFWs) streichen. Oder man kann eine Positivliste mit „guten“ Wörtern bevorzugt verwenden – Wörter mit hoher positiver Level-2-Differenz.&lt;/p&gt;
&lt;p&gt;Vorab wird das Verfahren validiert: In einer Ermittlungsgruppe (vier Konrad-Texte) werden „gute“ und „schlechte“ Wörter identifiziert.[1] In einer Kontrollgruppe (vier andere Konrad-Texte) zeigt sich, dass die Erkennungsquote durch dieses Verfahren bei Bag-of-Words mit 2.000 Wortformen steigt – beim bevorzugten Verwenden „guter“ Wörtern stärker als beim Aussortieren der „schlechten“. Danach werden alle acht Konrad-Texte erneut zur Bildung der Listen der „guten“ und „schlechten“ Wortformen herangezogen. Als geeignete Parameter haben sich gezeigt:&lt;/p&gt;
&lt;p&gt;„Gute Wörter“: Level-2-Differenzen &gt;+2,31 in 6 von 7 Ermittlungsgruppen-Ratetexten, 304 items&lt;/p&gt;
&lt;p&gt;„Schlechte Wörter“: Level-2-Differenzen &lt;-1,2 in 2 von 7 Ermittlungsgruppen-Ratetexten, 174 items&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Im Attributionstest 1 wird die ‚Halbe Birne‘ als Autor-Trainingstext verwendet, als Ratetexte werden die acht Konrad-Texte sowie das ‚Herzmäre‘ verwendet; im ‚Herzmäre‘-Test bleibt es bei acht Konrad-Ratetexten; das ‚Herzmäre‘ ist Autor-Trainingstext. Hier erreicht das ‚Herzmäre‘ nur 4,5%, ein schlechter Wert, obwohl hier die Autorschaft nicht infrage gestellt wurde. Dagegen liegt die Erkennungsquote bei der ‚Halben Birne‘ auch ohne zusätzliche Wortlisten bereits über dem Zufallswert: Wenn ein Konrad-Text aus dem Ratekorpus nun nicht einem der 20 Texte von anderen Autoren zuordnet wird, sondern der ‚Halben Birne‘, dann stehen die Chancen dafür 1 zu 21. Wenn es also auf den Zufall zurückzuführen wäre, dass ein Text dem richtigen Autor zugeordnet wird, dann müsste die Erkennungsquote bei 5% liegen – so beim ‚Herzmäre‘. 83,8% bei der ‚Halben Birne‘ sind ein ordentlicher Wert, wenn man bedenkt, dass nur kurze Bag-of-Words mit 2.000 Wortformen getestet werden können und dass gattungsübergreifend getestet wird.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Beim Attributionstest 1 befand sich die ‚Halbe Birne‘ im Trainingskorpus. Im Ratekorpus waren inklusive ‚Herzmäre‘ 9 Konrad-Texte. Nun werden umgekehrt ‚Halbe Birne‘ bzw. ‚Herzmäre‘ als Ratetexte verwendet. Ins Trainingskorpus gebe ich zu den 20 Distraktortexten in separaten Tests jeweils einen Konrad-Text als Autor-Trainingstext.&lt;/p&gt;
&lt;p&gt;Attributionstest 2:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Im Attributionstest 2 übersteigen die meisten Werte 86%. Es gibt lediglich zwei deutliche Ausreißer, an denen jeweils das ‚Herzmäre‘ beteiligt ist. Dieses Minneleid-und-Minnetod-Märe fügt sich nicht zur politischen Propagandadichtung ‚Turnier von Nantes‘. Auch zur ‚Halben Birne‘ passt das ‚Herzmäre‘ nicht: Dort geht es um eine Dame, die einen Ritter abweist, weil er beim Birnenverzehr keine Tischmanieren an den Tag legt. Die Dame schläft mit einem vermeintlich taubstummen Hofnarren, der sich jedoch später als der abgewiesene Birnen-Ritter entpuppt. Interessante Fehlattributionen (etwa ‚Birne‘ zu ‚Häslein‘ statt zum ‚Herzmäre‘) werden im Vortrag vorgestellt.&lt;/p&gt;
&lt;p&gt;3         Ein kleiner Schritt für die Attribution der ‚Halben Birne‘ an Konrad&lt;/p&gt;
&lt;p&gt;Als Katharina Zeppezauer-Wachauer (Salzburg) mir einige Mären aus der Mittelhochdeutschen Begriffsdatenbank überlassen hat (vielen Dank dafür!), hat sie notiert: „Vielleicht können Sie ja wirklich, wie Edith Feistner gefordert hat, ‚Konrad seine Birne wiedergegeben‘!“ Auch wenn die Zahlen in beiden Attributionstests trotz der geringen Textlänge und trotz der Gattungsproblematik überraschend eindeutig sind, möchte ich bei einer vorsichtigen Interpretation bleiben. Zwar ist die Wahrscheinlichkeit sehr gering, dass die gefundene Nähe der ‚Halben Birne‘ zum Konrad-Korpus auf dem Zufall beruht. Allerdings wären ‚Kontrollpeilungen‘ (Eibl 2013) wünschenswert: Eine Attribution sollte nicht auf einem einzelnen Test mit einer Methode erfolgen, wünschenswert wären Bestätigungen mit anderen Methoden. Immerhin aber geht es hier nicht um eine blinde Attribution, sondern lediglich um Widerspruch gegen eine Athetese der Forschung. Eine Attribution stünde in Einklang mit Konrads Selbstnennung in vier von fünf Handschriften.&lt;/p&gt;
&lt;p&gt;Zudem würde ich den Test gerne mit einem größeren Mären-Korpus wiederholen, in dem idealerweise längere Texte wären und mehr Texte, die näher an Konrads Schaffenszeit liegen. Dass die Birne nicht zu Kaufringer clustert, könnte auch dem zeitlichen Abstand geschuldet sein, der durch gemeinsame groteske oder frivole Inhaltselemente nicht überlagert wird.&lt;/p&gt;
&lt;p&gt;Wichtig ist mir auch das Verfahren: Bislang ist eine Feature-Eliminierung oder Feature-Selektion häufig auf dem Weg des maschinellen Lernens erfolgt (Büttner et alia 2016) – mit dem Nachteil, dass der Weg der Kategorisierung teilweise im Dunklen bleibt. Ermittelt man „gute“ oder „schlechte“ Wörter via Level-2-Differenzen, so ist transparent, wie man zu den Parametern kommt und wie auf dieser Basis die weiteren Berechnungen erfolgen.&lt;/p&gt;
&lt;br clear="all" /&gt;
&lt;p&gt;[1] Im Trainingskorpus verwende ich hier und für die folgenden Attributionstests 7 Romane und 13 Mären: Barlaam, Daniel, Lanzelet, Meleranz, Parzival, Tristan, Wigalois; Frauentreue, Haeslein, Heidin_B, JvFreiberg_Raedlein, Kaufringer_Moerderin, Kaufringer_Rache, Kaufringer_listige_Frauen, Pyramus, Rosenpluet_Pfarrer, Schlegel, Schueler_Paris, StudentenAbenteuer_A, Zwickauer_Moenches_Not.&lt;/p&gt;

</p2_abstract>
  <p3_paperID>294</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Herrmann, J. Berenike</p3_authors>
  <p3_organisations>Universität Basel, Schweiz</p3_organisations>
  <p3_emails>berenike.herrmann@unibas.ch</p3_emails>
  <p3_presenting_author>Herrmann, J. Berenike</p3_presenting_author>
  <p3_title>Praktische Tagger-Kritik. Zur Evaluation des POS-Tagging des Deutschen Textarchivs </p3_title>
  <p3_abstract>&lt;p&gt;Der vorliegende Beitrag leistet eine Tool- und Methoden-Kritik der automatischen Auszeichnung von Wortarten (Part of Speech-, bzw. POS-Taggern) an literarischen Texten des 19. und frühen 20. Jahrhunderts. Er geht über eine rein intellektuelle Reflektion hinaus, indem er erste Schritte einer empirischen Evaluation des POS-Tagging des Deutschen Textarchivs (DTA, http://www.deutschestextarchiv.de/) und seiner praktischen Verbesserung vorlegt.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>71</session_ID>
  <session_title>Kaffeepause</session_title>
  <session_start>2018-02-28 15:30</session_start>
  <session_end>2018-02-28 16:00</session_end>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>119</session_ID>
  <session_short>MV</session_short>
  <session_title>DHd-Mitgliederversammlung</session_title>
  <session_start>2018-02-28 16:00</session_start>
  <session_end>2018-02-28 17:30</session_end>
  <session_room_ID>1</session_room_ID>
  <session_room>Hörsaal B, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <attendee_count>4</attendee_count>
 </session>

 <session>
  <session_ID>167</session_ID>
  <session_short>AG 6</session_short>
  <session_title>AG-Film und Video Treffen</session_title>
  <session_start>2018-02-28 17:45</session_start>
  <session_end>2018-02-28 19:00</session_end>
  <session_room_ID>18</session_room_ID>
  <session_room>S 25, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>160</session_ID>
  <session_short>Fight Club</session_short>
  <session_title>Keynote, Buffet und Tanz</session_title>
  <session_start>2018-02-28 19:15</session_start>
  <session_end>2018-02-28 23:59</session_end>
  <session_room_ID>21</session_room_ID>
  <session_room>Arttheater</session_room>
  <session_room_info>Ehrenfeldgürtel 127, 50823 Köln</session_room_info>
  <chair1>Dieckmann, Lisa</chair1>
  <chair2>Fischer, Franz</chair2>
  <session_info>In ausgelassener Atmosphäre wird im Kölner Art-Theater ein für den Wissenschaftsbetrieb ungewöhnliches Panel stattfinden: Das Fight Club-Format sieht vor, dass die PanelistInnen mit steilen Thesen zum Tagungsthema um die Gunst des Publikums wettstreiten. &lt;br&gt;&lt;br&gt;
Mit: Prof. Dr. Henning Lobin, Prof. Dr. Heike Zinsmeister, Prof. Dr. Hubertus Kohle, Dr. Mareike König</session_info>
  <attendee_count>10</attendee_count>
  <chair1_name>Lisa Dieckmann</chair1_name>
  <chair1_organisation>Universität zu Köln</chair1_organisation>
  <chair1_email>lisa.dieckmann@uni-koeln.de</chair1_email>
  <chair1_ID>1293</chair1_ID>
  <chair2_name>Franz Fischer</chair2_name>
  <chair2_organisation>Universität zu Köln</chair2_organisation>
  <chair2_email>franz.fischer@uni-koeln.de</chair2_email>
  <chair2_ID>1595</chair2_ID>
 </session>

 <session>
  <session_ID>105</session_ID>
  <session_title>Öffnungszeiten Konferenzsekretariat</session_title>
  <session_start>2018-03-01 08:30</session_start>
  <session_end>2018-03-01 17:30</session_end>
  <session_room_ID>5</session_room_ID>
  <session_room>Hörsaal F, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>115</session_ID>
  <session_short>VP_4a</session_short>
  <session_title>Digitale Literaturwissenschaft</session_title>
  <session_start>2018-03-01 09:00</session_start>
  <session_end>2018-03-01 10:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Viehhauser, Gabriel</chair1>
  <attendee_count>3</attendee_count>
  <chair1_name>Gabriel Viehhauser</chair1_name>
  <chair1_organisation>Universität Stuttgart</chair1_organisation>
  <chair1_email>viehhauser@ilw.uni-stuttgart.de</chair1_email>
  <chair1_ID>1135</chair1_ID>
  <sessionID>115</sessionID>
  <presentations>3</presentations>
  <p1_paperID>214</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Krautter, Benjamin</p1_authors>
  <p1_organisations>Universität Stuttgart, Deutschland</p1_organisations>
  <p1_emails>Benjamin.Krautter@gmail.com</p1_emails>
  <p1_presenting_author>Krautter, Benjamin</p1_presenting_author>
  <p1_title>Quantitatives „close reading“? Vier mikroanalytische Methoden der digitalen Dramenanalyse im Vergleich.</p1_title>
  <p1_abstract>&lt;p&gt;Analog zu jüngsten Forschungsergebnissen mit prosaischen Textkorpora versucht der Beitrag eine je distinktive Figurenrede auch in dramatischen Texten zu prüfen. Neben der Stilometrie sollen drei weitere quantitative Methoden genutzt werden, um mikroanalytische Analysen durchzuführen und die Ergebnisse gegeneinander zu halten.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>227</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Jannidis, Fotis
Konle, Leonard
Zehe, Albin
Hotho, Andreas
Krug, Markus</p2_authors>
  <p2_organisations>Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland</p2_organisations>
  <p2_emails>fotis.jannidis@uni-wuerzburg.de
leonard.konle@uni-wuerzburg.de
zehe@informatik.uni-wuerzburg.de
hotho@informatik.uni-wuerzburg.de
markus.krug@uni-wuerzburg.de</p2_emails>
  <p2_presenting_author>Jannidis, Fotis
Zehe, Albin</p2_presenting_author>
  <p2_title>Analysing Direct Speech in German Novels</p2_title>
  <p2_abstract>&lt;p&gt;Detecting direct speech in fiction allows gaining insight into an important element of its narrative structure. In literary studies, there are assumptions on the factors influencing the distribution of direct speech, like genre, period and aesthetic complexity.&lt;/p&gt;
&lt;p&gt;This paper aims to provide a detailed analysis of the use of direct speech across different time periods and domains. To create a reliable database for these analyses, we need to measure the usage of direct speech in a large and representative corpus. This task is more challenging than it may sound: While direct speech is often marked very explicitly by the use of quotes, this has not always been consistently the case. Many historical novels are not available in a well edited form, meaning that there may be inconsistent use of quotation, or no quotation at all (Brunner, 2013). In this case, a more robust method for detecting direct speech is necessary.&lt;/p&gt;
&lt;p&gt;Our first contribution is therefore a method to detect direct speech using large amounts of weakly labelled data extracted from raw text. This has multiple advantages over the use of manually annotated training data: First, manually annotating large amounts of text is very time-intensive and therefore costly. Furthermore, annotations for one type of texts may not be transferrable to other types, leading to the necessity of new annotated data for new corpora. Being able to learn from the already existing weakly labelled data is therefore desirable, as this data can automatically be extracted for a news corpus.&lt;/p&gt;
&lt;p dir="ltr"&gt;Our second contribution is applying this approach to explore the development of direct speech in the nineteenth century as well as the differences of current high literature and pulp fiction to gain insights into narrative trends.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>289</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Henny-Krahmer, Ulrike
Betz, Katrin
Schlör, Daniel
Hotho, Andreas</p3_authors>
  <p3_organisations>Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland</p3_organisations>
  <p3_emails>ulrike.henny@uni-wuerzburg.de
katrin.betz@uni-wuerzburg.de
schloer@informatik.uni-wuerzburg.de
hotho@informatik.uni-wuerzburg.de</p3_emails>
  <p3_presenting_author>Henny-Krahmer, Ulrike
Schlör, Daniel</p3_presenting_author>
  <p3_title>Alternative Gattungstheorien: Das Prototypenmodell am Beispiel hispanoamerikanischer Romane</p3_title>
  <p3_abstract>&lt;p&gt;Die Einteilung von Texten in Gattungen oder Klassen ist in den Literaturwissenschaften und den Digital Humanities ein aktuelles, ungelöstes Problem. Gattungsbegriffe können als Sammelbegriffe verstanden werden, die erfassen, in welcher Hinsicht Texte zu Textgruppen zusammengefasst werden können. Dabei stellt sich die Frage, um welche Art von Kategorien es sich bei dem Gattungsbegriff handelt (Kayser 1956: 330-387, Zymner 2003) und wie die Zugehörigkeit von Texten zu Gattungen definiert ist. In den Digital Humanities ist von Interesse, ob und wie Textgattungen anhand formaler Merkmale zu erkennen sind (Calvo Tello et al. 2017, Hettinger et al. 2016a, Hettinger et al. 2016b, Schöch et al. 2016, Schöch 2015, Schöch 2013). Das Ziel dieses Beitrags ist es, die Problematik der Gattungsklassifikation auf der theoretischen Basis moderner Gattungstheorien und mit Hilfe informatischer Mittel aus einer neuen Perspektive zu betrachten. Dazu wird exemplarisch die Anwendbarkeit des Prototypenmodells als alternatives Gattungskonzept für digitale gattungsstilistische Studien überprüft. Als Testkorpus dienen Fallbeispiele aus der hispanoamerikanischen Romanliteratur des 19. Jahrhunderts.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>123</session_ID>
  <session_short>VP_4b</session_short>
  <session_title>Der sehende Computer I</session_title>
  <session_start>2018-03-01 09:00</session_start>
  <session_end>2018-03-01 10:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Schelbert, Georg</chair1>
  <attendee_count>6</attendee_count>
  <chair1_name>Georg Schelbert</chair1_name>
  <chair1_organisation>Humboldt-Universität zu Berlin</chair1_organisation>
  <chair1_email>georg.schelbert@hu-berlin.de</chair1_email>
  <chair1_ID>1243</chair1_ID>
  <sessionID>123</sessionID>
  <presentations>3</presentations>
  <p1_paperID>115</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Hodel, Tobias</p1_authors>
  <p1_organisations>Staatsarchiv des Kantons Zürich, Schweiz</p1_organisations>
  <p1_emails>tobias.hodel@hist.uzh.ch</p1_emails>
  <p1_presenting_author>Hodel, Tobias</p1_presenting_author>
  <p1_title>Konsequenzen automatischer Texterkennung – Ein Aufriss zur Texterkennung mit Machine Learning</p1_title>
  <p1_abstract>&lt;p&gt;Die automatisierte Erkennung von großen Textmengen und die Arbeit mit erkanntem Text gehören zum geisteswissenschaftlichen Alltag. Bei gedruckten Texten wird zu Scans oder Fotografien &lt;em&gt;optical character recognition&lt;/em&gt; (OCR) erwartet. Die Suche über große Textkorpora gehört denn auch zu den Vorteilen einer digitalisierten Geisteswissenschaft. Gleichzeitig wird häufig Masse und Auswertung mit Applikationen als zentrales Argument gesetzt, um die Vorzüge der &lt;em&gt;digital humanities&lt;/em&gt; zu betonen: Die Suche nach dem Vorkommen von Begriffen und Entitäten über große Korpora ist in unterschiedlichen Ausprägungen ein Desiderat. Ansätze wie etwa&lt;em&gt; topic modeling &lt;/em&gt;sind ausgesprochen mächtig, um mehrere tausend Seiten oder hunderte von Büchern auszuwerten (Schöch 2017). Alle diese Vorgehen brauchen jedoch erkannte Volltexte.&lt;/p&gt;
&lt;p&gt;Sowohl Korpuslinguistik als auch Literatur- und Geschichtswissenschaften sind interessiert am Auffinden von Einzelbelegen, Mustern oder Entitäten in grossen Datenmengen. Der Prozess der Erkennung, der Weg zu den durchsuchbaren Texten steht in den Überlegungen der Fächer jedoch meistens nicht im Zentrum. Obwohl Probleme der OCR-Erkennung angemerkt werden, ist die Textgüte nur bedingt ein Feld der Reflexion (Ausnahme: Piotrowski 2012), die über Klagen hinausläuft. Bedingt durch die Nutzung kommerzieller Produkte und entsprechend nur bedingt offengelegter Prozesse, wird die Erkennleistung als gegeben angenommen und höchstens im&lt;em&gt; post-processing &lt;/em&gt;die Qualität der Texte verbessert (bspw. PoCoTo: Vobl 2014).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Welcher Text?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Über das monieren von Fehllesungen heraus, fehlt eine Reflexion, wie mit automatisch erkannten Texten umgegangen werden soll gänzlich. Die Frage „welcher Text erkannt werden soll“, wird nicht thematisiert. Das hängt auch damit zusammen, dass Textverständnisse im digitalen Raum geprägt sind durch die Editionswissenschaften, einer Fachschaft, die aus einer anderen Richtung das digitale Feld bearbeitet. Die Qualität der Texterkennung, etwa der Transkription handschriftlicher Dokumente, steht nicht im Fokus, da bei menschlicher Erkennung durch Experten von einer Güte um 99,99% ausgegangen werden kann. Die Unsicherheiten, die unsicheren Lesungen sind, höchstens Teil von Paläographie orientierter, fachspezifischer Debatten und nicht grundsätzlich ein qualitativer Messwert.&lt;/p&gt;
&lt;p&gt;Die Editionswissenschaften waren es auch, die im Zuge der Digitalisierung Überlegungen zum Verständnis von Text hervorbrachten und sich – ganz im Sinne post-moderner Texttheorie – darauf einigten, dass es nicht den neutralen zu edierenden Text gibt. Ausgefaltet und auch visuell umgesetzt durch das Textrad von Sahle (Sahle 2013: 45-52). Erst das Verständnis von der Mehrschichtigkeit und Formbarkeit des Textbegriffes machte mehr oder minder konsequente Umsetzungen von digitalen Editionen überhaupt möglich und entspannte die Diskussion zwischen Philologie und Geschichtswissenschaft zu den je eigenen Vorstellungen von Text(-aufbereitung).&lt;/p&gt;
&lt;p&gt;Gerade für Fragen zur Umsetzung von Editionen, hilft das Textrad bei der Identifikation von Schwerpunkten, die Auszeichnungs- und Editionsentscheidungen unterstützen. Aber auch bei einzelnen Auszeichnungen, oder der Eruierung des Verhältnisses von Auszeichnung und Registerdatenbank, ist die Reflexion von Textvorstellungen hilfreich, um editorische Entscheide nachvollziehbar zu machen und konsequent handzuhaben. &lt;/p&gt;
&lt;p&gt;Im Rahmen von Projekten zur Texterkennung und Handschriftenerkennung durch grosse EU-Infrastrukturprojekte wurden selten Überlegungen zum Text als Ressourcen angestellt und mehr auf Nachfragen beziehungsweise der Übernahme impliziter Vorstellungen abgestellt, die durch Informatiker oder Mathematiker bei der Entwicklung von Erkennungsalgorithmen eingebracht wurden. Dadurch muss auch deren Perspektive bei einer Theoretisierung der Texterkennung mitberücksichtigt werden.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Automatisch erkannter Text&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Die Anwendung des Sahleschen Textrads auf automatisierte Texterkennung macht deutlich, dass gewisse Textformen auch mit besten Erkennmethoden nicht isoliert werden können: Ausgangspunkt ist immer eine Textversion (Druck- oder Manuskriptseite), die in Form eines Faksimilie/Digitalisat vorliegen muss. Zeichenhaftigkeit und auch intellektuelle Bezüge sind daraus ableitbar, Text als Werk etwa, wie es rekonstruiert oder abstrahiert wird, lässt sich dagegen nicht erkennen.&lt;/p&gt;
&lt;p&gt;Das digitalisierte Objekt agiert bei der automatisierten Erkennung jeweils als Ausgangspunkt, das erneut konsultiert werden kann und bei Kontrolle und Überprüfung hilft. Das bedingt jedoch, dass auch die Art und Weise der Digitalisierung (Auflösung und Farbechtheit aber auch Format und Aufnahmeverfahren), bei einer Kritik berücksichtigt werden müssen. Bereits der zugrundeliegende „Text“ ist also technisch geprägt. &lt;/p&gt;
&lt;p&gt;Mit Fokus auf die Ausgabe des Erkennprozesses, werden erkannte Strings ins Zentrum gesetzt. Dabei muss nicht zwangsläufig nur ein String („die beste Lesung“) vorgelegt werden, sondern Varianten, also eine Reihe von Strings, die mehrere mögliche Lesungen enthalten, sind extrahierbar. Ergänzt um die durch die maschinell errechnete Wahrscheinlichkeit der Erkennung wird eine Matrix (sog. &lt;em&gt;confidence matrix&lt;/em&gt;) an möglichen Lesungen und deren Wahrscheinlichkeit erstellt, die ebenfalls durchsucht werden kann. Insbesondere innerhalb von grossen Quellenmassen lassen sich so potente Suchen (Volltextsuchen ohne zugrunde liegendem Volltext sozusagen) realisieren, die ausgesprochen gute Ergebnisse erzielen. Mit dem Nachteil, dass je nach Suche auch&lt;em&gt; false-positive &lt;/em&gt;Variantenlesungen vorgelegt werden. Die Methode wird daher nur bedingt für Auswertungen nutzbar, die auf Quantifizierung beruhen.&lt;/p&gt;
&lt;p&gt;Innerhalb des Vorgangs zur Erkennung von Text, kommt der eigentliche Erkennprozess jedoch erst an zweiter Stelle. Ebenso zentral und – leider auch – ebenso fehleranfällig, ist die Identifikation des Layout bzw. die Unterscheidung zwischen texttragenden und textfreien Zonen auf den zu erkennenden Digitalisaten. Sowohl im Umgang mit handschriftlichem Material aber auch bei komplexen gedruckten Werke, insbesondere Tabellen, schafft die Layoutanalyse noch keine oder nur eine ungenügende Identifikation.&lt;/p&gt;
&lt;p&gt;Der Punkt der Layouterkennung wird noch problematischer, da nur schwierig ausgewiesen werden kann, welche Teile als „texttragend“ identifiziert wurden. Allen Verfahren gemein ist der Bezug auf von Menschen hergestellte, relativ subjektive Grundlagen. Fakt ist, alles was nicht als Teil des Layouts identifiziert wird, kann im darauffolgenden Prozess nicht als Text erkannt werden.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Vor-)Entscheidungen&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Alle diese Überlegungen stellen nicht mehr als Grundlagen beziehungsweise Vorannahmen dar, die getroffen werden, bevor eine Erkennung überhaupt stattfinden kann. Im Gegensatz etwa zu händisch erstellten digitalen Editionen, ist eine Anpassung des Textbegriffs nicht im Erstellprozess möglich sondern höchstens vor Beginn oder beim Abschluss der Bearbeitung.&lt;/p&gt;
&lt;p&gt;Noch deutlicher wird die Abgeschlossenheit, nähert man sich der Texterkennung aus einer technischen Perspektive. Insbesondere für die Texterkennung von Handschriften und frühen Drucken lohnt sich der Einsatz von &lt;em&gt;machine learning&lt;/em&gt;, konkret rekurrenten neuronalen Netzen (rNN). Das Training solcher Netze muss selbstredend vor der eigentlichen Erkennung erfolgen. Die trainierte Ausgabe entspricht dabei einem determinierten menschlichen Input, der nachgeahmt wird und je nach Grösse des Trainingssets und der Variabilität der Schriften mehr oder weniger genau erreicht wird. Zentral im Prozess ist das erwartete Resultat oder anders formuliert die Art und Weise wie Text aufbereitet wird. Die Aufbereitung selbst, beispielsweise die stillschweigende Auflösung von Abkürzungen, Normalisierung von Schreibungen oder das Einfügen bzw. Zusammenführen von Zeilenumbrüchen, wird Konsequenzen auf die Ergebnisse haben. Auch der verwendete Zeichensatz (etwa die Codierung in Unicode) oder Vereinheitlichungen wird das Resultat beeinflussen.&lt;/p&gt;
&lt;p&gt;Aus technischer Sicht gibt es innerhalb des Trainingsprozesses selbst nur einige wenige Parameter, die kontrolliert werden können. Entsprechend ist der gesamte Rest, Teil einer &lt;em&gt;blackbox&lt;/em&gt;, die auch nicht näher analysiert werden kann, da das funktionieren der einzelnen Neuronen in einem Netz nur schwer und häufig ohne Einsichten zum Funktionieren der gesamten Erkennung beobachtet werden können.&lt;/p&gt;
&lt;p&gt;Ein Kontrollmechanismus findet sich einzig im standardisierten Testen der trainierten Modelle mit Hilfe von Testsets, also nach gleichem Muster hergestellte Seiten, die nicht für’s Training verwendet wurde und entsprechend Auskunft über die Leistungsfähigkeit eines Modells geben können. Zentrale Messwerte dabei sind Character Error Rate und Word Error Rate.&lt;/p&gt;
&lt;p&gt;Eine weitere Form der Einflussnahme besteht in der Verwendung von Wörterbüchern, die bei Unsicherheit herangezogen werden und plausiblere Lesungen (= im Wörterbuch) gegenüber anderen Strings bevorzugen. Für historische Schreibformen bestehen zwar Korpora, jedoch ist der Einsatz für Texte vor Ende des 19. Jahrhunderts (insbesondere für vormoderne Texte) umstritten, da keine Konventionen bestanden und die Gefahr der Hyperkorrektur aufgrund des verwendeten Wörterbuchs besteht.&lt;/p&gt;
&lt;p&gt;Auch wenn bislang nur beschränkt Erfahrungen mit dem Einsatz von&lt;em&gt; machine learning&lt;/em&gt; bei der automatischen Erkennung von Text besteht, ist absehbar, dass die Verbesserungen zum Einsatz der Technologie führen werden. Mit dem Preis, dass Probleme des&lt;em&gt; machine learnings &lt;/em&gt;mit eingeführt werden. Die &lt;em&gt;biases&lt;/em&gt;, insbesondere die Perspektiven im Moment der Aufbereitung, von Trainingsdaten etwa werden übernommen (Zundert 2016: 341). Im Kontext von automatisierter (Vor-)Aufbereitung von Bewerbungsdossiers oder nicht gender-gerechten Auswertungen von Informationsmassen wird das Probleme des Datenbias bei Methoden des &lt;em&gt;machine learnings&lt;/em&gt; rasch sichtbar (Siehe dazu einen jüngeren Artikel aus dem britischen Guardian, Devlin 2017). Da die Konsequenzen bei der Texterkennung gesellschaftlich weniger gravierend sind, fallen sie vielleicht weniger auf, problematisch und kritisch zu analysieren sind sie nichtsdestotrotz.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ansprüche an automatische Texterkennung&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Wie der kurze Abstecher in die Welt des maschinellen Lernens zeigte, ist eine Kontrolle der Erkennleistung nur ganz bedingt und basierend auf wenigen Faktoren möglich, entsprechend lässt sich zusammenfassend im Umgang mit automatisierten Texterkennungsalgorithmen eine Reihe von Forderungen ableiten, damit erkannte Texte kritisch eingeordnet werden können.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Messwerte basierend auf Testsets müssen ausgewiesen werden&lt;/strong&gt;: Character Error Rate und Word Error Rate geben Aufschlüsse zur Qualität des erkannten Textes. Darüber hinaus ist eine Einschätzung sinnvoll, in welchem Bereich Falschlesungen häufig identifiziert wurden (Eigennamen, Zahlen etc.). Insgesamt sollte dadurch einsichtig werden, in welchen Bereichen Qualitätsprobleme zu erwarten sind.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Standards und Kernfragen an die publizierten Texte offen dokumentieren&lt;/strong&gt;: In den Editionswissenschaften bereits praktiziert, wird der Umgang mit Frageperspektiven verbesserte Einsichten liefern, vor welchem Hintergrund Textkorpora erstellt wurden. Daran schliesst sich die Forderung nach &lt;strong&gt;Offenlegung der zugrunde liegenden Grund-Truth zur Erstellung von Test- und Trainingssets &lt;/strong&gt;an: Damit wird nachvollziehbar, was zur Modellerstellung genutzt und auch, welchen Standards, Richtlinien und Gepflogenheiten dabei gefolgt wurde. &lt;/p&gt;
&lt;p&gt;Durch den kritischen Umgang mit automatisch erkannten Texten, eröffnet sich ein fundierter Umgang mit denselben, der mit gewissen Sicherheiten eine Weiternutzung von Text ermöglicht und die textzentrierten Teile der&lt;em&gt; digital humanities&lt;/em&gt; in eine kritikfähige Zukunft führt.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>213</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Dunst, Alexander
Hartel, Rita</p2_authors>
  <p2_organisations>Universität Paderborn, Deutschland
Universität Paderborn, Deutschland</p2_organisations>
  <p2_emails>alexander.dunst@gmail.com
rst@upb.de</p2_emails>
  <p2_presenting_author>Dunst, Alexander</p2_presenting_author>
  <p2_title>Auf dem Weg zur Visuellen Stilometrie: Automatische Genre- und Autorunterscheidung in graphischen Narrativen</p2_title>
  <p2_abstract>&lt;p&gt;In diesem Vortrag stellen wir automatische Bildanalysen vor, die stilometrische Unterscheidungen zwischen Genres und Autoren auf den Bereich multimodaler Erzählungen übertragen.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>223</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Laubrock, Jochen
Dubray, David
Krügel, André</p3_authors>
  <p3_organisations>Universität Potsdam, Deutschland
Universität Potsdam, Deutschland
Universität Potsdam, Deutschland</p3_organisations>
  <p3_emails>laubrock@uni-potsdam.de
ddubray@uni-potsdam.de
kruegel@uni-potsdam.de</p3_emails>
  <p3_presenting_author>Laubrock, Jochen</p3_presenting_author>
  <p3_title>Computationale Beschreibung visuellen Materials am Beispiel des Graphic Narrative Corpus</p3_title>
  <p3_abstract>&lt;p&gt;Die digitale Revolution in den Geisteswissenschaften hat diesen eine Reihe neuer Methoden eröffnet. Durch die heute verfügbaren großen Datenmengen und intelligenten Algorithmen haben sich zwar einige bisher offengeliebene geisteswissenschaftliche Kernfragen beantworten lassen, jedoch wird mancherorts eine Methodenfokussiertheit und Theoriemangel kritisiert (Gumbrecht, 2014). Ob die Digitalen Geisteswissenschaften zu einer darüber hinausgehenden tiefergreifenden Veränderung der geisteswissenschaftlichen Erkenntnis führen werden, bleibt abzuwarten; derzeit scheint es, als werde das Potenzial der neuen methodischen Zugänge erst noch ausgelotet. In anderen Wissenschaften hat aber die Verfügbarkeit computationaler Modelle und der damit einhergehende Zwang, implizite Annahmen zu explizieren und Theorien formal testbar zu machen, signifikant zur Theoriebildung und -prüfung beigetragen (cf. Myung &amp; Pitt, 2002; Lewandowsky &amp; Farrell, 2011). Deshalb besteht die begründete Hoffnung, dass computationale Modellierung auch die Geisteswissenschaften bereichern wird.&lt;/p&gt;
&lt;p&gt;Ein Großteil der Forschung in den digitalen Geisteswissenschaften beschäftigt sich mit Text. Es gibt hier eine fruchtbare interdisziplinäre Zusammenarbeit von Literaturwissenschaften und Computerlinguistik; im Umfeld des "Distant Reading” sind umfangreiche Werkzeuge entstanden, mit denen sich etwa stilometrische Analysen oder Topic Modeling computergestützt vornehmen lassen (Blei, 2012; Juola, 2006). Auch netzwerkanalytische Methoden aus der theoretischen Physik und computationalen Soziologie haben hier interessante neue Perspektiven eröffnet (Schich et al., 2014). Dagegen ist die digitale Analyse visuellen Materials noch relativ wenig entwickelt oder standardisiert, obwohl dieses für Disziplinen wie z.B. Kunstgeschichte oder Archäologie von zentralem Interesse ist. In den letzten Jahren wurden durch Entwicklungen im Bereich der Convolutional Neural Networks (CNN) die Möglichkeiten automatisierter Bildanalyse revolutioniert. Während in klassischen Ansätzen der maschinellen Bildverarbeitung ein hohes Ausmaß an Expertenwissen notwendig war, um Merkmale zu definieren, mit denen sich das Material sinnvoll beschreiben ließ ("engineered features"), lernen CNNs die Merkmale durch Fehlerrückführung (Backpropagation) selbst.&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks sind eine besondere Klasse künstlicher neuronaler Netze, die sich durch eine 2D-Anordnung der Neuronen, innerhalb einer Schicht geteilte Gewichte und lokale Konnektivität auszeichnen. Deshalb kodieren sie implizit Ortskoordinaten und eignen sich insbesondere für die Analyse von Bildmaterial. Die Netze sind typischerweise auf einer großen Anzahl von Fotos in Objektklassifikationsaufgaben trainiert worden, dabei bilden sich auf verschiedenen Ebenen der CNNs Repräsentationen aus, die denen im menschlichen visuellen System ähnlich sind. Neuronen auf niedrigen Ebenen des Netzwerks haben oft eine Filterantwort, die relativ einfache Merkmale kodiert, vergleichbar z.B. mit Kantendetektoren im frühen visuellen Kortex, während Neuronen auf höheren Ebenen recht komplexe Merkmale kodieren können, z.B. Texturen oder Teile von Gesichtern. Da diese Merkmale relativ generisch sind, ist zu erwarten, dass Transfer auf neuartiges Material gelingt. Es sind heute einige derart vortrainierte Netzwerke verfügbar, die sich mit relativ wenig Aufwand an neues Material anpassen lassen. Der Vergleich der Gewichte für verschiedene Materialtypen erlaubt dann auch Rückschlüsse über deren Unterschiede.&lt;/p&gt;
&lt;p&gt;Generalisieren die auf Fotos vortrainierten Netzwerke auch auf zeichnerisches Material? Wir berichten von Experimenten, in denen wir das Material des Graphic Narrative Corpus (GNC, Dunst et al., 2017) mit CNNs beschreiben. Der Graphic Narrative Corpus repräsentiert das erste digitale Korpus von englischsprachigen Graphic Novels mit derzeit 130 Titeln. Die ersten Kapitel dieser Werke werden von menschlichen Kodierern annotiert, dabei werden u.a. die Identität und der Ort zentraler Charaktere, Orte von Panels, Sprechblasen und Textboxen (Captions) und Onomatopeia sowie der Text selbst notiert. Außerdem werden Blickbewegungen von Lesern erhoben (Eye-Tracking), um Aufschluss über die Aufmerksamkeitsverteilung auf Seite der Rezipienten zu erhalten.&lt;/p&gt;
&lt;p&gt;Die Beschreibung des GNC mit CNNs hat verschiedene Ziele. Erstens erhoffen wir uns Aufschluss über stilistische Unterschiede zwischen Werken und Genres. Allgemeiner könnte so der Weg zu einer visuellen Stilometrie aufgezeigt werden, die auch für inhaltliche Bereiche außerhalb der Graphic Novels relevant ist, etwa im Sinne einer computationalen Kunstgeschichte (Saleh &amp; Elgammal, 2015; Manovich, 2015). Zweitens ermöglicht die Beschreibung mit Hilfe der Merkmale tiefer CNNs durch sogenannte Region Proposal Networks (Girshick et al., 2013) die Detektion von Objektklassen. Beispielsweise könnten sich Sprechblasen oder handelnde Charaktere lokalisieren lassen. Wenn Klassen von Objekten automatisiert lokalisiert werden können, erleichtert dies die Arbeit der Annotatoren sehr. Die Ergebnisse können also zurück in das Annotationswerkzeug fließen, um eine Teilautomatisierung zu ermöglichen. Drittens ist aus kognitionspsychologischer Perspektive interessant, welche Merkmale die Aufmerksamkeit auf sich ziehen. Die Korrelation der Netzwerkbeschreibung mit den Blickbewegungsdaten ermöglicht eine Modellierung der Aufmerksamkeitssteuerung auf einem deutlich höheren Auflösungsgrad als die subjektive Beschreibung.&lt;/p&gt;
&lt;p&gt;Für die Modellierung des Materials nutzen wir die Architektur VGG der Visual Geometry Group in Oxford (Simonyan &amp; Zisserman, 2014), insbesondere VGG-16 und VGG-19. Diese Wahl ist motiviert durch die Einfachheit der Architektur, die die Interpretation der Gewichte erleichtert. Das zugrundeliegende Netzwerk lässt sich aber prinzipiell austauschen; andere Architekturen wie ResNet (He et al., 2015) oder Inception (Szegedy et al., 2015) sind denkbar und sollten ähnlich gute Ergebnisse liefern. Für die Vorhersage der Aufmerksamkeitsverteilung der Leser nutzen wir die Architektur Deep Gaze II (Kümmer et al., 2016). Deep Gaze II ist ein neuronales Netz, das auf VGG-19 aufsetzt und die Antwort einiger dessen Schichten nutzt, um "empirische Salienz" vorherzusagen. Empirische Salienz ist operationalisiert durch Messung von Mauspositionen beim Aufdecken eines verschwommenen Bildes bzw. Messung von Blickbewegungsdaten beim Betrachten von Fotos natürlicher Szenen. Die Fotos sind andere, als die für das Training von VGG-19 benutzten. Man beachte, dass sowohl VGG-19 als auch Deep Gaze II auf Fotos trainiert wurden, also nie Graphic Novels gesehen haben. Da sie jedoch Merkmale und Gewichte herausgebildet haben, die für die Interpretation (von Bildern) der menschlichen Umwelt nützlich sind, kann man vermuten, dass sie sich auch für die Analyse von Zeichnungen eignen. Zwar sind Zeichnungen Abstraktionen, haben aber als solche einen Bezug zur visuellen (Photo-)Realität.&lt;br /&gt; Die Ergebnisse zeigen, dass sich mit Hilfe von Neuronen auf höheren Ebenen der tiefen CNNs recht gut bestimmte Klassen von Objekten lokalisieren lassen. Beispielsweise eignen sich einige Kombinationen von Merkmalen zuverlässig als Sprechblasendetektoren (Abb. 1). Dies ist insofern bemerkenswert, als die Detektion von Sprechblasen sich für klassische Ansätzen der maschinellen Bildbverarbeitung als schwieriges Problem dargestellt hat (Rigaud et al., 2013). Auch für die Erkennung gezeichneter Gesichter eignen sich CNNs, allerdings ist hier ein Training auf Ansichten in verschiedenen Perspektiven (Frontal, Profil) notwendig. Und schließlich lässt sich die empirische Fixationsverteilung mit Deep Gaze II insgesamt sehr überzeugend reproduzieren (Abb. 2). Die CNN-Features kodieren also aufmerksamkeitsrelevante Merkmale. Insgesamt eigenen sich auf Fotos trainierte CNNs schon ohne spezifisches weiteres Training recht gut zur Beschreibung gezeichneten Materials in Graphic Novels.&lt;/p&gt;
&lt;p&gt;Die "objektive" Beschreibung eröffnet vielfältige Anwendungen. Einerseits kann, wie oben skizziert, die Annotation visuellen Materials durch Nutzung von vorgeschlagener Regionen deutlich erleichtert werden, etwa vergleichbar mit dem bei verbalen Material durch Verwendung von Optical Character Recognition (OCR) ermöglichten Übergang von kompletter Transkription zum Korrekturlesen. Andererseits sind durch das Vorliegen visueller Merkmale (Features) vielfältige stilometrische Anwendungen denkbar. Zum Beispiel lassen sich aufgrund der Merkmale Ähnlichkeiten verschiedener Zeichner und Künstler berechnen und durch Klumpenbildung (Clustering) im Merkmalsraum auch Stile definieren. Auch die weitergehende Exploration der Repräsentation auf verschiendenen Schichten des Netzwerks scheint eine vielversprechende Aufgabe weiterer Forschung. Beispielsweise könnte der Vergleich der Antworten auf fotographische versus zeichnerisch abstrahierte Abbilder von Exemplaren einer Kategorie Hinweise auf das Wesen der Abstraktion geben, oder es lassen sich visuelle Merkmale identizieren, die in besonderem Ausmaß die Aufmerksamkeitszuwendung im Leseprozess und bei der Rezeption von Zeichnungen leiten.&lt;/p&gt;
&lt;p&gt;Wir haben beispielhaft aufgezeigt, wie sich Werkzeuge der mathematisch-computationalen Modellierung eignen, um grafisches Material zu analysieren und zu beschreiben. Die Hoffnung ist, dass eine visuelle Stilometrie die Digitalen Geisteswissenschaften im Bereich visuellen Materials in ähnlicher Art und Weise bereichert wie computerlinguistische Ansätze im Bereich der Textanalyse. Digitale Analysen liefern mächtige neue Werkzeuge, die mittel- bis längerfristig auch eine neue Theoriebildung fördern könnten.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>124</session_ID>
  <session_short>VP_4c</session_short>
  <session_title>Wissenschaftsorganisation II</session_title>
  <session_start>2018-03-01 09:00</session_start>
  <session_end>2018-03-01 10:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Scholger, Walter</chair1>
  <attendee_count>4</attendee_count>
  <chair1_name>Walter Scholger</chair1_name>
  <chair1_organisation>Universität Graz</chair1_organisation>
  <chair1_email>walter.scholger@uni-graz.at</chair1_email>
  <chair1_ID>1450</chair1_ID>
  <sessionID>124</sessionID>
  <presentations>3</presentations>
  <p1_paperID>159</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Moeller, Katrin</p1_authors>
  <p1_organisations>Martin-Luther-Universität Halle-Wittenberg, Deutschland</p1_organisations>
  <p1_emails>katrin.moeller@geschichte.uni-halle.de</p1_emails>
  <p1_presenting_author>Moeller, Katrin</p1_presenting_author>
  <p1_title>Ist kooperativ jetzt umsonst? Die Ausweisung von Datenautorenschaft als neue Form wissenschaftlicher Reputation zur Förderung offener Forschungsdatenkulturen</p1_title>
  <p1_abstract>&lt;p&gt;&lt;strong&gt;Vorbemerkung&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Die Bedeutung von Forschungsdatenmanagement ist mittlerweile umfänglich in der Wissenschaftskultur angekommen. Die Vorteile der Open Data Sciences überzeugen schnell, auch wenn die jüngst vom Rat für Informationsinfrastrukturen (RfII) pointiert formulierten Herausforderungen (RfII 2016, RfII 2017) weiterhin bestehen. Sie betreffen vor allem den Umgestaltungsprozess hinsichtlich des Aufbaus einer Landschaft von dauerhaften, stabilen Infrastrukturangeboten der Langzeitarchivierung, die durch die Anpassung und Koordination von Fördermechanismen, Personalentwicklung, Qualitätssicherung und der Entwicklung einer neuen "Forschungsdatenkultur" geprägt sind (AG Datenzentren 2017: 2-3). Den letzten Punkt möchte ich mit meinem Vortrag aufgreifen und aus dezidiert geisteswissenschaftlicher Perspektive einer datenproduzierenden und -bewahrenden Institution einen Beitrag zum Forschungsdatenmanagement formulieren, der meines Erachtens bisher kaum reflektiert wird: Es geht um die Entwicklung neuer Forschungsdatenkulturen, um Anreize und vor allem darum, welche wissenschaftliche Reputation sich mit der Nachnutzung von Forschungsdaten für den Urheber von Daten verbindet. Welche Anreize können Forschenden geboten werden, Daten tatsächlich zur möglichst flexiblen Nachnutzung frei zu geben? Diskutiert wird dazu nicht die Datenautorenschaft selbst, die über das Urheberrecht längst etabliert ist (Beer u.a. 2014: 4) und die heute bereits durch die Veröffentlichung von Forschungsdaten auf einem Forschungsdatenrepositorium über Lizenzen geregelt wird (CreativeCommons 2017, Beer u.a. 2014: bes. 24f.). Hier geht es vielmehr um eine Diskussion, welchen weiteren Weg die Datenautorenschaft nach der Veröffentlichung in einem anerkannten Repositorium nimmt und wie Datenautorenschaft innerhalb der Wissenschaft mehr Anerkennung, Sogkraft und Reputation entfalten. Grundlegend kann dies zu einer größeren Bereitschaft von Forschenden zum Data-Sharing beitragen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Was beinhaltet ausgewiesene Datenautorenschaft?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Momentan werden alle Urheber eines wissenschaftlichen Forschungsergebnisses in Textpublikationen unabhängig vom inhaltlichen Beitrag als Autoren genannt. Es ist sinnvoll, hier ein Unterscheidungskriterium einzuführen. Dabei wird strikt nach den eigentlichen Textautoren und allen anderen Beiträgern eines wissenschaftlichen Forschungsergebnisses unterschieden. Textautoren sind ausschließlich diejenigen, die maßgeblich den Inhalt eines Textes verfassen, der das Forschungsergebnis und die Analyse repräsentiert. Sie sind für die Inhalte des Beitrags verantwortlich. Neben diesen Autoren für den Text werden dann alle weiteren Autoren als "Datenautor(en)" oder "Datengeber" geführt. Sie haben ebenfalls am Zustandekommen des Forschungsergebnis wesentlichen Anteil. Diese Beteiligung kann bspw. darin bestehen, prinzipiell urheberrechtlich geschützte Daten über Lizenzen offen für eine Analyse zur Verfügung zu stellen. Wichtig ist dabei, dass die Daten des Datengebers - anders als beim Zitat - in einem wesentlichen Umfang (mehr als ein Drittel) Verwendung finden. Es ist dahingehend nicht entscheidend, ob die Daten im Datenkonvolut des Datennutzers auch einen wesentlich Bestandteil bilden oder letztlich nur einen kleinen Baustein ausmachen.&lt;/p&gt;
&lt;p&gt;Neben die Autoren, Herausgeber und Übersetzer würde also eine weitere qualitative Gruppe der in einer Textpublikation genannten Autoren treten, die nun allerdings explizit keinen eigenen Beitrag am Text, wohl aber einen unmittelbaren, wesentlichen Beitrag zur Quellenbasis eines Forschungsergebnisses leisten. Dieses Kriterium ist das wesentliche Unterscheidungsmerkmal zum Textautor. In den Metadaten einer Publikation würden diese Autoren analog zu den Herausgebern eines Sammelwerks mit einem geeigneten Kürzel benannt (DA) und ggf. auch separat ausgewiesen. Für den Datenautor zählt diese Nennung aber als weitere Publikation, wenn auch mit einem abgestuften Renommee.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wozu brauchen wir extra ausgewiesene Datenautorenschaft?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;a) Vergleich verschiedener Methoden zur Messung wissenschaftlicher Leistung&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Es ist relativ einfach zu zeigen, warum viele WissenschaftlerInnen zögerlich bleiben, Daten zu teilen. Vor allem in der Geisteswissenschaft gibt es dafür eine Reihe von Gründen, von denen hier nur einige wenige knapp skizziert werden sollen:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Datenproduktionen in den Geisteswissenschaften sind kosten-, zeit- und personalintensiv, für Datenbereinigung und Dokumentation müssen zusätzliche Aufwendungen gemacht werden&lt;/li&gt;
&lt;li&gt;Erschließungsprozesse von Medien und Quellen sind Teil des wissenschaftlichen Forschungsprozesses mit eigener Fachdisziplin (z. B. den Grund- und Hilfswissenschaften in der Geschichtswissenschaft)&lt;/li&gt;
&lt;li&gt;Forschungsdaten unterstehen damit - vielleicht auch im Unterschied zu sensorgestützten Datenproduktion in einigen naturwissenschaftlichen Sparten - dem Urheberrecht, da sie über eine eigenständige wissenschaftliche Leistung mit ausreichender Schöpfungshöhe verfügen. Dies gilt umso mehr, wenn Forschungsdaten nicht nur die Wiedergabe einer einzelnen Quelle in Form von Transkriptionen repräsentieren, sondern über den Erschließungsprozess mit einer Vielzahl von Annotationen und editionskritischen Anmerkungen versehen werden oder komplexe Datenstrukturen einer ganzen Serie von Quellen kombinieren.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Zunächst möchte ich kurz einen Vergleich des Umgangs mit der Datenautorenschaft in den verschiedenen Forschungsdisziplinen anstellen. Grundlegende Unterschiede liegen in der Messbarkeit der Forschungsleistung über mehr oder weniger umstrittene Methoden der Auszählung von Zitationsraten, die in den STM-Fachdisziplinen (Naturwissenschaften, Technik, Medizin) eine etablierte Forschungspraxis repräsentieren. Abgesehen davon, dass der Journal Impact Factor auch in den STM-Fächern keineswegs unumstritten und grundsätzlich davon auszugehen ist, dass Daten in Repositorien wie auch in Open Access-Veröffentlichungen durch die bevorzugten Evaluationsmetriken nicht gleichwertig erfasst werden (Herb 2010, Kap. 1.3), ist dieses Verfahren in den Geisteswissenschaften bisher grundsätzlich nicht anwendbar (Wissenschaftsrat 2006: 48ff. Jehne 2009: 59). Die wissenschaftliche Reputation durch die Zitation von Daten greift in den Geisteswissenschaften daher nicht in der gleichen Weise, wie in den STM-Fächern. Auch in den Naturwissenschaften entstehen durch die Vervielfachung der Co-Autorenschaft erhebliche neue Unschärfen bei der Leistungsbewertung von wissenschaftlichen Ergebnissen, was wiederholt zu Reglungsbedarf der Hochschulen und der DFG führte (DFG 2013: 20). Eine Trennung von Text- und Datenautorenschaft auf der Ebene der Metadatenhaltung brächte hier wesentliche Vorteile für alle Wissenschaftsdisziplinen, da sie Beiträge an Forschungsergebnissen durch klarere Definitionskriterien wieder transparenter ausweisen würde.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;b) Zitation versus Datenautorenschaft&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Im zweiten Teil möchte ich durch einen Vergleich von Zitation und Datenautorenschaft die Vor- und Nachteile beider Prinzipien diskutieren und überlegen, wie man die Vorzüge beider Verwendungsweisen miteinander kombinieren könnte.&lt;/p&gt;
&lt;p&gt;Das Grundprinzip des Zitats ist die Belegfunktion. Ein Zitat ist nach dem Urheberrecht dann zugelassen, wenn es eigene Ideen oder Gedanken unterstützt bzw. Ideen anderer aufgreift und in den eigenen Text integriert. Ist dem Recht genüge getan, verliert nach einer Faustregel der Text auch ohne das Zitat nicht an Sinn (Schwenke 2011). Das Urheberrecht regelt im § 52a, dass in der wissenschaftlichen Forschung "kleine Teile eines Werkes, Werke geringen Umfangs sowie einzelne Beiträge aus Zeitungen oder Zeitschriften" für die wissenschaftliche Forschung genutzt werden können (Bundesministerium 2013, § 52a). Auch wenn es kein festgesetztes Limit für den Umfang eines Zitats gibt, sollte es klar begrenzt sein. Als Faustregel führt ein Zitat nicht mehr als ein Drittel eines Textes auf (Schwenke 2011). Die Verwendung großer Teile von Daten oder ganzer Datenkonvolute zur Produktion eines Forschungsergebnisses verlässt den Rahmen einer Zitation deutlich.&lt;/p&gt;
&lt;p&gt;Der wesentliche Vorteil des Zitierens ist neben seiner festen Kanonisierung in allen Wissenschaftsdisziplinen das Prinzip der Kontaktlosigkeit. Zudem wird klargestellt, dass nur der Autor inhaltliche Verantwortung für ein Forschungsergebnis trägt. Während Autoren sich untereinander abstimmen, Inhalte diskutieren und Rechte klären müssen, kann ein Wissenschaftler durch die Zitation Ergebnisse anderer unter den genannten Voraussetzungen in seine eigene Leistung einbinden und ausweisen. Die Autorenschaft ist daher organisatorisch aufwändiger und setzt die Erreichbarkeit des Urhebers voraus, wiewohl heute mit internetbasierten Referenzsystemen wie OrcID (ORCID 2017) Voraussetzungen dafür geschaffen werden.&lt;/p&gt;
&lt;p&gt;Letztlich verzichtet der Autor durch die Lizenzierung von Forschungsdatensätzen in Forschungsrepositorien weitgehend auf alle Nutzungs- und Verwertungsrechte zugunsten einer möglichst breiten Nachnutzung von Daten. Neben anonymen Lizenzen ist der meistgewählte Typ an Lizenzen vermutlich die Auflage zur Namensnennung, die über das Zitat erfolgt. Dies ist möglich, indem Datenrepositorien die Freistellung der Daten von allen Verwertungsrechten nach § 15 des Urheberrechtsvertrages in ihre Nutzungsverträge übernehmen. Während Wissenschaftler und Wissenschaftlerinnen mit solchen Formen der Freistellung von Daten oft keine größeren Probleme haben, möchten sie aber weitgehend nicht auf die wissenschaftliche Verwertung ihrer Daten und die damit in Verbindung stehende wissenschaftliche Reputation verzichten. Sie stellt die eigentliche Währung der Wissenschaft dar.&lt;/p&gt;
&lt;p&gt;Zudem möchte kein Datenautor für die Verwendung seiner Daten verantwortlich gemacht werden: Neben der formalen Trennung von Daten- und Textautor, kommt auch die inhaltliche zum Tragen. Das Forschungsergebnis verantwortet hier nur der Textautor.&lt;/p&gt;
&lt;p&gt;Daher sollten Lizenzmodelle in Forschungsrepositorien idealerweise die Vorteile der Autorenschaft (wissenschaftlicher Mehrwert) und die Vorteile des Zitierens (Kontaktlosigkeit, keine rechtliche Regelung, Klarstellung der Autorenschaft) miteinander verbinden. Möglich wäre dies, indem Datenrepositorien die Datenautorenschaft von allen Rechten freistellen (wie dies in Lizenzen der Fall ist) aber entsprechend einer Verwendung von Daten in großen Teilen die Nennung als Datenautor oder Datengeber vorschreiben. Letztlich würde sich an der bisher diskutierten Praxis der Repositorien nichts Wesentliches ändern, mit Ausnahme der veränderten Notation im Sinne eines Autors des wissenschaftlichen Ergebnisses.&lt;/p&gt;
&lt;p&gt;Datennutzer müssten lediglich zur Meldung von Veröffentlichungen beim Repositorium verpflichtet werden. Der Datengeber kann sich auf diese Weise bei Wunsch über eigene Veröffentlichung informieren.&lt;/p&gt;
&lt;p&gt;Die Unterscheidung in Text- und Datenautorenschaft ist damit vor allem eine transparente Präzisierung der einzelnen Forschungsleistungen, die rechtlich über die Nutzungsverträge der Datenrepositorien Regelung erfährt. Dieses Verfahren findet bei jenen Daten Anwendung, deren Lizenzen eine freizügige, verschneidbare Nutzung ermöglichen. Auch bei anderen Daten ist natürlich eine Trennung von Datenautorenschaft und Textproduzent möglich, hängt jedoch durch die urheberrechtlichen Beschränkungen nach wie vor an einem persönlichen Austausch der beteiligten Personen.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>235</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Kamocki, Pawel
Ketzan, Erik
Wildgans, Julia
Witt, Andreas</p2_authors>
  <p2_organisations>WWU Münster, Germany; ELDA, France; IDS Mannheim
Birkbeck, University of London
IDS Mannheim; Universität Mannheim
IDS Mannheim; Universität zu Köln</p2_organisations>
  <p2_emails>pawel.kamocki@gmail.com
eketzan@gmail.com
j.wildgans@googlemail.com
witt@ids-mannheim.de</p2_emails>
  <p2_presenting_author>Kamocki, Pawel
Witt, Andreas</p2_presenting_author>
  <p2_title>Das neue "Gesetz zur Angleichung des Urheberrechts an die aktuellen Erfordernisse der Wissensgesellschaft" und seine Auswirkungen für Digital Humanities</p2_title>
  <p2_abstract>&lt;p&gt;Forschungsdaten im Bereich der Digital Humanities sind bekanntlich häufig urheberrechtlich bzw. durch das sui-generis-Recht für Datenbanken geschützt. Dementsprechend ist eine Erhebung und Verwendung der Daten nur rechtlich zulässig, wenn der Rechteinhaber seine Zustimmung erteilt hat oder eine gesetzlich vorgesehene Schrankenregelung eingreift. Die Einholung der notwendigen Lizenzen ist allerdings häufig sehr aufwendig und nicht zuletzt kostspielig; um den mit der Nutzung der Daten verbundenen Aufwand zu verringern, führte der Gesetzgeber nun Schrankenregelungen für die Wissenschaft ein.&lt;/p&gt;
&lt;p&gt;Auf EU-Ebene eröffnete die Urheberrechtsrichtlinie (RL 2001/29/EG) den Mitgliedstaaten die Möglichkeit, Ausnahmeregelungen zu schaffen, um die Vervielfältigung von Werken und ihre öffentliche Zugänglichmachung für nicht-kommerzielle Zwecke zu ermöglichen. Einzige Voraussetzung dafür war die Angabe der jeweiligen Quelle. Eine ähnliche Ausnahmeregelung für Forschungszwecke ist in Art. 9 b der Richtlinie 96/9/EG über den rechtlichen Schutz von Datenbanken vorgesehen; allerdings erlaubt diese lediglich die Entnahme (und nicht die Weiterverwendung) von Daten aus einer der Öffentlichkeit - in welcher Weise auch immer - zur Verfügung gestellten Datenbank.&lt;/p&gt;
&lt;p&gt;Damit diese Regelungen in den Mitgliedstaaten verbindliche Geltung erlangen können, müssen die Richtlinien von den nationalen Gesetzgebern in nationales Recht umgesetzt werden. Dabei haben sie allerdings einen weiten Spielraum: Die Richtlinie ist lediglich hinsichtlich ihres Ziels verbindlich. Die Mitgliedstaaten können also selbst entscheiden, ob und inwieweit sie die genannten Ausnahmeregelungen in ihren nationalen Rechtsordnungen aufnehmen (denn diese können, müssen aber nicht eingeführt werden). Um die Interessen von Wissenschaftlern auf der einen Seite und Rechteinhabern (d.h. insbesondere den Verlagen) auf der anderen Seite auszugleichen, entscheiden sich die nationalen Gesetzgeber häufig für die Einführung enger Schrankenregelungen. So ist beispielsweise in Deutschland gem. § 52a UrhG lediglich die Nutzung von veröffentlichten “kleinen Teilen” eines Werkes (also - richterrechtlich festgelegt - bis zu 25 % eines Werkes bis max. 100 Seiten) bzw. Werken “geringen Umfangs” (also Werke mit weniger als 25 Seiten, einzelne Bilder und Musikstücke) für nicht-kommerzielle Zwecke zur Forschung erlaubt. Damit verbunden ist allerdings zwingend ein Vergütungsanspruch des jeweiligen Rechteinhabers, der nur durch eine Verwertungsgesellschaft geltend gemacht werden kann; die dazu notwendigen Verhandlungen zwischen den Universitäten und der VG Wort dauerten viele Jahre und mussten schließlich durch einen Richter geklärt werden. Erst 2006 konnte ein Rahmenvertrag unterzeichnet werden, der den Preis vergleichsweise tief festsetzte: 0,008 EUR pro Seite pro Nutzer. In der Praxis ergab sich aber bald das Problem, dass die Ausnahmeregelung (und der damit verbundene Vergütungsanspruch) durch eine vertragliche Regelung umgangen wurden - Denn wurde der Inhalt aufgrund eines Vertrags (z.B. einer Lizenz) zugänglich gemacht, so konnte eine Regelung des Vertrags dem Nutzer einfach verbieten, das Werk in der gem. § 52a UrhG gestatteten Weise zu nutzen. Dies hatte zur Folge, dass den Wissenschaftlern alle Vorteile der Schrankenregelung wieder verloren gingen.&lt;/p&gt;
&lt;p&gt;Im Jahr 2017 entschied sich der deutsche Gesetzgeber zu handeln: Das Bundesministerium der Justiz und für Verbraucherschutz erarbeitete einen Entwurf für das sog. Urheberrechts-Wissensgesellschafts-Gesetz, das letztlich - nach einem bemerkenswert kurzen Gesetzgebungsprozess - vom Bundestag verabschiedet wurde. Ab März 2018 wird der alte § 52a UrhG (und weitere Normen, die urheberrechtliche Nutzung von Werken in der Forschung, Archiven und Bibliotheken zum Gegenstand hatten) durch die neuen §§ 60a-60h UrhG ergänzt. Dies gilt zunächst für einen Zeitraum von 5 Jahren. Danach muss der Gesetzgeber entscheiden, ob er die Gültigkeit explizit verlängert oder durch andere Regelungen ersetzt (was nicht völlig unwahrscheinlich ist, wenn eine neue EU-Richtlinie für den digitalen Binnenmarkt erlassen wird).&lt;br /&gt; Von besonderem Interesse für die Digital Humanities sind dabei § 60c und § 60d UrhG.&lt;/p&gt;
&lt;p&gt;§ 60c UrhG erlaubt nun ausdrücklich die Vervielfältigung, die Verbreitung und die öffentliche Zugänglichmachung von bis zu 15 % eines Werkes zum Zwecke der nicht-kommerziellen wissenschaftlichen Forschung (also nicht wie die bisherige Rechtsprechung 25 %). Die Regelung beinhaltet also keine Seitenanzahl mehr, wodurch sie dem digitalen Zeitalter angepasst wird. Für die eigene wissenschaftliche Forschung (also ohne Veröffentlichung) dürfen sogar bis zu 75 Prozent eines Werkes vervielfältigt werden.&lt;br /&gt; Unabhängig von diesen Regelungen dürfen Abbildungen, einzelne Beiträge aus Zeitungen oder Zeitschriften, sonstige Werke geringen Umfangs und vergriffene Werke vollständig genutzt werden.&lt;/p&gt;
&lt;p&gt;§ 60d UrhG erlaubt das Data Mining für nicht-kommerzielle Forschungszwecke (die Beschränkung “nicht kommerziell” stammt dabei aus der zugrundeliegenden Richtlinie und darf daher vom nationalen Gesetzgeber nicht übergangen werden - sonst liegt ein Verstoß gegen EU-Recht vor!): Ursprungsmaterial darf also auch automatisiert und systematisch vervielfältigt werden, um daraus insbesondere durch Normalisierung, Strukturierung und Kategorisierung ein auszuwertendes Korpus zu erstellen und dieses einem bestimmt abgegrenzten Kreis von Personen (vermutlich den Mitgliedern des eigenen Forschungsteam) für die gemeinsame wissenschaftliche Forschung zur Verfügung zu stellen. Nach Abschluss des Forschungsprojekts ist das gesamte Korpus zu löschen oder einem Archiv oder eine Bibliothek zur dauerhaften Aufbewahrung zu übermitteln. Diese Ausnahmeregelung betrifft nicht nur urheberrechtlich geschützte Werke, sondern erfasst auch Werke, die durch das sui-generis-Recht für Datenbanken geschützt sind: Obwohl die Richtlinie 96/9/EG keine Schrankenregelung für die Weiterverwendung zu Forschungszwecken vorsieht, fand der nationale Gesetzgeber einen geschickten Weg, diese Einschränkung zu umgehen.&lt;/p&gt;
&lt;p&gt;Anzumerken ist, dass die neuen Schrankenregelungen nicht mehr durch vertragliche Regelungen umgangen werden können, d.h. auf Vereinbarungen, die erlaubte Nutzungen nach den §§ 60a bis 60f UrhG zum Nachteil der Nutzungsberechtigten beschränken oder untersagen, kann sich der Rechteinhaber nicht berufen (vgl. § 60g UrhG). Allerdings sind die Nutzungen zu vergüten; dieser Anspruch kann erneut nur durch die Verwertungsgesellschaften geltend gemacht werden. Auch die angemessene Höhe dieser Vergütung wird wahrscheinlich Gegenstand langer Verhandlungen werden, die Preisfestsetzung wird jedenfalls eine abschreckende Wirkung haben. Beispielsweise setzte eine Vereinbarung zwischen den Bibliotheken und den Verwertungsgesellschaften im Jahr 2006 die Vergütung für die Digitalisierung und öffentliche Zugänglichmachung von Büchern fest auf 120 % des Nettopreises des Buches.&lt;/p&gt;
&lt;p&gt;Die neuen Schrankenregelungen sind jedenfalls ein entscheidender Schritt in die richtige Richtung. Es ist wichtig, die DH-Gemeinschaft über diese aktuellen Entwicklungen zu informieren. Um allerdings das volle Potenzial des Data Mining in der EU zur Entfaltung zu bringen, ist der EU-Gesetzgeber gefragt. Tatsächlich gab es Ende 2016 einen Vorschlag für eine neue Richtlinie für den digitalen Binnenmarkt, die eine verbindliche Schrankenregelung für das Data Mining öffentlicher Forschungseinrichtungen (wie z.B: Universitäten) vorsieht, auch zu kommerziellen Zwecken. Derselbe Richtlinienvorschlag enthält auch einige vernünftige Einschränkungen für den Zugang zu Material - insbesondere verpflichtet er die Lizenznehmer zur regelmäßigen Information des Lizenzgebers über die Nutzung ihrer Werke. Allerdings muss auch er zunächst vom EU-Gesetzgeber vollständig ausgearbeitet und verabschiedet und anschließend von den Mitgliedsstaaten umgesetzt werden. Zum jetzigen Zeitpunkt ist eine Vorhersage darüber, wie die finale Version der Richtlinie aussehen wird, absolut nicht möglich. Allerdings sollte die DH-Community unbedingt zeitnah diesbezüglich mit Informationen versorgt werden.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>237</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Mayr, Eva
Schreder, Günther
Windhager, Florian</p3_authors>
  <p3_organisations>Donau Universität Krems, Österreich
Donau Universität Krems, Österreich
Donau Universität Krems, Österreich</p3_organisations>
  <p3_emails>eva.mayr@donau-uni.ac.at
guenther.schreder@donau-uni.ac.at
florian.windhager@donau-uni.ac.at</p3_emails>
  <p3_presenting_author>Mayr, Eva</p3_presenting_author>
  <p3_title>Digital HUMANities - Eine benutzerzentrierte Perspektive</p3_title>
  <p3_abstract>&lt;p&gt;Wenn von Digital Humanities (DH) die Rede ist, liegt der Fokus oft auf dem Digitalen, auf den neuen Möglichkeiten, welche die technischen Entwicklungen der letzten Jahre eröffnen. Jedoch sollte bei Digital Humanities nicht primär &lt;em&gt;digital &lt;/em&gt;im Vordergrund stehen, sondern die &lt;em&gt;Human- und Geisteswissenschaften&lt;/em&gt;, die sich digitaler Methoden zur Unterstützung ihrer wissenschaftlichen Forschung bedienen (vgl. Siemens, 2016). Im Zentrum dieser Forschung stehen trotz digitaler Optionen dennoch die etablierten Fragestellungen der Humanwissenschaften. Technische Entwicklungen können neue Wege der Erkenntnis eröffnen oder bestehende Methoden vereinfachen und erleichtern, doch ohne fundierten ExpertInnen der Geisteswissenschaften in diesem Prozess eine zentrale Rolle zuzugestehen, können digitale Forschungs-Systeme die Bedürfnisse und Zielsetzungen ihrer BenutzerInnen nicht (gut genug) unterstützen und werden auch nicht nachhaltig von diesen aufgenommen.&lt;/p&gt;
&lt;p&gt;Welche Möglichkeiten gibt es, den Einfluss von prä-, non-, oder postdigitalen Konzeptionen der Geisteswissenschaft in den DH zu stärken? Wie können Projekte in den DH weniger technologie- und stärker menschen- oder inhaltsgetrieben geplant und durchgeführt werden? Eine Antwort darauf können benutzerzentrierte Gestaltungsprozesse (“&lt;em&gt;user centered design&lt;/em&gt;”) geben, in denen den BenutzerInnen der Systeme eine zentrale Rolle zukommt - von der Planung bis zur Evaluation der technologischen Entwicklungen.&lt;/p&gt;
&lt;p&gt;Im Folgenden werden die Grundprinzipien und Methoden des benutzerzentrierten Designs erörtert und ein aktuelles Projekt - als Anwendungsfall eines benutzerzentrierten Designprozesses - vorgestellt.&lt;/p&gt;
&lt;p&gt;1. Benutzerzentrierte Gestaltungsprozesse&lt;/p&gt;
&lt;p&gt;Im Gegensatz zu technikzentrierter Entwicklung, steht in benutzerzentrierten Gestaltungsprozessen die BenutzerIn, ihre Bedürfnisse und Aufgaben, ihr Fühlen und Denken im Vordergrund. Ausgangspunkt sind daher auch nicht die technischen Möglichkeiten, sondern eine Analyse der Zielgruppe: Welche Eigenschaften und welche Arbeitsweisen zeichnet sie aus? Für welche Probleme bedarf es einer technischen Lösung? Erst danach werden geeignete Technologien entwickelt und laufend unter wiederholter Einbeziehung der Zielgruppe getestet.&lt;/p&gt;
&lt;p&gt;Maguire (2011) definierte vier Schlüsselprinzipien für benutzerzentriertes Design: (1) das aktive Miteinbeziehen der BenutzerInnen, sowie ein klares Verständnis der Zielgruppe und ihrer Bedürfnisse, (2) eine geeignete Aufteilung der Funktionen und Prozesse zwischen Benutzer und System, (3) iterative Entwicklung und Testung der Technologie, und (4) Zusammenarbeit in einem inter- bzw. transdisziplinären Team.&lt;/p&gt;
&lt;p&gt;Was bedeutet das umgelegt auf DH? Es gibt Stimmen, die generell daran zweifeln, dass Benutzer ihre Bedürfnisse verbalisieren können bzw. dieses Wissen von Nutzen für technologische Entwicklungen ist. Kemman und Klappe (2014) befragten daher Geisteswissenschaftler nach ihren Anforderungen für eine DH-Anwendung. Sie stellten fest, dass diese ihre Bedürfnisse gut verbalisieren konnten, sie fanden aber auch etliche (aus ihrer Sicht) irrelevante Bedürfnisse und vermissten eine Vorstellungskraft dafür, welche Möglichkeiten über den Standard-Forschungsprozess hinaus mithilfe neuer Technologien erschlossen werden könnten. Unserer Meinung nach bedarf es daher anderer Methoden als einer reinen Befragung nach den Bedürfnissen der Benutzer: Möglichkeiten für eine solche erweiterte Bedarfsanalyse sind Beobachtungen der Forschungsprozesse, aber auch Literaturstudien. Um die technischen Möglichkeiten und die geisteswissenschaftlichen Bedürfnisse in die Definition der Anforderungen mit einzubeziehen, bedarf es innovativer Ansätze, wie etwa Design-Sprints (Venturini, Munk &amp; Meunier, 2017), in denen alle Beteiligten kollaborativ eine gemeinsame Perspektive entwickeln (Vertreter der Zielgruppe, GeisteswissenschaftlerInnen, DH-ExpertInnen und ComputerwissenschaftlerInnen). Da in der Zusammenarbeit zwischen ComputerwissenschaftlerInnen und GeisteswissenschaftlerInnen zwei sehr heterogene Welten aufeinander prallen (unterschiedliche Wissenschaftskultur, Terminologien, Epistemiken, sowie unterschiedliche Wege des Erkenntnisgewinns), ist es offensichtlich entscheidend, in einen aktiven und strukturierten Dialog miteinander zu treten, Herausforderungen arbeitsteilig zu lösen, den Wissensaustausch zu fördern, aber auch idealerweise diesen Prozess durch Personen zu mediieren, die in beiden Kulturen sozialisiert und fachsprachlich versiert sind.&lt;/p&gt;
&lt;p&gt;Benutzerzentrierte Gestaltung definiert dabei nur ein Vorgehensmodell, in dem die Bedürfnisse der BenutzerInnen am Anfang stehen und in dem diese wiederholt einbezogen werden, es bestimmt jedoch nicht die zur Anwendung kommenden Forschungsmethoden, sondern bedient sich der jeweils passenden Methoden, zum Beispiel aus der Usability-Forschung: Benutzertests mit Prototypen, Beobachtungen, Befragungen, Fokusgruppen, cognitive walkthroughs, lautes Denken oder heuristische Evaluationen (siehe z.B. Barnum, 2008; Sarodnik &amp; Brau, 2006). Im Folgenden soll die Spezifikation dieses Prozesses anhand einer Fallstudie zur Visualisierung kultureller Sammlungen exemplifiziert werden.&lt;/p&gt;
&lt;p&gt;2. Fallstudie: Visualisierung kultureller Sammlungen&lt;/p&gt;
&lt;p&gt;In den letzten 10 Jahren wurden kulturelle Sammlungen im großen Stil digitalisiert und aggregiert (z.B. Europeana, DPLA) mit dem Ziel die Zugänglichkeit zum kulturellen Erbe zu verbessern. Diese digitalen Sammlungen stellen jedoch für Benutzer ohne fachliche Expertise große Hürden dar (Walsh &amp; Hall, 2015): Die Oberflächen sind zumeist von einer Suchfunktion dominiert, die eine Kenntnis der Datenbankstruktur voraussetzt, und das Suchergebnis wird als unstrukturierte Liste präsentiert, was das Gewinnen eines Überblicks über die Sammlung erschwert. Neuere Ansätze fordern daher “großzügigere” Benutzeroberflächen (Whitelaw, 2015), die ein “Flanieren” durch die Informationen ermöglichen (Doerk et al., 2011). Das Projekt &lt;em&gt;polycube &lt;/em&gt;(Windhager et al., 2016) entwickelt diesen Anforderungen folgend Informationsvisualisierung von kulturellen Sammlungen zur Verbesserung der Zugänglichkeit und des Verständnisses dieser Sammlungen für die allgemeine Bevölkerung.&lt;/p&gt;
&lt;p&gt;Für eine bessere Einbettung dieses Forschungsvorhabens wurde der aktuelle Stand der Technik erhoben (Windhager et al., 2017) und 48 Publikationen zur Visualisierung kultureller Sammlungen unter anderem mit Hinblick auf die Einbeziehung der Zielgruppe in den Gestaltungsprozess bewertet: In 6 Publikationen wurden rein technische Aspekte besprochen, Zielgruppen wurden dabei nicht genannt. In 17 Publikationen wurden zwar die Zielgruppen erwähnt, aber es fanden sich keine Informationen, ob und wie sie in die Entwicklung einbezogen wurden. In 5 Publikationen wurden (geplante) Usertests erwähnt – allerdings ohne weitere Details. Nur in 20 Publikationen wurde von Studien berichtet, die in ihrem Umfang stark variieren - von Einzelfallanalysen bis hin zu elaborierten Testungen mit großen Benutzergruppen (vgl. Abbildung 1).&lt;/p&gt;

&lt;p&gt;Abbildung 1: Überblick über verschiedene Methoden zur Einbeziehung der BenutzerInnen&lt;/p&gt;
&lt;p&gt;Im Projekt &lt;em&gt;polycube &lt;/em&gt;(https://www.donau-uni.ac.at/de/polycube) stand am Anfang eine Data-Users-Tasks-Analyse (Miksch &amp; Aigner, 2014). Es wurden ExpertInneninterviews mit HistorikerInnen und DH-ForscherInnen durchgeführt zu den &lt;em&gt;Daten&lt;/em&gt;: Welche Zusammenhänge gibt es zwischen den Objekten der Sammlung? Welche Daten sind digital vorhanden, welche Informationen fehlen in der digitalen Datenbank? Eine Literaturstudie widmete sich den &lt;em&gt;BenutzerInnen &lt;/em&gt;und ihren &lt;em&gt;Aufgaben &lt;/em&gt;(Mayr et al., 2016a): Da es sich um alltägliche NutzerInnen handelt, gestaltete sich die Definition von relevanten Aufgaben als besonders herausfordernd. Stattdessen wurden relevante Informationsbedürfnisse und Verhaltensmuster definiert.&lt;/p&gt;
&lt;p&gt;Auf diesen Erkenntnissen aufbauend wurde ein erster Prototyp entwickelt, der derzeit in einer qualitativen Studie getestet wird. Im Mittelpunkt steht dabei die Frage, wie die entwickelten Informationsvisualisierungen die BenutzerInnen beim Aufbau eines mentalen Modells über die kulturelle Sammlung unterstützen (Mayr et al., 2016b). Iterativ werden die gewonnenen Erkenntnisse in die Entwicklung des Prototypen einfließen und in zwei weiteren Experimenten gegen alternative Informationsvisualisierungen getestet werden.&lt;/p&gt;
&lt;p&gt;Das gewählte benutzerzentrierte Vorgehen soll dazu beitragen, dass die entwickelten Informationsvisualisierungen ein besseres Verständnis der kulturellen Sammlungen vermitteln, damit intuitiv interagiert werden kann und diese zu einer weiteren Auseinandersetzung mit den Informationen anregen.&lt;/p&gt;
&lt;p&gt;3. Diskussion&lt;/p&gt;
&lt;p&gt;In der Abgrenzung zu den nicht-digitalen Geisteswissenschaften versuchen die DH “die Prozesse der Gewinnung und Vermittelung neuen Wissens unter den Bedingungen einer digitalen Arbeits- und Medienwelt weiter zu entwickeln” (DHd). Dabei agieren ihre AkteurInnen nicht selten mit einem Fokus auf  Technik- und Infrastrukturentwicklung statt mit einem Fokus auf geisteswissenschaftliche Prozesse und Methoden des Erkenntnisgewinns. Um diesen Prozessen einen stärkeren Stellenwert in DH-Projekten einzuräumen, haben wir in diesem Beitrag benutzerzentrierte Gestaltungsprozesse als eine Herangehensweise diskutiert, in der geisteswissenschaftliche ExpertInnen und andere Zielgruppen in die Entwicklung neuer Technologien intensiv miteinbezogen werden.&lt;/p&gt;
&lt;p&gt;Warum ist ein benutzerzentrierter Gestaltungsprozess gerade in der DH von Vorteil?&lt;/p&gt;
&lt;p&gt;(1) Unterschiedliche Wege des Erkenntnisgewinns und Forschungsmethoden in den Geisteswissenschaften und den Computerwissenschaften erschweren ein Verständnis der Probleme und Anliegen der jeweils anderen Disziplinen. Durch die intensive Zusammenarbeit und Koordination in einem benutzerzentrierten Designprozess können die Beteiligten in einen intensiven Wissensaustausch eintreten und ein besseres Verständnis füreinander aufbauen.&lt;/p&gt;
&lt;p&gt;(2) Die Definition von zu lösenden Problemen aus Sicht der GeisteswissenschaftlerInnen am Beginn eines Projektes erlaubt die Entwicklung von innovativen Technologien im Dienste der Geisteswissenschaft, anstatt geisteswissenschaftliche Daten als Anwendungsfeld für neue technische Entwicklungen zu instrumentalisieren.&lt;/p&gt;
&lt;p&gt;(3) Die Iteration von Entwicklungen und Testungen führen zu einer regelmäßigen Evaluation der Technologien in verschiedenen Entwicklungsstadien und ermöglichen die Korrektur von Fehlentwicklungen bereits früh im Projektverlauf. Im Gegensatz dazu bleiben summative Evaluationen am Ende von Projekten oft ohne Einfluss auf die entwickelte Technologie bzw. erlauben nur mehr geringfügige Adaptierungen. Eine Iteration von Entwicklungs- und Evaluationsphasen erhöht die Anzahl der explorierten Design-Optionen und erlaubt die Auswahl und Weiterentwicklung der am besten geeigneten Varianten.&lt;/p&gt;
&lt;p&gt;Benutzerzentriertes Design ist aber nicht immer das Mittel der Wahl. Um etwa radikal innovative Produkte zu erschaffen, besitzen zukünftige BenutzerInnen oft nicht die Vorstellungskraft, welche Möglichkeiten sich durch die neuen technischen Entwicklungen ergeben (Norman, 2010; vgl. die Beobachtungen von Kemman &amp; Klappe, 2014). Hier empfiehlt es sich, die Benutzer erst in der Optimierung der Produkte mit einzubeziehen. Auch zur Lösung von institutionellen, finanziellen, oder politischen Problemen sollten statt benutzerzentrierter eher transdisziplinäre Methoden gewählt werden.&lt;/p&gt;
&lt;p&gt;Unsere Erfahrung in DH Projekten zeigt, dass der methodengestützte Dialog zwischen Computer- und Geisteswissenschaften essentiell ist und sein Potenzial für gute und nachhaltige technische Entwicklungen in benutzerzentrierten Gestaltungsprozessen besonders gut entfalten kann. Die präsentierte Analyse von Publikationen zur Visualisierung kultureller Sammlungen (Windhager et al., 2017) zeigt, dass eine Einbeziehung der BenutzerInnen in den DH keine Selbstverständlichkeit ist und dass eine benutzerzentrierte Entwicklung derzeit nicht zum Stand der Technik gehört. Das Potenzial dieses Vorgehensmodells ist jedoch sehr groß, wenn es darum geht die Bedürfnisse der NutzerInnen zu erfüllen und die Technologie soweit daran anzupassen, dass deren Akzeptanz und nachhaltige Nutzung sichergestellt werden kann.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>72</session_ID>
  <session_title>Kaffeepause</session_title>
  <session_start>2018-03-01 10:30</session_start>
  <session_end>2018-03-01 11:00</session_end>
  <attendee_count>4</attendee_count>
 </session>

 <session>
  <session_ID>37</session_ID>
  <session_short>VP_5a</session_short>
  <session_title>Digitale Edition I</session_title>
  <session_start>2018-03-01 11:00</session_start>
  <session_end>2018-03-01 12:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Kuczera, Andreas</chair1>
  <attendee_count>3</attendee_count>
  <chair1_name>Andreas Kuczera</chair1_name>
  <chair1_organisation>Regesta Imperii, Universität Gießen, Akademie der Wissenschaften Mainz</chair1_organisation>
  <chair1_email>andreas.kuczera@geschichte.uni-giessen.de</chair1_email>
  <chair1_ID>1058</chair1_ID>
  <sessionID>37</sessionID>
  <presentations>3</presentations>
  <p1_paperID>107</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Gronemeyer, Sven
Diehr, Franziska
Prager, Christian
Diederichs, Katja
Wagner, Elisabeth
Brodhun, Maximilian
Grube, Nikolai</p1_authors>
  <p1_organisations>Rheinische Friedrich-Wilhelms-Universtität Bonn, Deutschland; La Trobe University Melbourne, Australien
Niedersächsische Staats- und Universitätsbibliothek Göttingen, Deutschland
Rheinische Friedrich-Wilhelms-Universtität Bonn, Deutschland
Rheinische Friedrich-Wilhelms-Universtität Bonn, Deutschland
Rheinische Friedrich-Wilhelms-Universtität Bonn, Deutschland
Niedersächsische Staats- und Universitätsbibliothek Göttingen, Deutschland
Rheinische Friedrich-Wilhelms-Universtität Bonn, Deutschland</p1_organisations>
  <p1_emails>sgronemeyer@uni-bonn.de
diehr@sub.uni-goettingen.de
cprager@uni-bonn.de
katja.diederichs@uni-bonn.de
ewagner@uni-bonn.de
brodhun@sub.uni-goettingen.de
ngrube@uni-bonn.de</p1_emails>
  <p1_presenting_author>Gronemeyer, Sven
Diehr, Franziska</p1_presenting_author>
  <p1_title>Vagheit hoch Zweifel plus Kritik! Die Bewertung von Widersprüchen in einer digitalen Entzifferungsarbeit der Maya-Hieroglyphen</p1_title>
  <p1_abstract>&lt;p dir="ltr"&gt;Bei der Untersuchung einer noch nicht vollständig entschlüsselten Schrift und Sprache entstehen während des Forschungsdiskurses umstrittene wie plausible Aussagen über die sprachliche Entzifferung von Schriftzeichen, wobei jede Entzifferungshypothese für sich den Anspruch erhebt, im untersuchten Kontext aussagekräftig zu sein. Seit über 150 Jahren wird an der Entzifferung der Maya-Schrift gearbeitet, die “Geburts- und Sterberate” von Entzifferungsvorschlägen ist entsprechend hoch und ihre Dokumentation und kritische Bewertung für ein digitales Wörterbuch steht derzeit im Fokus unseres Forschungsprojekts “Textdatenbank und Wörterbuch des Klassischen Maya” (TWKM).&lt;/p&gt;
&lt;p dir="ltr"&gt;Wir stehen vor der Herausforderung, bisherige und eigene Entzifferungshypothesen nicht nur digital zu dokumentieren, sondern zu bewerten und qualitativ so einzustufen, damit sie einer kritischen Prüfung standhalten und dadurch erst für die linguistische Analyse der Texte anwendbar werden. Unterschiedliche Kontexte erzeugen beim Lesen und Verstehen antiker Texte unterschiedliche Plausibilitäten und diese Perspektivendifferenz nicht nur auszuhalten, sondern auch in ein Modell umzusetzen, ist ein moderner Ansatz der digitalen Epigraphik, den wir am Beispiel unseres digitalen Inventars der Zeichen und Graphe der Maya-Schrift vorstellen möchten.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Die Hieroglyphenschrift des Klassischen Maya&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Die logosyllabische Hieroglyphenschrift der Maya umfasst rund 1000 Zeichen und wurde etwa zwischen 350 v. Chr. und 1550 n. Chr. im südlichen Mesoamerika zur Aufzeichnung der Hochsprache des Klassischen Maya verwendet. Die Anzahl der Graphe ist raum-zeitlich betrachtet mit rund 3000+ Formen weitaus höher, da Zeichen gleichzeitig mehrere Graphvarianten aufweisen können, die von einer Vollform abgeleitet sind.&lt;/p&gt;
&lt;p dir="ltr"&gt;Einzelne Graphe werden in einem meist mit einem Wort oder morphemischem Verbund identischem Hieroglyphenblock arrangiert, ähnlich dem koreanischen Hangul. Allerdings offenbart das Maya aufgrund seines Variantenreichtums eine wesentlich größere kalligraphische Freiheit als die einzelnen Varianten nur durch simples Aneinanderreihen im Block zu schreiben. Je nach Platzbedarf und Ästhetik können Graphe etwa miteinander verschmelzen, infigiert oder gedreht werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Bei der Katalogisierung der Graphe muss weiterhin mehr als ein Jahrtausend paläographischer Entwicklung berücksichtigt werden, ebenso unterschiedliche Stile in skulptierten oder gemalten Texten. Wegen all dieser Eigenheiten und der herausfordernden Struktur widersetzt sich die Maya-Schrift aktuell, Teil des Unicode-Standards zu werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Zeichenkataloge als Hilfskonstrukte der Epigraphik&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Bis in die 1950er Jahre war die Maya-Schrift nicht entziffert und blieb es in großen Teilen bis in die 1980er Jahre, als eine Reihe bahnbrechender Erkenntnisse einen Kaskadeneffekt in Gang setzte. Bis heute kennt man ebensowenig die genaue Zahl der Zeichen und ihrer graphischen Repräsentationen, da alle bisher publizierten Verzeichnisse aufgrund des Entzifferungsprozesses unvollständig und unzulänglich sind. Über 300 Zeichen sind bis heute nur vage oder gar nicht entziffert. Für viele dieser Fälle  existieren  konkurrierende Entzifferungsvorschläge, die vielleicht nur in ausgewählten Kontexten valide sind, sich aber wegen möglicher Polyvalenz nicht gegenseitig ausschließen müssen. Es gilt nicht nur, das Zeicheninventar vollständig zu erfassen, sondern existierende Entzifferungsvorschläge kritisch im Textzusammenhang zu prüfen, ob sie verifizierbar sind, und wo sie sich als falsch oder nicht überprüfbar herausstellen und somit nicht weiter berücksichtigt werden müssen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die elf bisher publizierten Zeicheninventare weisen viele Schwachstellen auf, besonders problematisch sind dabei Mehrfachinventarisierungen von Allographen als verschiedene Zeichen. Ein weiterer offensichtlicher Nachteil der traditionellen Zeichenkataloge ist die unveränderbare Natur einer gedruckten Fassung. Dies verhindert, dass gegebene Mehrfach- und Fehlklassifikationen korrigiert oder neue Beziehungen zu Zeichen und zu verschiedenen Zeichenfunktionen erstellt werden können, zudem neue Entzifferungen nicht berücksichtigt werden können.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Ein digitales Zeichen- und Graphinventar für das Klassische Maya&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Als Resultat interdisziplinärer Arbeit, bei dem die Modellierung und Verarbeitung der Daten auf Grundlage epigraphischer Prinzipien und Forschungsfragen erfolgte, ist unser digitaler Zeichenkatalog so konzipiert, dass er sowohl bisherige Forschungsergebnisse kritisch abbilden als auch noch zu erwartende Erkenntnisse flexibel einbinden kann.&lt;/p&gt;
&lt;p dir="ltr"&gt;Der Katalog basiert auf einem innovativen Konzept der flexiblen Zuordnung von Zeichen zu ihren Graphen: Zeichen als Träger sprachlicher Informationen und Graphe als Form ihrer schriftlichen Realisierung werden getrennt erfasst, und erst die Verbindung eines Zeichens mit seinen Graphen macht es zu dessen Allograph, deren Gesamtheit bildet das Graphem des Zeichens. Der Zeichenkatalog ist auf Basis von Standard-Ontologien modelliert und in RDF realisiert. Die ontologisch-vernetzte Struktur des Modells erlaubt es, semantische Relationen über persistente URIs zwischen eindeutig referenzierbaren Entitäten herzustellen. Dadurch ist es möglich, Zuordnungen flexibel anzupassen und Allographe durch neue Verknüpfungen hinzuzufügen oder Falschzuweisungen zu korrigieren, etwa wenn ein Graph tatsächlich aus zwei Graphen verschiedener Zeichen besteht.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Bewertung und qualitative Einstufung von Lesungshypothesen&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Bei einer noch nicht vollständig entschlüsselten Schrift machen Epigraphiker zwangsläufig verschiedene Annahmen zur phonemischen Lesung von Zeichen. Es ist notwendig, alle plausiblen und nicht eindeutig widerlegten Entzifferungsvorschläge zu dokumentieren, vor allem aber, deren Qualität nach formalen Kriterien bewertbar zu machen. Dazu haben wir ein neutrales, transparentes System modelliert, das anhand formaler Kriterien eine qualitative Einstufung von Lesungshypothesen ermöglicht und deren Plausibilität im Textkorpus überprüfbar macht.&lt;/p&gt;
&lt;p dir="ltr"&gt;Dem Zeichen werden syllabische und logographische Zeichenfunktionen zugewiesen, die jeweils einen graphemischen Transliterationswert haben, z.B. “la” für ein Silbenzeichen. Aufgrund der Polyvalenz kann ein Zeichen etwa zwei logographische Lesungen mit distinktem Transliterationswert haben, z.B. “AJAW” und “SAK” für die Katalognummer 533. Für die Konfidenz eines Transliterationswertes werden jene Kriterien ausgewählt, auf die er sich stützt. Für jede Art der Zeichenfunktion wurde ein eigenes Kriterien-Set entwickelt, das sich vor allem am graphematischen und sprachlichen Nutzungskontext orientiert, z.B. hat die Übereinstimmung mit einer bestimmten Wortart durch das Syntagma für ein Silbenzeichen keine Bedeutung, jedoch für Logogramme - hier besonders auch der Nachweis in modernen Maya-Sprachen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Kriterien sind mittels Aussagelogiken so miteinander in Bezug gesetzt, dass je nach Kombination eine qualitative Einstufung vorgenommen wird. Dabei steht “1” für die höchstmögliche Konfidenz und eine evidente Lesung. Die Anzahl der Konfidenzstufen je Zeichenfunktion ist unterschiedlich: während Logogramme eine granulare Einteilung benötigen, brauchen Silbenzeichen weniger Stufen, da deren Permutationen im Kontext eines Wortes recht eindeutig sind.&lt;/p&gt;
&lt;p dir="ltr"&gt;Das Wortzeichen “CH’AM” etwa taucht mit phonemischen Komplementen auf, die entsprechenden Kriterien ergeben Stufe 2. Damit liegt eine wahrscheinliche, aber ohne funktional äquivalente syllabische Substitution noch keine gesicherte Entzifferung vor. Die Kriterien sind bewusst streng gehalten, um für jede Lesungsaussage eine kritische Evaluation gegenüber den Zeichenvorkommen im von uns TEI-kodierten Textkorpus durchführen zu können und damit unserem Wörterbuch eine hohe Zuverlässigkeit für den Nutzer zu geben. Lesungen unterhalb einer bestimmten Konfidenz werden nämlich nicht aufgenommen, so dass in unserem Wörterbuch bestimmten Entzifferungsvorschlägen der normative Charakter genommen wird, den sie vielleicht in der epigraphischen Forschung durch Zitierung gewonnen haben.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Zusammenspiel von Zeichenkatalog, Textkorpus und linguistischer Analyse&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Ohne eine sichere Identifizierung aller Graphe und mit vielen Zeichen unbekannter oder umstrittener Lesung kann das TEI-kodierte Textkorpus weder aus einem festen Schriftzeichensatz  wie Unicode noch aus phonemisch transliterierten Werten bestehen. Für ein flexibles Korpus, das auf neue Entzifferungen und verschiedene Lesungshypothesen reagieren kann, nutzen wir den Zeichenkatalog als eine Art “Grundbaukasten” bei der Korpuserstellung.&lt;/p&gt;
&lt;p dir="ltr"&gt;Im kodierten Text wird jede Hieroglyphe mittels Katalognummer und einer Referenz zur URI im Zeichenkatalog erfasst. Mittels einer Software zur linguistischen Analyse, Teil unserer virtuellen Arbeitsumgebung, wird in einem weiteren Prozessierungsschritt die numerische Transliteration (Katalognummern) zunächst in eine graphemische Transliteration (Transliterationswerte) überführt. Dies geschieht wegen der Polyvalenz semi-automatisch, wenn der Epigraphiker nach Verwendungskontext eine Entscheidung fällen muss - erst damit wird der Text “menschenlesbar”.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die qualitative Einstufung der Entzifferungsvorschläge im Zeichenkatalog wird jetzt relevant: Die Analyse kann anhand hoher oder niedriger Konfidenzstufen durchgeführt werden. Die Entzifferungsaussagen können in ihrem Verwendungskontext überprüft werden, was idealerweise zu neuen Erkenntnissen für die Entzifferung führen kann, aber auch der Vorbereitung der zweiten Stufe der phonemischen Transliteration dient. Jetzt werden die quasi als Container genutzten Transliterationswerte aus dem Zeichenkatalog kontextuell der korrekten sprachlichen Lesung angepasst. So besitzt das Zeichen 561 “CHAN” üblicherweise die Lesung “chan” - “Himmel”, syllabische Substitutionen in Nordwest-Yukatan zeigen aber, dass das Zeichen dort in einem vernakularen Kontext “káan” ausgesprochen wurde.&lt;/p&gt;
&lt;p dir="ltr"&gt;Das Tool zur linguistischen Analyse ermöglicht darüber hinaus die Anlage paralleler, als gleichwertig anzusehende Textanalysen, damit wird den verschiedenen Entzifferungsvorschlägen Rechnung getragen. Durch die Verbindung von Zeichenkatalog, Textkorpus und linguistischer Analyse entsteht letztendlich ein dynamischer Text, der je nach Forschungsfrage individuell generiert werden kann. Dieser Ansatz der ontologischen Vernetzung der Komponenten dürfte auch für die Erforschung weiterer nicht entzifferter Schriften von Interesse sein.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Neue Perspektiven für die Maya-Epigraphik&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Auch ein digitaler Zeichenkatalog des Klassischen Maya kann nur so gut sein wie die epigraphische Forschung, und ist vor allem vom Grad der kritischen Selbstreflektion abhängig. Konkurrierende Entzifferungsvorschläge werden erst einmal als gleichwertig aufgenommen und erst dann anhand formaler Kriterien kategorisiert. Die Kriterienvergabe folgt den Argumenten der Hypothese und ist damit faktisch. Die Aussagelogiken zur Festlegung der Konfidenzstufen sind dabei eine kritische Zusammenführung fast 70-jähriger epigraphischer Forschungspraxis, auch im Vergleich mit den Methoden bei der Entzifferung anderer nicht-alphabetischer Schriftsysteme. Die Konfidenz eines Entzifferungsvorschlags ist damit weit mehr als ein Bayesscher Wahrscheinlichkeitsbegriff. Das Datenmodell ist dabei dem noch nicht gefestigten Erkenntnisstand zum Mayaschriftsystem angepasst.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Arbeit mit digitalen Methoden hat so manche Kritik an der eigenen Forschungstradition erst in Gang kommen lassen, lenkt diese aber auch in eine vorher nicht denkbare Richtung. Die Möglichkeit eines digitalen Zeichenkatalogs und eines digitalen Textkorpus erlaubt erstmals, den gesamten Schriftschatz des Klassischen Maya nach Entzifferungskontexten zu durchsuchen, anstatt sich auf sein “cerebrales” Textkorpus verlassen zu müssen. Erst die Verbindung von geisteswissenschaftlichen und digitalen Methoden erlaubt es, die Maya-Epigraphik in eine neue Phase eintreten zu lassen.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>217</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Börner, Ingo
Fischer, Frank
Hechtl, Angelika
Jäschke, Robert
Trilcke, Peer</p2_authors>
  <p2_organisations>Universität Wien, Österreich
National Research University Higher School of Economics, Moskau, Russland
Wirtschaftsuniversität Wien, Österreich
Humboldt-Universität, Berlin
Universität Potsdam, Deutschland</p2_organisations>
  <p2_emails>ingo.boerner@univie.ac.at
ffischer@hse.ru
angelika.hechtl@wu.ac.at
jaeschke@l3s.de
trilcke@uni-potsdam.de</p2_emails>
  <p2_presenting_author>Börner, Ingo
Hechtl, Angelika</p2_presenting_author>
  <p2_title>Cäsar Flaischlens „Graphische Litteratur-Tafel“ – digitale Erschließung einer großformatigen Karte zur Deutschen Literatur</p2_title>
  <p2_abstract>&lt;p&gt;Das vorgestellte Projekt „Cäsar Flaischlens Graphische Litteratur-Tafel digital“ unternimmt den Versuch eines ‚reverse engineering‘ einer historischen Literaturkarte (1890) und erschließt dieses Dokument früher Visualisierung literaturgeschichtlicher Daten mit Methoden der Digital Humanities (Annotation und Bildanalyse).&lt;/p&gt;
</p2_abstract>
  <p3_paperID>281</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Wagner, Andreas
Glück, David</p3_authors>
  <p3_organisations>Max-Planck-Institut für europäische Rechtsgeschichte, Frankfurt am Main
Akademie der Wissenschaften und der Literatur, Mainz; Johann Wolfgang Goethe-Universität Frankfurt am Main</p3_organisations>
  <p3_emails>wagner@rg.mpg.de
glueck@rg.mpg.de</p3_emails>
  <p3_presenting_author>Wagner, Andreas
Glück, David</p3_presenting_author>
  <p3_title>An den Grenzen der Interoperabilität: Eine kritische Reflexion über digitale Forschungsdaten und -anwendungen in der Online-Edition des Projekts "Die Schule von Salamanca"</p3_title>
  <p3_abstract>&lt;p&gt;&lt;strong&gt;Einleitung&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Der Beitrag diskutiert kritisch, welche nicht allein arbeitsökonomischen, sondern vor allem intellektuellen, methodologischen und wissenschaftstheoretischen "Kosten" mit der Digitalisierung und dem dadurch etablierten Fokus auf Fragen der Interoperabilität entstehen. Die deutlichen wissenschaftlichen Vorteile von Interoperabilität müssen nämlich einem Umbau (und z.T. Rückbau) in wissenschaftlichen Modellierungen und im Verständnis wissenschaftlicher Erkenntnisvermehrung gegenüber gestellt werden. Dabei wird im Sinne des doppelten Genitivs "Kritik der digitalen Vernunft" sowohl das naheliegende Phänomen diskutiert, dass die digitale Transformation Hoffnungen weckt und Lösungen wissenschaftlich-methodologischer Schwierigkeiten nahelegt, die sich (z.T. erst bei der Implementierung) als wissenschaftlich nicht akzeptabel erweisen, als auch der umgekehrte Fall, dass die digitale Transformation ein kritisches Umdenken in der Ausrichtung der eigenen wissenschaftlichen Arbeit und eine Umorientierung wissenschaftlicher Ambitionen erzwingt.&lt;/p&gt;
&lt;p&gt;Diese Diskussion wird im Durchgang durch einige beispielhafte Entwicklungen im seit vier Jahren laufenden Projekt "Die Schule von Salamanca" geführt, bevor ein Versuch der Verallgemeinerung unternommen wird. Dass Interoperabilität signifikante wissenschaftliche Kosten mit sich bringt, heißt im Übrigen nicht, dass diese nicht womöglich durch die Vorteile aufgewogen würden. So werden wir die These vertreten, dass es erstens darum gehen muss, die "Einbußen" zwar deutlich als solche zu verstehen, dass es sich aber zweitens doch lohnt, diese in Kauf zu nehmen und den wissenschaftlichen Fortschritt durch eine Bereicherung des Diskurses an anderen Punkten zu befördern.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interoperabilität&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Mit der digitalen Transformation sind die Möglichkeiten des überregionalen und interdisziplinären Austauschs und der Weiterverwendung von Forschungsdaten in einer ganz neuen Weise möglich geworden. So hat sich der Begriff der Interoperabilität als zentrales Paradigma in den Digital Humanities etabliert, um den Anspruch zu beschreiben, diese Möglichkeiten methodisch auszubauen und ein Evaluationskriterium für Forschungsleistungen und -ergebnisse anzubieten. Interoperabilität ist auf mehreren Ebenen zu verstehen (vgl. Gradmann 2009) und wird auf den konkreten technischen und syntaktischen Ebenen vor allem durch Standards für Datenformate, Schnittstellen, sowie den Bezug auf Normdaten realisiert. Dabei liegen die Vorteile von Interoperabilität zunächst auf der Hand: Technische Interoperabilität ermöglicht die "Nutzung von Daten auf unterschiedlichen Betriebssystemen und mit verschiedenen Werkzeugen", während inhaltliche Interoperabilität unabdingbar für die "Verständlichkeit der Datenstrukturen und der Metadaten" (Schöch 2017: 227) ist – was insbesondere für die Nachvollziehbarkeit der Forschungsergebnisse essentiell ist. Hinzu kommt der Aspekt der Langzeitverfügbarkeit von Daten, der durch die im Hinblick auf Interoperabilität vollzogene Standardisierung der Datenformate der Weg mindestens geebnet scheint.&lt;/p&gt;
&lt;p&gt;Interoperabilität offenbart sich letztendlich als geradezu implizit-selbstverständliches Desiderat digitaler Projektarbeit, verbleibt jedoch (oder vielleicht gerade deswegen) gleichzeitig in einer methodologisch vagen Abstraktheit. Die praktische Anwendbarkeit derartig standardisierter Datenmodelle und Anwendungen kann nur im Rahmen spezifischer Projektarbeit ermittelt werden. Dabei stellt sich die Frage nach dem Nutzen, den Grenzen und Kosten interoperabler Techniken im Projekt "Die Schule von Salamanca" insofern auf besondere Weise, als dieses einerseits über einen langen Zeitraum und mit Blick auf eine effiziente und langzeitverfügbare Erschließung von relativ großen (Text-)Datenmengen operiert, andererseits aber auch die Erforschung innovativer Textaufbereitungsverfahren und Webanwendungsfunktionen zur Ermöglichung neuartiger Forschungserkenntnisse für die beteiligten Fachwissenschaften anstrebt.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Das Projekt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Im durch die Akademie der Wissenschaften und der Literatur | Mainz geförderten und insgesamt auf 18 Jahre angelegten Projekt "Die Schule von Salamanca. Eine digitale Quellensammlung und ein Wörterbuch ihrer juristisch-politischen Sprache" (Duve et al. 2013) werden voraussichtlich insgesamt etwa 120 Texte der gleichnamigen Schule iberischer Theologen und Juristen des 16. und 17. Jahrhunderts nach und nach digitalisiert und als Volltexte erfasst. Die in TEI-XML ausgezeichneten und aufwändig normalisierten Texte werden nicht nur strukturell, sondern auch im Hinblick auf als Linked Open Data (LOD) referenzierbare Entitäten – etwa Personennamen – erschlossen; dabei werden die Text-Digitalisate, die Volltexte und die LOD-Datensammlungen online bereit gestellt. Hinzu kommt ein digitales (und schließlich auch gedrucktes) Wörterbuch, in dem sowohl biographische Informationen zu den in der Edition vertretenen Autoren als auch zentrale Begriffe der Rechts- und politischen Ideengeschichte und deren Entwicklung im Diskussionszusammenhang der "Schule von Salamanca" erfasst werden. In diesen beiden Säulen des Projekts ist die Erschließung und Repräsentation der Struktur der internen Verweise (Autoren, die sich wechselseitig zitieren, Wörterbuchartikel, die auf Textstellen verweisen) eines der zentralen wissenschaftlichen Ergebnisse.&lt;/p&gt;
&lt;p&gt;Die sowohl für die Benutzung als auch für die Bereitstellung der Daten als zentrales Portal dienende Webseite des Projekts (www.salamanca.school) ist technisch in Form einer komplex modularisierten Webanwendung implementiert, die fachwissenschaftlichen BenutzerInnen eine Vielzahl von Funktionen bieten soll, z.B. eine geräteübergreifende und performante Darstellung der Editionstexte, eine intelligente Suchfunktion, die den frühmodern-lateinischen und -spanischen Volltexten angepasst ist, und eine feinkörnige Referenzierung der Texteinheiten. Durch technische Mechanismen (content negotiation, API, RESTful Microservice-Architektur), Exportfunktionen (abschnitts-, text- oder corpusweise, plaintext-, TEI- oder andere Formate) und die Orientierung an Formatstandards wird Anforderungen der Interoperabilität explizit Rechnung getragen. Der Quelltext der Webanwendung wird bis Januar 2018 ebenfalls veröffentlicht und fortan in Open Source weiterentwickelt werden.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interoperabilitäts-"Konflikte": Einige Beispiele&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Einhergehend mit der Open Source-Veröffentlichung der Webanwendung soll über einige repräsentative Aspekte der Projektarbeit reflektiert werden, in denen sich an vermeintlich technischen Herausforderungen Konflikte um die wissenschaftlichen Implikationen von Interoperabilität entzünden:&lt;/p&gt;
&lt;p&gt;a) Textrepräsentation:&lt;/p&gt;
&lt;p&gt;Während in diesem Kernbereich der Projektarbeit gegenwärtig noch mit einem eigens spezifizierten und den bisherigen Forschungsanforderungen entsprechenden "idiosynkratischen" TEI-Format gearbeitet wird und die Datenmodellierung somit eher als "research-driven" (vgl. Flanders/Jannidis 2016: 233) bezeichnet werden kann, wird eine langfristige Verfügbarmachung der Texte in einem interoperableren, "curation-driven" Format wie etwa &lt;em&gt;TEI Simple&lt;/em&gt; (Text Encoding Initiative Consortium 2016) erwägt. Dies ist nicht zuletzt auch durch den Blick auf die Wissenschaftsförderung motiviert (vgl. etwa Deutsche Forschungsgemeinschaft 2015: 38), die die Interoperabilität von Textauszeichnungen dem erst noch nachzuweisenden Erkenntnisgewinn 'reicher' Annotationen gegenüberstellt. Allerdings sind die Module im XML-"Ökosystem" der Edition, das auch reine Metadaten-Module etwa zur Zeichenkodierung enthält und somit über reine Textrepräsentation hinausgeht, nicht ohne weiteres in &lt;em&gt;TEI Simple&lt;/em&gt; abbildbar. So ergibt sich eine nur durch einigen Aufwand aufzulösende Spannung zwischen detaillierter wissenschaftlicher Gegenstandsbeschreibung und Maßnahmen zur Förderung der Nachnutzbarkeit der eigenen Ressourcen (z.B. des Angebots mehrerer alternativer Datenformate), und es stellt sich die Frage, wann und durch wen jener Aufwand erbracht werden soll. Selbst eine gegenüber den "Experten"-Annotationen tolerantere Zielvorgabe, die sich etwa am Konzept des 'Interchange' (als einer durch menschliche Interpretation vermittelten Nachnutzung, vgl. Holmes 2017) orientiert und im Vergleich zum Modell einer bruchlosen Weiterverwertbarkeit der Daten durch automatische Prozesse gemäßigtere Ansprüche erhebt, sieht sich ähnlichen Fragen der Standardisierung von Schnittstellen und Datenformaten ausgesetzt.&lt;/p&gt;
&lt;p&gt;b) Modulare Infrastruktur:&lt;/p&gt;
&lt;p&gt;Im Zuge der Einrichtung einer Linked Open Data-Infrastruktur haben wir &lt;em&gt;content-negotiation&lt;/em&gt;-Mechanismen, Weiterleitungen und die Adressierung unterschiedlicher Funktionen über verschiedene Server und Server-Adressen eingeführt. Diese Entwicklung legt eine Fortsetzung nahe, die den Umbau der Web-Anwendung insgesamt in ein Ensemble von Microservices bedeuten würde (vgl. Wolff 2016). Während die Modularisierung von komplexen Anwendungen nachhaltige Entwicklung theoretisch befördert (vgl. Fisher et al. 2017) und die Interoperabilität, d.h. konversions- und barrierearme Nachnutzbarkeit der Daten enorm verbessert (etwa dadurch, dass auf verschiedenen Ebenen in verschiedenen Formaten und verschiedenen Granularitäten Daten und Dienste verfügbar sind, vgl. Turska et al. 2016) hat dies jedoch auch den Nachteil, dass der Daten- und Anwendungszusammenhang als ganzer nur in einer sehr viel aufwändigeren Weise repliziert und ggf. archiviert werden kann: Während erste Infrastrukturen die Archivierung und langfristige Zugänglichkeit von Javascript-Anwendungen erlauben sollen (vgl. Bingert/Buddenbohm 2016; kritischer: Brunelle 2016), so ist dies für eine solche Infrastruktur mit mehreren kooperierenden und kommunizierenden Servern nur sehr viel schwerer vorstellbar.&lt;/p&gt;
&lt;p&gt;c) Adressierung, Versionierung und Persistenz:&lt;/p&gt;
&lt;p&gt;Im Zusammenhang mit der Adressierung von einzelnen Textpassagen haben wir ein System eingeführt, das sowohl semantische Aussagen über Text-Entitäten als auch die Realisierung der Verweisstrukturen auf implementierungs- und plattformunabhängige, intellektuell intuitive Weise erlaubt (im Anschluss an das Canonical Text Services-Schema, Blackwell/Smith 2014; vgl. Wagner 2016). Diese Struktur der komplexen Verknüpfung von Ressourcen und Entitäten untereinander verträgt sich aktuell nicht mit etablierten Methoden, Dokumente persistent zu identifizieren und die Überarbeitungshistorie der Ressourcen in einer Versionierung transparent und nachvollziehbar zu machen. Diese Methoden spezifizieren nämlich zumeist den Umgang mit einem ganzen Dokument und vernachlässigen den Bedarf, Entitäten innerhalb des Dokuments (persistent) zu referenzieren oder die Einbettung jenes Dokuments in ein Corpus zu verwalten. Wenn ein Dokument beispielsweise verändert wird, muss es unter anderem normalerweise eine neue persistente ID erhalten - damit entsteht ein Aktualisierungs- oder mindestens Kontrollbedarf bei den Querverweisen innerhalb des Dokuments sowie in allen weiteren Dokumenten des Corpus (und in unserem Falle in allen Artikeln des Wörterbuchs), welche Verweise auf das aktuelle Dokument enthalten. Mechanismen wie das Memento framework (van de Sompel/Nelson 2015) bieten eine transparente und flexible Versionierung, die für Web-Dokumente wie für Semantische Ressourcen gleichermaßen funktioniert, sind jedoch in ihrer Integration mit PID-Systemen ebenfalls noch nicht erprobt.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diskussion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Die hier am Beispiel digitaler Projektarbeit aufgezeigten Probleme weisen auf grundlegendere wissenschafts- und erkenntnistheoretische Fragestellungen hin: Entspricht die Anpassung der eigenen Forschungsergebnisse an Interoperabilitätsbedürfnisse stets einer verlustbehafteten Konvertierung fachwissenschaftlicher "Expertensprache" in eine breiter verständliche "Umgangssprache"? Kann am Ende vielleicht Expertenwissen gar nicht "interoperabel" sein? Vor allem berührt die Frage nach den durch Interoperabilität verursachten Konflikten auch die methodologischen und epistemologischen Fundamente der Digital Humanities, die ja nicht zuletzt auf der Hoffnung gegründet sind, die traditionell stark individualisierte Forschung in den Geisteswissenschaften eben durch Interoperabilität ihrer Ergebnisse besser nachvollziehbar machen zu können, und betrifft somit auch ihre Verortung im Spannungsfeld zu den angrenzenden Disziplinen.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>36</session_ID>
  <session_short>VP_5b</session_short>
  <session_title>Der sehende Computer II</session_title>
  <session_start>2018-03-01 11:00</session_start>
  <session_end>2018-03-01 12:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Burghardt, Manuel</chair1>
  <attendee_count>2</attendee_count>
  <chair1_name>Manuel Burghardt</chair1_name>
  <chair1_organisation>Universität Regensburg</chair1_organisation>
  <chair1_email>manuel.burghardt@ur.de</chair1_email>
  <chair1_ID>1076</chair1_ID>
  <sessionID>36</sessionID>
  <presentations>3</presentations>
  <p1_paperID>158</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Bermeitinger, Bernhard
Howanitz, Gernot
Radisch, Erik</p1_authors>
  <p1_organisations>Lehrstuhl für Informatik mit Schwerpunkt Digital Libraries and Web Information Systems, Universität Passau, Deutschland
Lehrstuhl für Slavische Literaturen und Kulturen, Universität Passau, Deutschland
Lehrstuhl für Digital Humanities, Universität Passau, Deutschland</p1_organisations>
  <p1_emails>Bernhard.Bermeitinger@uni-passau.de
Gernot.Howanitz@uni-passau.De
Erik.Radisch@uni-passau.de</p1_emails>
  <p1_presenting_author>Bermeitinger, Bernhard
Howanitz, Gernot
Radisch, Erik</p1_presenting_author>
  <p1_title>Contextualizing Bandera: Ein Distant Watching Ansatz</p1_title>
  <p1_abstract>&lt;p dir="ltr"&gt;Anhand einer Beispielstudie zu der Rezeption der historischen Figur Stepan Banderas setzt die hier vorgestellte neue Methode “Distant Watching” um und stellt damit erstmals das Bild und dessen Inhalt an sich in den Mittelpunkt einer quantitativen Untersuchung. Ein State-of-the-art Regional Convolutional Neural Network (RCNN) wird auf konkrete vorselektierte Symbole in Videos trainiert, um diese in einem großen Videokorpus automatisiert erkennen zu können. Damit wird erstmals der Bildinhalt von Videos automatisiert erfass- und quantitativ messbar. Je nach (Co-)Präsenz oder Absenz von Symbolen können Rückschlüsse auf den Inhalt des Videos gezogen werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Diese Ausweitung des methodischen Repertoires auf Bilder bzw. Videos ermöglicht den Kulturwissenschaften quantitative Perspektiven auf Malerei, Photographie und Film. Auch Objekte oder performative Handlungen können über Bild- bzw. Videodokumentationen einer quantitativen kulturwissenschaftlichen Analyse zugeführt werden. Diese quantitative methodische Innovation muss allerdings durch eine qualitative ergänzt werden. Die vorliegende Studie setzt sich zum Ziel, diese Innovationen anzustoßen.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>229</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Böttger, Lucie
Zeckey, Alexander
Langner, Martin</p2_authors>
  <p2_organisations>Georg-August-Universität Göttingen, Deutschland - GCDH/Archäologisches Institut
Georg-August-Universität Göttingen, Deutschland - GCDH/Archäologisches Institut
Georg-August-Universität Göttingen, Deutschland - GCDH/Archäologisches Institut</p2_organisations>
  <p2_emails>Lucie.boettger@stud.uni-goettingen.de
alexander.zeckey@stud.uni-goettingen.de
mlangne@gwdg.de</p2_emails>
  <p2_presenting_author>Böttger, Lucie
Zeckey, Alexander
Langner, Martin</p2_presenting_author>
  <p2_title>Wahrnehmung und digitale Mustererkennung am Beispiel antiker Terrakottastatuetten</p2_title>
  <p2_abstract>&lt;p&gt;Sowohl die Informatik als auch die Bild- und Objektwissenschaften nutzen für die Klassifizierung von Objekten und der Bestimmung ihres Ähnlichkeitsgrades Klassifizierungsverfahren und Methoden der Mustererkennung. Während allerdings die Informatik darauf abzielt durch Mustervergleich die Klassifizierung unbekannter Objekte zu automatisieren, dienen Typologien in der Archäologie als Ordnungskriterien für soziokulturelle Fragen, um Informationen über Funktion, Bedeutung oder Produktion zu erfahren.&lt;/p&gt;
&lt;p&gt;Die in der Archäologie angewandten qualitativen Analysen basieren auf einem wissenschaftlichen Konstrukt von Klassifikationskriterien [Adams – Adams 2008]. In Fällen, in denen eine große Anzahl von Artefakten eine ganz ähnliche Form hat, sich aber in gewissen Einzelheiten deutlich unterscheidet, wie bei seriell hergestellten Terrakotta-Figuren, die nachträglich überarbeitet wurden, hat der Begriff der Typologie jedoch seine Grenzen erreicht. In Bezug auf die Wahrnehmung und den Wert der Figuren gibt es zu viele verschiedene Kriterien, die eine Bedeutung tragen. Nur ein statistischer Ansatz, der die Hauptmerkmale in Kombination mit archäologischen Quellen und den intrinsischen ästhetischen Werten (wie Farbe oder Stil) berücksichtigt, kann das Problem lösen.&lt;/p&gt;
&lt;p&gt;Die Methoden der 3D-Mustererkennung basieren in der Regel auf Konzepten der kognitiven Psychologie zur Objekterkennung. Die Form eines Objektes wird in geometrische Grundelemente zerlegt und statistisch nach Teilen und Teilsegmentierungen analysiert. Maschinelle Lernalgorithmen helfen, diesen Prozess zu automatisieren. Für die Klassifizierung von Artefakten können diese Methoden jedoch nur grobe Näherungswerte liefern. Auf der Grundlage archäologischer Kategorien kann eine rechnerische Merkmalsextraktion bisher nur manuell durch qualitativen Formvergleich durchgeführt werden. Darüber hinaus könnten in Bereichen, in denen archäologische Methoden keine entsprechenden Typologien schaffen konnten, Methoden der digitalen Formerkennung (shape recognition) hilfreich sein, um geeignete archäologische Kategorien zu definieren.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Deshalb sollen qualitative und quantitative Klassifizierungsmethoden kombiniert werden, um die Typologie von Artefakten zu überarbeiten und somit Vorarbeit zur Erschließung großer technischer bzw. mentaler Bildcorpora zu leisten. Hier werden die Methoden der Informatik (Objekterkennung und Formvergleich) und Archäologie (Typologie und Kopienkritik) von einander profitieren, um die oben genannten Mängel zu überwinden.&lt;/p&gt;
&lt;p&gt;Im Vordergrund steht die Frage nach der computergestützten Analysefähigkeit und ihren Grenzen in der Adressierung von Binnenstrukturen sowie der Möglichkeit, neuartige Analyseverfahren zu entwickeln.&lt;/p&gt;
&lt;p&gt;Als Materialgrundlage dienen die kleinformatigen, seriell aus Modeln genommenen Tonfiguren. [Burn 2012; Erlich 2015]. Diese bieten sich für Fragen der Klassifizierung zum einen aufgrund ihrer hoch überlieferten Anzahl, zum anderen wegen ihrer großen Formenvarianz an.&lt;/p&gt;
&lt;p&gt;Die antiken Terrakotten weisen untereinander verschiedene, vom Archäologen präzis definierbare Grade der Ähnlichkeit auf: So existieren die aus derselben Matrize genommenen Figuren, die eine exakte Übereinstimmung verbindet, die aus denselben Patrizen gewonnenen Figuren, die sich nur in der Größe von ihren ansonst genauen Ebenbildern unterscheiden und die ebenfalls aus derselben Matrize genommenen Figuren, die aber nachträglich noch durch zusätzliche Additionen und Abänderungen ein verändertes Erscheinungsbild aufweisen. Zudem die Terrakotten, die nicht derselben Matrize entstammen, sich aber in Haltung und Drapierung der Tracht untereinander ähneln. Zwar kann man auf der handwerklichen Ebene konstatieren, dass zwei Terrakotten aus derselben Produktion stammen, ist dies aber nicht der Fall, fehlen bislang geeignete Kriterien zur Bestimmung der Ähnlichkeit und ihrer Abstufungen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Grundlage einer Interpretation ist in der Archäologie die visuelle Beschaffenheit der Artefakte. Ein Großteil dieser Eigenschaften wie Größe, Form, Farbe oder Material lässt sich genau vermessen, somit verbal erfassen und dient vielen Digitalen Corpora der Klassischen Archäologie als Schlagworte zur Einordnung der Artefakte – eine zeit- und resourcenaufwändige Methode.&lt;/p&gt;
&lt;p&gt;Ein dem Textmining vergleichbares Objectmining ist in der Klassischen Archäologie bislang noch nicht erprobt. Hier setzt unser Projekt an, das sowohl anwendungsbezogen als auch methodenreflektierend vorgehen möchte, denn es sollen sowohl Verfahren der automatisierten Corpusbildung durch 3D-Mustererkennung entwickelt werden als auch die damit verbundenen Schematisierungen und ihr wissenschaftlicher Nutzen reflektiert werden. In mehreren Schritten werden die Ergebnisse wiederholt evaluiert und die Verfahren feiner kalibriert. Eine systematische Untersuchung formaler Elemente könnte als Schlüssel zur Entwicklung eines Konzepts der Materialisierung von Wissen und Anschauung dienen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Drei zentrale Leitfragen haben sich bisher herauskristallisiert:&lt;/p&gt;
&lt;p&gt;Lassen sich Figurentypen mit digitalen Methoden der Musterkennung nonverbal erfassen und in welcher Exaktheit?&lt;/p&gt;
&lt;p&gt;In welchem Umfang ist sprachlogische Begrifflichkeit zur sinnvollen Ausdifferenzierung der Typen notwendig?&lt;/p&gt;
&lt;p&gt;Sind die von der archäologischen Stilforschung entwickelten Kategorien zur Beschreibung von Typen auch für digitale Verfahren nutzbar oder müssen an ihre Stelle neue diakritische Verfahren treten?&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Ende des 19. Jhs. erstellte Franz Winter einen Katalog antiker Terrakotten, der ihre „Typen“ möglichst vollständig erfassen sollte [Winter 1903]. Seine Materialanordnung kann als frühe Form der archäologischen Mustererkennung gelten, denn er präsentierte die “Typen” in vereinfachten Umzeichnungen. Für eine weitreichende Erforschung der Terrakotten reichte dies jedoch kaum aus, denn die von Winter erstellten „Typen“ sind weder betitelt noch verbal definiert. Unter diesen somit alleinig durch die Zeichnung definierten „Figurentypen“ subsumierte er vermeintliche Wiederholungen, die keineswegs Typen im Sinne der bereits damals in der Skulpturenforschung etablierten Terminologie und schon gar nicht Terrakotten aus derselben Werkstatt oder gar Form bezeichneten [Heilmeyer 2008; Anguissola 2015]. Zudem wurden Abweichungen vom angezeigten „Typus“ zwar in wenigen Fällen erwähnt, es wurde allerdings nie auf den genauen Ähnlichkeitsgrad der zusammengefassten Terrakotten eingegangen.&lt;/p&gt;
&lt;p&gt;Aufgrund dieser evidenten Schwächen wurde kein vergleichbarer Versuch unternommen, die Gesamtheit der antiken Terrakotten in Typen zu unterteilen. Deshalb hat sich die archäologische Forschung den Fundkontexten der Terrakotten [Rotroff 1987; Graepler 1997; Rumscheid 2006] oder semiotischen Ansätzen [Haase 2003] zugewandt. Erst unter Einfluss der Material Culture Studies kehrte die archäologische Forschung zur morphologischen und ästhetischen Wirkung der Figurinen zurück [Bailey 2005; Lesure 2011]. Dieser Ansatz erscheint vielversprechend, vor allem, wenn der methodische Rahmen der Material Culture Studies [Berger 2009; Malafouris – Renfrew 2010; Gerritsen –  Riello 2015] mit einer Überprüfung von Konzepten zur Typologie kombiniert wird [Koortbojian 2002; Mattusch 2015].&lt;/p&gt;
&lt;p&gt;In diesem Projekt wird der Versuch Winters aufgegriffen eine „Typologie“ der Terrakotten zu erstellen. Um den Typus einer Figur besser klassifizieren zu können, wurde sie in die drei Ebenen Haltung, Gewanddrapierung und Oberflächengestaltung untergliedert. Für diese wurden erste vereinfachte Schemata erstellt.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Eine ökonomische Massendigitalisierung von 3D-Artefakten stellt weiterhin ein ungelöstes Problem dar. Obwohl die semantische Anreicherung von 3D-Daten anspruchsvoll ist, sind Methoden zur Verwendung der Geometrie der 3D-Form für das Data Mining ein aktives Forschungsgebiet [De Luca et al. 2014; Aggarwal 2015]. Verschiedene Verfahren der 3D-Object-Recognition sind seit vielen Jahren bekannt: CAD-Modelle, datengesteuerte geometrische Grundelemente, Oberflächen-Klassifizierung auf Grundlage des Gaußschen Image [Amann 1990; Taylor – Kleeman 2006] und digitaler Bildvergleich [Hueting et al. 2015]. Sie basieren meistens darauf, Grundformen automatisch aus Bereichsdaten zu extrahieren und bekannten Mustern zuzuweisen, um unbekannte Objekte zu klassifizieren. Die Formanalyse wird in der Regel statistisch durchgeführt [Dryden – Mardia 1998]. Statistische Werte, die geometrische Eigenschaften ähnlicher Formen beschreiben, werden mit der Hauptkomponentenanalyse (PCA) [Jolliffe 2002] ausgewertet, um die Formvariabilität zu analysieren. Darüber hinaus sind aber auch partielle Formanpassungsmethoden weit verbreitet [Funkhouser – Shilane 2006] Daneben läuft ein Umrissvergleich eines oder mehrerer Scheiben des 3D-Modells [Tal 2014] und mit bildbasierten 3D-Rekonstruktionsansätzen und formalisierten Grundelementen, um eine Elementbibliothek durch die einfache Deklaration einer Sequenz von Formteilen zu erzeugen [De Luca et al. 2014]. Im Allgemeinen ist es viel einfacher, die Form eines konzentrischen Feststoffs abzurufen als die einer komplexen Struktur. Die verfügbaren Methoden und Technologien bieten keine endgültige Lösung für diese. So entstehen Forschungsfragen in der inhaltsbasierten 3D-Objekt-Retrieval-Adressenabfrage und -klassifizierung auf texturierten 3D-Modellen, Bereichs-Scans-basierte 3D-Formwiedergewinnung, Formabfrage auf nicht starren 3D-Wasserdicht-Meshes, umfangreiches 3D-Formular-Retrieval und 3D-Objekt-Retrieval mit multimodalen Ansichten. Diese verschiedenen algorithmenbasierten Ansätze klassifizieren 3D-Modelle nur in Form von Grundinstanzen (wie Frau, Hund, Becher usw.).&lt;/p&gt;
&lt;p&gt;Eine schnelle Teilzerlegung in einfache geometrische Formen ist daher unzureichend. Vielmehr müssen geeignete Mustererkennungsverfahren entwickelt werden, die den Grad der Simplifizierung und Abstraktion an menschliche recognition and dissemination patterns knüpft und so die Klassifikation unbekannter Objekte schrittweise evaluiert und kalibriert.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Zur automatisierten Erfassung von Artefakten kamen diese Methoden bislang kaum zum Einsatz, obwohl Experimente mit Kurvenerkennung und Entlastungserkennung bereits mit archäologischen Artefakten durchgeführt wurden [Tal 2014]. Die Ursache liegt zum einen darin, dass keine hinreichende Zahl an Bildwerken als 3D-Modell vorliegt, um diese Verfahren in signifikantem Ausmaß auf ihre Anwendbarkeit zu überprüfen. Zum anderen stellen Kunstwerke (anders als z.B. Bauteile) wegen ihrer hohen Variabilität eine große Herausforderung an jede computergestützte Klassifikation dar. Die Zuordnung einer spezifischen Instanz zu einer allgemeineren Klasse fällt hier weitaus schwerer, da sie sich untereinander in ihrer Form, Größe und Farbe erheblich unterscheiden können.&lt;/p&gt;
&lt;p&gt;Daher wurde ein einfacher rechnerischer Formvergleich für "best fit" von Archäologen verwendet, um die Ähnlichkeit von zwei Artefakten zu analysieren [z.B. Beenhouwer 2008]. Best-Fit-Prozesse sind in Engineering und ähnlichen Branchen etabliert und es gibt zahlreiche Software-Lösungen. Diese Tests sind eher qualitativ als quantitativ und wurden bereits für die Toleranz basierte Pass/Fail Shape Analysis antiker Skulptur verwendet [z.B. Www.digitalsculpture.org/laocoon/index.html; Lu et al. 2013; Frischer 2014].&lt;/p&gt;
&lt;p&gt;Infolgedessen reicht es nicht aus, die Modelle in einfache geometrische Formen zu zerlegen. Ein vielversprechender Ansatz für Formerkennungsverfahren wird derzeit anhand der neu erstellten Terrakotta-Schemata unter Verwendung verschiedener Verfahren der shape comparison im Bereich 2/3D und machine learning entwickelt und evaluiert. Ziel ist es den Grad der Vereinfachung und Abstraktion nicht nur mit den menschlichen Erkennungs- und Verbreitungsmustern zu verknüpfen, um die unbekannten Objekte inkrementell zu bewerten und zu klassifizieren, sondern auch die in der Archäologie und Kunstgeschichte entwickelten Kategorisierungen zu verwenden. Die 3D-Mustererkennung der Hauptkomponenten (Form, Größe und Farbe) muss daher mit archäologischer Subkategorisierung und geeigneten Formen des maschinellen Lernens einhergehen [Bishop 2006].&lt;/p&gt;
</p2_abstract>
  <p3_paperID>287</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Donig, Simon
Christoforaki, Maria
Bermeitinger, Bernhard
Handschuh, Siegfried</p3_authors>
  <p3_organisations>Universität Passau, Deutschland
Universität Passau, Deutschland
Universität Passau, Deutschland
Universität Passau, Deutschland</p3_organisations>
  <p3_emails>simon.donig@uni-passau.de
Maria.Christoforaki@Uni-Passau.De
Bernhard.Bermeitinger@uni-passau.de
Siegfried.Handschuh@uni-passau.de</p3_emails>
  <p3_presenting_author>Donig, Simon</p3_presenting_author>
  <p3_title>Bildanalyse durch Distant Viewing - zur Identifizierung von klassizistischem Mobiliar in Interieurdarstellungen.</p3_title>
  <p3_abstract>&lt;p dir="ltr"&gt;In den vergangenen Jahren haben digitale Forschungsinstrumente in Kunst-, Architektur- und Designgeschichte sowie den Material-Culture Studies an Bedeutung gewonnen (Berry &amp; Fargerjord 2017; Klinke, 2017; Auslander 2005).  Wie viele disruptive Technologien verändern neue Techniken im Bereich der Computer Vision (Bell &amp; Ommer 2015; dies. 2016) die Arbeitsweise unsere Disziplinen. &lt;/p&gt;
&lt;p dir="ltr"&gt;Durch unsere Arbeit am Neoclassica-Framework (Donig, Christoforaki, Bermeitinger &amp; Handschuh, 2017) möchten wir  Forschenden solche neue Instrumenten und Methoden für die Analyse und Klassifizierung materialer Kultur, konstruktiver Merkmale und ästhetischer Formen des Klassizismus an die Hand geben. In unserer Forschung konzentrieren wir uns dabei zunächst auf Raumkunst (insbesondere Mobiliar und Innenausstattung) sowie Architektur und deren jeweilige visuelle Darstellungen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Klassifizierung von einzelnen Artefakten und ihre Identifizierung in Raumdarstellungen bildet deshalb einen nicht unwichtigen Meilenstein für das Projekt als Ganzes.&lt;/p&gt;
&lt;p dir="ltr"&gt;In dem hier vorliegenden Beitrag beschreiben und reflektieren wir einen Zugang zur Klassifizierung von Objekten in Einzeldarstellungen bzw. zu ihrer Identifikation in Raumdarstellungen, aufbauend auf unseren Experimenten (Bermeitinger, Donig, Christoforaki, et al., 2017) zur automatisierten Klassifizierung von materialer Kultur des Klassizismus mit “Tiefem Lernen”  (Deep Learning) - hier konkret Faltenden Neuronalen Netzen (Convolutional Neural Networks, CNN) (Krizhevsky, Sutskever, and Hinton 2012).&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p&gt;Interieuranalyse durch Distant Viewing – theoretische Überlegungen&lt;/p&gt;
&lt;p dir="ltr"&gt;“Distant Viewing” (hier verstanden als technische Überbrückung sowohl zeitlicher wie räumlicher Distanz und der Menge betrachtbarer Bilder)  eröffnet uns ein Instrument, das es ermöglicht, ein Verständnis für die tatsächlich oder auch nur kontemporär imaginierte Beschaffenheit von vergangenen Räumen zu entwickeln.&lt;/p&gt;
&lt;p dir="ltr"&gt;Da Bildquellen als selbstreferentielle Systeme mit einem spezifischen Eigensinn ausgestattet sind, bedürfen sie einer besonderen methoden- und quellenkritischen Durchdringung, denn selbst Bildwerke, die ihrem Anspruch nach einen dokumentierenden Charakter haben, bleiben ästhetischen Zwängen des Mediums unterworfen (verwiesen sei etwa auf die kritische vergleichende Analyse der Raumwirkung von Aquarellen aus dem sogenannten Wittelsbacher Album durch (Langenholt 2002: 47-49)).&lt;/p&gt;
&lt;p dir="ltr"&gt;Durch die Ausweitung kultureller Produktion und Konsumption - sowohl von Texten wie von Bildern - an der Epochenschwelle 1800 stehen für Forschungen in diesem Bereich reichhaltige Quellenbestände zur Verfügung. Diese reichen von eher typisierenden Raumdarstellungen (wie etwa in Karikaturen oder häufig auch zeitgenössischen Darstellungen des Alltags unterbürgerlicher Schichten) über ihrem Bewusstsein nach eher historisch-dokumentierende Ansätze (beispielsweise Alben mit Raumansichten, wie sie in den Oberschichten etwa als Hochzeitsgeschenke für “ausheiratende” weibliche Familienmitglieder beliebt waren) bis hin zu visionären Raum- und Werkstücksentwürfen, die einem konsumierbares Erzeugnis in ihrer Antikenrezeption zugleich auch eine gesellschaftliche Vision einschrieben (Pawlitzki, Bruer, Kunze 2009); (Kepetzis 2006); (Auslander 1996).&lt;/p&gt;
&lt;p dir="ltr"&gt;Auch wenn in der Forschungspraxis beide Ansätze stets aufeinander bezogen bleiben müssen, kann man idealtypisch zwischen einem Ansatz unterscheiden, der das Bildwerk vor allem im Hinblick auf seine Produktionsbedingungen und Produzenten befragt, und einem Ansatz, der das Bild als Quelle des Dargestellten analysiert.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;br /&gt;Betrachtet man Bildwerke etwa als Zeugnisse vergangener Alltagskultur und ihrer Praktiken, wird es durch den hier besprochenen Zugang beispielsweise möglich, Veränderungen in der Ausstattung und -gestaltung einander ähnlicher Räume über längere Zeit zu verfolgen. Wir können so zugleich Cluster und Typen von Räumen aufgrund der Darstellung ihrer Beschaffenheit zu bilden, die es wiederum erlauben tradierte Funktionszuschreibungen von Räumen kritisch zu hinterfragen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Stellt man dagegen das Dargestellte als Repräsentationsform von Geschmack in den Mittelpunkt, wird es in ähnlicher Weise möglich, Artefakte mit anderen digitalisierten Korpora abzugleichen. Unabhängig davon, ob das dargestellte Objekt jemals zur Ausführung gelangt ist, macht unser Zugang so beispielsweise für die Rezeptionsforschung den Transfer spezifischer Darstellungsweisen durch Medien wie etwa die Collection de Meubles  et Objets de Goût (La Mésangère 1761-1831) oder Musterbücher neu zugänglich.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p&gt;Instrumentenentwicklung&lt;/p&gt;
&lt;p dir="ltr"&gt;Ein erster wichtiger Meilenstein im Rahmen unseres Forschungsprogramms war die automatisierte Erkennung von Objekten in Einzeldarstellungen. Um zeitnah einen größeren Trainingskorpus zu generieren, haben wir zunächst Abbildungen von materialer Kultur des Klassizismus – hauptsächlich Möbel und Kleinkunst (darunter etwa Bronzen, Silberarbeiten etc.) – aus den Beständen des Metropolitan Museum of Art gescrapt, welches diese Bildwerke Anfang 2017 als Public Domain zugänglich gemacht hat (The Metropolitan Museum of Art, 2017). Dadurch ist es möglich die Trainings- und Testdaten an interessierte Dritte weiterzugeben, was die Reproduzierbarkeit des Experiments sicherstellt.&lt;/p&gt;
&lt;p&gt; Da ein Faltendes Neuronales Netz Bilder als ganzes klassifiziert, haben wir zunächst sichergestellt, dass jedes Bild lediglich ein Objekt in der Totalen zeigt. Dazu wurden alle Darstellungen von Möbelmerkmalen wie Nahaufnahmen ausgeschlossen sowie alle Bildwerke, die Interieurs, Ensembles und Möbel à la suite zeigen, derart aufgespalten, dass auf jeder Abbildung nur noch ein Möbelstück zu sehen ist. Diese Darstellungen haben wir gegebenenfalls so bearbeitet, dass noch sichtbare Teile benachbarter Objekte mit einer homogenen Farbe abgedeckt wurden. Im Endergebnis entstand so ein Korpus von 1246 Bildern, der 379 Artefakte umfasst. Diese wurden gemäß der Neoclassica-Ontologie (Donig, Christoforaki &amp; Handschuh, 2016) annotiert.&lt;/p&gt;
&lt;p dir="ltr"&gt;Zunächst haben wir mit der Standardimplementierung des VGG19 Layouts, das an einem Subset von 1000 Klassen von  ImageNet (Deng et al., 2009) vortrainiert wurde, diesen Korpus prozessiert. Dieses Verfahren resultierte in einer durchschnittlichen Genauigkeit von 0.82 und einem durchschnittlichen F1 Wert von 0.62. Wir wiederholten das Experiment 21 mal mit Aufteilungen des Korpus in verschiedene Trainings- und Test-Sets, wobei wir für jeden Durchlauf sehr ähnliche Ergebnisse erhalten haben.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p&gt;Überprüfung des Befunds an zeitgenössischen Möbelzeichnungen&lt;/p&gt;
&lt;p dir="ltr"&gt;Da der Korpus in seiner überwältigenden Mehrheit aus Fotografien des 20. Jahrhunderts besteht, beschlossen wir, durch einen Kontrollkorpus der aus einer Kompilation von Thomas Sheratons Möbelzeichnungen bestand (Sheraton/Munro 1910), nachzuprüfen, ob zeitgenössische Grafiken (und deren Reproduktionen) vergleichbar gute Resultate liefern können.&lt;/p&gt;
&lt;p dir="ltr"&gt;Wiederum wurden alle Blätter die mehrere Objekte zugleich zeigen so aufgesplittet, dass daraus Einzeldarstellungen wurden.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Dieser relativ begrenzte Korpus von 64 Abbildungen wurde mit einer durchschnittlichen Genauigkeit von 0.63 beziehungsweise 0.78 in den Top-2 und 0.84 in den Top-3 Klassen erkannt.&lt;/p&gt;
&lt;p dir="ltr"&gt;Bei der Kontrolle der Ergebnisse durch “Close” Viewing stellte sich zudem heraus, dass in einigen Fällen eine präzisere Klassifizierung vorgenommen worden war, als das Ausgangslabeling durch Sheraton selbst (so war etwa einem bei Sheraton pauschal als “Arm Chair” bezeichneten Möbel gemäß der Neoclassica Ontologie die präzisere Klasse Desk Chair zugewiesen worden).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Wir schließen daraus, dass zeitgenössische Grafiken nicht alleine mit einer ähnlich großen Genauigkeit klassifiziert werden können wie moderne Fotografien, sondern, dass damit vor allem auch der von uns gewählte Ansatz prinzipiell geeignet ist, um historische Interieurdarstellungen auszuwerten.&lt;/p&gt;
&lt;p&gt;Artefakte in Interieurszenen – zwei Zugänge&lt;/p&gt;
&lt;p dir="ltr"&gt;Ein Artefakt in einer Interieurszene zu klassifizieren, stellt eine besondere Herausforderung an das Schließen der Semantischen Lücke dar. Idealtypisch muss dazu erstens eine interessante Region im Bild identifiziert und zweitens muss diese anschließend korrekt klassifiziert werden. Ein derzeit verbreitetes Werkzeug für derartige Identifikations- bzw. Klassifizierungsaufgaben sind Regionale Faltende Neuronale Netze (Regional Convolutional Neural Network, RCNN) (Girshick et al. 2014). Dabei wird ein Korpus in einer Weise feinkörnig annotiert, dass spezifische Regionen (in der Gestalt von Polygonen) Objekte von Interesse und ihre Annotationen beinhalten.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Die Nachteile dieses Verfahrens sind vor allem forschungspraktischer Natur: Zum einen wird hochqualifizierte Domänexpertise benötigt, um eine große Menge an Annotationen vorzunehmen. Zum anderen fanden wir keine Werkzeuge mit einem für Domäneexperten und -expertinnen geeigneten Interface und Workflow, um Bildannotationen gemäß einer Ontologie vorzunehmen. Dies macht den gesamten Vorgang sehr zeitaufwändig.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Wir haben daher zunächst versucht, die bereits gelabelten Bilder des MET-Korpus derart zu nutzen, dass wir die Gesamtbilder der Einzelartefakte als Bounding-Box und das Label gemäß der Ontologie als Annotation verwenden. Diese haben wir dann mit einem einem RCNN prozessiert, das an COCO (Lin et al., 2014) vortrainiert war. Mit diesem Netz haben wir anschließend die in der Kuratierung des Ausgangskorpus ausgeschlossenen Raumdarstellungen klassifiziert. Dieser Ansatz hat jedoch leider nicht die von uns erhofften Resultate erbracht.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;In einem weiteren Schritt haben wir deshalb versucht, die Funktionsweise des RCNN vereinfacht nachzubilden, indem wir die Raumdarstellungen in Kacheln (Panels) unterteilt haben, die jeweils als “virtuelle Gesamtbilder” mit dem CNN klassifiziert wurden. Dadurch wird es möglich, den bereits erfolgreich trainierten Klassifikator weiterzuverwenden. Da dieses Vorgehen den Abbildungsmaßstab und die Position der für uns “interessanten Regionen” im Bild nicht aktiv bestimmen kann, haben wir mit Kacheln von 224x224 Pixeln begonnen (der Eingabegröße des von uns verwendeten VGG19-Layouts) und diese durch größere und kleinere Kachelformate ergänzt, die anschließend skaliert wurden.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Während dieser Zugang in zahlreichen Fällen zu sehr viel befriedigenderen Ergebnissen geführt hat, als unser erster Ansatz, hat sich dabei aber auch die Hypothese bestätigt, dass die Wahrscheinlichkeit der richtigen Zuordnung eines Artefakts zur angegebenen Klasse vom Zusammenspiel von Kachelgröße und Objektmaßstab abhängt. Dadurch, dass der von uns zunächst trainiere Klassifikator nicht an anderen Raummerkmalen wie ornamentaler  Stuckatur, architektonischen Strukturen wie Pfeilern usw. trainiert worden ist, kommt es zugleich unverhältnismäßig häufig zu Fehlklassifizierungen (False Positives).&lt;/p&gt;
&lt;p dir="ltr"&gt;Derzeit entwickeln wir unsere Forschung dergestalt weiter, dass wir ein RCNN zur Identifizierung “interessanter Regionen” in Interieurszenen trainieren. Dazu experimentieren wir mit dem durch Polygone annotierten Korpus von Sheraton-Zeichnungen. Diese können einerseits innerhalb desselben Frameworks klassifiziert werden, andererseits kann der Klassifizierungsschritt aber auch durch einen anderen Klassifikator wie das am MET-Korpus trainierte CNN erfolgen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Langfristig erhoffen wir uns durch das Erlernen von konstruktiven und ästhetischen Merkmalen, die ebenfalls von der Neoclassica-Ontologie repräsentiert werden sowie durch eine eingehendere Annotation der Artefakte, für die wir derzeit ein semantisches Annotations- und Kuratierungswerkzeug entwickeln eine weitere Verbesserung auf dem Weg hin zu einer effektiven Bildanalyse von Interieurdarstellungen.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>39</session_ID>
  <session_short>VP_5c</session_short>
  <session_title>Nachnutzung</session_title>
  <session_start>2018-03-01 11:00</session_start>
  <session_end>2018-03-01 12:30</session_end>
  <session_room_ID>1</session_room_ID>
  <session_room>Hörsaal B, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Gietz, Peter</chair1>
  <attendee_count>2</attendee_count>
  <chair1_name>Peter Gietz</chair1_name>
  <chair1_organisation>DAASI International</chair1_organisation>
  <chair1_email>peter.gietz@daasi.de</chair1_email>

  <chair1_ID>1019</chair1_ID>
  <sessionID>39</sessionID>
  <presentations>3</presentations>
  <p1_paperID>208</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Fischer, Frank
Jäschke, Robert</p1_authors>
  <p1_organisations>Higher School of Economics, Moskau
Humboldt-Universität, Berlin</p1_organisations>
  <p1_emails>ffischer@hse.ru
jaeschke@l3s.de</p1_emails>
  <p1_presenting_author>Jäschke, Robert</p1_presenting_author>
  <p1_title> Liebe und Tod in der Deutschen Nationalbibliothek</p1_title>
  <p1_abstract>&lt;p align="justify"&gt;Der Sammelauftrag der Deutschen Nationalbibliothek (DNB) beginnt 1913 und bezieht sich auf »lückenlos alle deutschen und deutschsprachigen Publikationen« (»Wir über uns«, 16.03.2017). Der DNB-Katalog ist natürlich längst digitalisiert und die Arbeit mit ihm mittlerweile sehr komfortabel, da der Datendienst der DNB unter &lt;u&gt;http://www.dnb.de/datendienst&lt;/u&gt; vierteljährlich einen Komplettabzug der Katalogdaten im RDF-Format bereitstellt, unter der freien Lizenz CC0 1.0. Momentan (Stand vom 23.06.2017) enthält er 14.102.309 Datensätze, also Metadaten zu von der DNB gesammelten Medien. Bisher gibt es aus geisteswissenschaftlicher Sicht nur wenige Versuche, diese Quelle nutzbar zu machen (eine Ausnahme bilden etwa Häntzschel u. a. 2009). Wir präsentieren ein einfaches Framework, mit dem verschiedene Aspekte des DNB-Katalogs untersucht werden können, seine Entwicklung über die knapp 105 Jahre seit Bestehen der Nationalbibliothek (vgl. auch Schmidt 2017, der für die Library of Congress einen ähnlichen Ansatz vorgestellt hat). Wir konzentrieren uns dabei auf Romane als Untersuchungsobjekt, von denen in der DNB rund 180.000 als solche rubriziert sind (dies entspricht nicht der Gesamtanzahl an Romanen, denn nachauflagen und Übersetzungen zählen dort mit hinein – außerdem fehlen auch einige Romane, da sie nicht entsprechend verschlagwortet worden sind. Dieser Vortrag ist methoden-, nicht vorderhand ergebniszentriert, wobei wir an zwei Anwendungsszenarien aus der Praxis der digitalen Literaturwissenschaft demonstrieren, wie Katalogmetadaten bei der Bearbeitung konkreter Forschungsfragen behilflich sein können bzw. diese überhaupt erst ermöglichen.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>279</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Blanken, Christine
Rettinghaus, Klaus</p2_authors>
  <p2_organisations>Bach-Archiv Leipzig, Deutschland
Sächsische Akademie der Wissenschaften zu Leipzig, Projekt Bach-Repertorium</p2_organisations>
  <p2_emails>blanken@bach-leipzig.de
rettinghaus@bach-leipzig.de</p2_emails>
  <p2_presenting_author>Blanken, Christine
Rettinghaus, Klaus</p2_presenting_author>
  <p2_title>Endstation Digital?! Herausforderung Metadaten und Nachhaltigkeit in musikwissenschaftlichen Datenbanken</p2_title>
  <p2_abstract>&lt;p&gt;Schwierigkeiten im aktuellen Umgang mit Metadaten sollen am Beispiel von „Bach digital“ (www.bach-digital.org) gezeigt und diskutiert werden.&lt;/p&gt;
&lt;p&gt;„Bach digital“ ist ein Kooperationsprojekt der Staatsbibliothek zu Berlin – Preußischer Kulturbesitz, der Sächsischen Landesbibliothek – Staats- und Universitätsbibliothek Dresden, dem Universitätsrechenzentrum Leipzig sowie dem Bach-Archiv Leipzig (seit 2017 zusätzlich der Staats- und Universitätsbibliothek Hamburg).&lt;/p&gt;
&lt;p&gt;Ausgangspunkt war eine reine Metadaten-Sammlung zu Werken und Quellen Johann Sebastian Bachs (1999-2008: „Göttinger Bach-Katalog“), die zum Abschluss der ‚Neuen Bach-Ausgabe‘ am Johann Sebastian Bach-Institut Göttingen erfolgte. Dieses Metadatensammlung war die Basis für  den Projektstart im Jahre 2008. Seitdem fördert die DFG das Projekt kontinuierlich, das mittlerweile mehrere Stufen der Bearbeitung durchlaufen hat:&lt;/p&gt;
&lt;p&gt;Das erste Digitalisierungsprojekt umfasste die sogenannten Originalquellen zu Johann Sebastian Bachs Musik, also Autographen und das originale Aufführungsmaterial Bachs, das sich zu etwa 90 % im Besitz der oben genannten Bibliotheken befindet. 2010 ging diese erste Stufe als www.bach-digital.de online.&lt;/p&gt;
&lt;p&gt;Daran schloss sich von 2013 bis 2016 die Digitalisierung von Sekundärquellen Bachscher Musik aus der Generation der Bach-Söhne und -Schüler an, ein Bestand, der besonders viel Tastenmusik J. S. Bachs umfasst und hier Forschungen zu individuellen Fassungen ermöglicht und Bachs Arbeitsweise in der Klavier- und Orgelmusik zwischen Kunstwerk und Unterrichtspraxis transparent zu machen hilft.&lt;/p&gt;
&lt;p&gt;Mittlerweile wurde die dritte Stufe gezündet: die konsequente Ausweitung der Datenbank in Metadaten und Digitalisaten auf die Musik der Bach-Söhne im Projekt „Quellenkorpus Bach-Söhne – Erschließung und Digitalisierung der Primärüberlieferung zu Werken Wilhelm Friedemann, Carl Philipp Emanuel, Johann Christoph Friedrich und Johann Christian Bach sowie deren Einbindung in das zu erweiternde Portal Bach digital“.&lt;/p&gt;
&lt;p&gt;Daneben werden Schritt für Schritt auch Werkverzeichnisse der Publikationsreihe „Bach-Repertorium“ sowie das derzeit neu erarbeitete „Bach-Werke-Verzeichnis III“ integriert sowie mittelfristig musikalische Incipits mittels Verovio implementiert bzw. suchbar gemacht werden.&lt;/p&gt;
&lt;p&gt;Diese stufenweise Bearbeitung eines Kernbestandes der Musik des 18. Jahrhunderts bewirkt hoffentlich eine mehr als solide Basis für eine quellenbasierte Forschung zur Musik der Bach-Familie: nicht nur als wichtiges Hilfsmittel der Bach-Forschung, sondern auch z. B. als Vergleichsobjekt für Studien zu anderen Repertoires, als unterstützendes Material für Forschungen zu Mitteldeutschland, als Kernbestandteil zur Provenienzforschung wichtiger Sammlungen wie Poelchau, Breitkopf etc. etc. Die Nutzungsmöglichkeiten sind in den vergangenen zehn Jahren stark angewachsen; und damit auf die Verantwortung, ein möglichst den diversen Anforderungen gerecht werdendes Material bestmöglich aufzubereiten.  &lt;/p&gt;
&lt;p&gt;Das MyCoRe-basierte Projekt wurde dabei von Anfang an durch eine Dokumentation begleitet, die es problemlos nachnutzbar macht: https://www.bach-digital.de/content/documentation.xml?XSL.lastPage.SESSION=/content/documentation.xml.&lt;/p&gt;
&lt;p&gt;Abseits der Bach-Forschung bzw. Musikwissenschaft werden Daten und Digitalisate von „Bach digital“ auch von einer breitgefächerten Bach-Community gesucht: das sind die weltweit großen Nutzerkreise musikinteressierter Laien sowie auch Musiker, die z. T. direkt nach originalen Quellen-Digitalisaten musizieren. Das Spektrum der Nutzer weitet sich nach unserer Erfahrung mit statistischen Daten zur Datenbank: je divergenter das ins Netz gestellte Material, desto vielfältiger die Nutzung.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;II.&lt;/p&gt;
&lt;p&gt;Die Menge der Datensätze an sich (es sind Stand September 2017 immerhin 8080 Musik-Quellen zu 3800 Werken der Bach-Familie) sowie der Umfang der Metadaten innerhalb eines Datensatzes ist nur mit hohem personellem Aufwand auf dem neuesten Stand der Forschung zu halten. Digitalisate und Metadaten werden so gut es geht laufend überprüft, auch mithilfe von Nutzer-Feedbacks – besonders jenen für die Bach-Forschung so wichtigen Power-Usern aus aller Welt. Dies ist eine ständige Anforderung, die die Daten selbst stellen, sobald sie öffentlich sichtbar sind. An der Aktualisierung der Datensätze aufgrund von Neuerkenntnissen der Bach-Forschung sollen deshalb nun auch mehr Mitarbeiter in der Forschungsabteilung des Bach-Archivs beteiligt werden als es Projektmitarbeiter für „Bach digital“ gibt. Ziel ist es, der Veraltung von Forschungsdaten entgegenzuwirken. Das Bach-Archiv sieht sich hier in der Verantwortung, die einmal publizierten Forschungsdaten mit den „Bach digital“-Nutzern möglichst zu teilen. Hierzu gehört auch die Mehrsprachigkeit, die derzeit nur mit Hilfe von strukturierten Daten umgesetzt werden kann. Fließtexte zu übersetzen ist mangels dafür vorhandener Projektmittel nur sehr begrenzt möglich. Alle anderen Daten sind aber mittlerweile auch in Englisch, Japanisch, Französisch (und mit Ende 2017 auch Spanisch) recherchierbar. Hierbei sind wiederum die Nutzer der Datenbank selbst behilflich. Geplant ist als nächstes eine Nutzerbefragung, die über die Interessen und Wünsche sowie Kritik oder weitere Formen der Common Science-Beteiligung Auskunft geben soll. Inwieweit dieses Ergebnis zu einer Umstrukturierung von Daten oder der Präsentation von Modulen führen wird oder muss, ist derzeit noch offen.&lt;/p&gt;
&lt;p&gt;Bei dieser prinzipiell optimistischen Sicht auf „Bach digital“ sollen weitere kritische Punkte nicht außer Acht gelassen werden, die aus dieser langjährigen Erfahrung mit den Metadaten resultieren:&lt;/p&gt;
&lt;p&gt;Die Datenbank-Struktur suggeriert Eindeutigkeit, suggeriert, dass die Daten dem - in letzter Zeit in den Polit-Medien - so beliebten Faktencheck standhalten. Die Herkunft der Daten, gerade auch bei Neuerkenntnissen, wird dabei oft nicht präzise offengelegt. Die Datenbankstruktur suggeriert indes meist, dass es hier um Fakten geht. Unsicherheiten können nur sehr begrenzt formuliert werden, gerade im Fall von strukturierten Daten.&lt;/p&gt;
&lt;p&gt;Gerade auch die gegenüber den Printmedien so einfach zu handhabende Datenänderung ist also ein Problem für die Transparenz von Forschungsdaten.&lt;/p&gt;
&lt;p&gt;Ein einfacher Daten-Austausch per Schnittstelle, sicher allgemein gewünscht und praktiziert, ist nur insofern dauerhaft praktikabel, so lange ermöglicht wird, dieses Procedere mehrfach zu wiederholen, gerade auch bei Richtigstellungen von Forschungsdaten. Ansonsten finden sich mehrere Versionen von Quellen- oder Werkdaten im Netz, die sicherlich unerwünscht sind.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;III.&lt;/p&gt;
&lt;p&gt;Immer mehr Datenbanken zu Musikern und musikalischen Quellen tummeln sich im Netz. Doch selbst wenn es inhaltliche Überschneidungen gibt, arbeiten sie zumeist aneinander vorbei. Dabei ist das größte Problem nicht einmal die Vergeudung von Ressourcen, sondern die prinzipielle Unmöglichkeit eines Datenaustauschs bzw. einer einfachen Nachnutzbarkeit der Metadaten – selbst wenn sie in den sogenannten „Quasi-Standards“ TEI oder MEI vorliegen. Bei öffentlich geförderten Projekten ist heutzutage Grundvoraussetzung, dass die „langfristige Sicherung von“ und der „grundsätzlich offene[n] Zugang zu“ Forschungsdaten gewährleistet sein muss, es aber bislang unklar ist, was genau dies heißt. Ist der Zugang schon „offen“ wenn man die Informationen im Internet finden kann, oder erst dann, wenn sie über eine Schnittstelle bereitgestellt werden? Solange Forschungsprojekte nur „digitale Inseln“ errichten, bringt das „Digitale“ keinen wirklichen Mehrwehrt.&lt;/p&gt;
&lt;p&gt;Zwar existieren bereits verschiedene Formate, die speziell für den Datenaustausch gedacht sind, wie z. B. MARC21 und METS/MODS, doch sind diese nur sehr eingeschränkt für (musikwissenschaftliche) Forschungsprojekte und Datenbanken einsetzbar. Auch spezielle Ontologien stehen bereit, die vom W3C zu den „Good Ontologies“ gezählt werden, also Ontologien, die vollständig dokumentiert, dereferenzierbar, von unabhängigen Datenlieferanten verwendet und möglicherweise von bestehenden Tools unterstützt werden („ontologies that are fully documented, dereferenceable, used by independent data providers and possibly supported by existing tools“). Beispiele dafür sind &lt;em&gt;Dublin Core&lt;/em&gt; und &lt;em&gt;The Music Ontology&lt;/em&gt;. Doch auch hier bleibt das Problem, dass diese Formate zu flexibel, zu schwammig gestaltet sind, um einen sinnvollen, nachvollziehbaren Datenaustausch zu gewährleisten, oder aber spezielle Forschungs-Erkenntnisse nicht hinreichend darin abgebildet werden können – ganz abgesehen davon, dass derlei Lösungen überhaupt erst einmal implementiert werden müssen.&lt;/p&gt;
&lt;p&gt;Auch können Projekte a-priori nicht immer vorhersehen, welche Daten genau anfallen werden, bzw. welche von anderen Forschern oder Projekten nachgenutzt werden könnten. Es ist also nicht unbedingt zielführend, Daten in allen möglichen Formaten anbieten zu wollen, selbst wenn die Ressourcen es gestatten verschiedene Daten-Export-Möglichkeiten bereitzustellen (und zu pflegen).&lt;/p&gt;
&lt;p&gt;Können RESTful APIs die Lösung aller Probleme sein? Diese ermöglichen es zwar, sehr spezielle Kombinationen aus Metadaten zusammen zu stellen. Dennoch bleibt das Problem der intern verwendeten Formate bestehen; beschreibt ein Feld „date“ ein Aufführungsdatum oder das Datum der Werkgenese?&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Um digitale Gräber zu verhindern, sind spezielle, klar definiert und strukturierte Datenformate vonnöten, die für klar definierte Anwendungsfälle einen echten Austausch ermöglichen und somit auch erstmals dezentrale Suchmaschinen ermöglichen. Solche Suchmaschinen können abseits von Google überhaupt erst wirkliche Interdisziplinarität herstellen, denn mit wachsender Zahl an digitalen Projekten – so begrüßenswert dies auch sein mag – steigt die Gefahr, dass man „den Wald vor lauter Bäumen nicht sieht“, also Ergebnisse anderer (vielleicht fachfremder) Projekte nicht wahrnimmt, und dadurch möglicherweise den eigenen Erkenntnisprozess behindert.&lt;/p&gt;
&lt;p&gt;Der aktuelle Umgang mit gesammelten Metadaten soll am Beispiel von „Bach digital“ gezeigt sowie mögliche Auswege skizziert und diskutiert werden. Vorgestellt werden dabei standardisierte Formate, die bereits heute den Informationsaustausch und -fluss ermöglichen und aufzeigen, was dadurch zukünftig möglich sein könnte, aber auch, wo die größten Lücken und dringendsten Desiderate bislang bestehen blieben.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p2_abstract>
  <p3_paperID>282</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Druskat, Stephan
Vertan, Cristina</p3_authors>
  <p3_organisations>Humboldt-Universität zu Berlin, Deutschland
Universität Hamburg, Deutschland</p3_organisations>
  <p3_emails>stephan.druskat@hu-berlin.de
fsha060@uni-hamburg.de</p3_emails>
  <p3_presenting_author>Druskat, Stephan
Vertan, Cristina</p3_presenting_author>
  <p3_title>Nachnutzbarmachung von Forschungsdaten und Tools am Beispiel altäthiopischer Korpora</p3_title>
  <p3_abstract>&lt;p&gt;Bei Werkzeugen für die Annotation von Sprachkorpora ist die Unterstützung verschiedener Skripte allein kein Zeichen dafür, dass tiefe Annotationen in der jeweiligen Software möglich sind. Insbesondere Mehrebenenannotationen nicht-europäischer oder historischer Sprachen verlangt dedizierte Tools. In diesem Beitrag werden wir zeigen, dass die Entwicklung dedizierter Annotationswerkzeuge dann als Lösung in Betracht gezogen werden kann, wenn gleichzeitig Schnittstellen zu Analyse-Tools entwickelt werden. Der Vorteil eines solchen Verfahrens ist die Realisierung eines Annotationsmodells, das exakt den Besonderheiten der Sprache entspricht und die Nachnutzbarmachung der annotierten Daten für weitere Forschungsfragen.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>130</session_ID>
  <session_short>VP_5d</session_short>
  <session_title>Modellierung</session_title>
  <session_start>2018-03-01 11:00</session_start>
  <session_end>2018-03-01 12:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Henrich, Andreas</chair1>
  <attendee_count>7</attendee_count>
  <chair1_name>Andreas Henrich</chair1_name>
  <chair1_organisation>Otto-Friedrich-Universität Bamberg</chair1_organisation>
  <chair1_email>andreas.henrich@uni-bamberg.de</chair1_email>
  <chair1_ID>1207</chair1_ID>
  <sessionID>130</sessionID>
  <presentations>3</presentations>
  <p1_paperID>178</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Braun, Manuel
Klinger, Roman
Padó, Sebastian
Viehhauser, Gabriel</p1_authors>
  <p1_organisations>Institut für Literaturwissenschaft, Universität Stuttgart
Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart
Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart
Institut für Literaturwissenschaft, Universität Stuttgart</p1_organisations>
  <p1_emails>manuel.braun@ilw.uni-stuttgart.de
roman.klinger@ims.uni-stuttgart.de
pado@ims.uni-stuttgart.de
viehhauser@ilw.uni-stuttgart.de</p1_emails>
  <p1_presenting_author>Viehhauser, Gabriel</p1_presenting_author>
  <p1_title>Digitale Modellierung von Figurenkomplexität am Beispiel des Parzival von Wolfram von Eschenbach</p1_title>
  <p1_abstract>&lt;p&gt;Einleitung&lt;/p&gt;
&lt;p&gt;Figuren gehören zu den einprägsamsten und wichtigsten Bestandteilen literarischer Erzählungen. Narratologische Analysen haben sich daher eingehend mit Figuren beschäftigt, und zwar insbesondere unter zwei Aspekten: dem ihrer strukturellen Bedeutung und dem ihrer Charakterisierung. Während der erste Aspekt von den Digital Humanities bereits modelliert worden ist (etwa im Rahmen von Netzwerkanalysen, vgl. Jannidis et. al. 2016, Piper et. al. 2017), steht die digitale Analyse der Figurencharakterisierung noch an den Anfängen, obwohl ihre Bedeutung für die Narratologie unstrittig ist (Jannidis 2004, 2009). Dies lässt sich darauf zurückführen, dass die Kennzeichnung von Figuren auf unterschiedlichste, durch formalisierte Analysen nur schwer erfassbare Arten erfolgen kann – etwa durch Beschreibungen, Figurenhandlungen und -reden oder Erzählerbewertungen.&lt;/p&gt;
&lt;p&gt;Um einen Einstieg in die digitale Erfassung von Figurencharakteristika zu ermöglichen, nehmen wir daher eine vergleichsweise einfach zu modellierende Facette der Figurendarstellung in den Blick, und zwar das Konzept der Figurenkomplexität, das auch in der traditionellen Theoriebildung eine prominente Rolle spielt: Nach der ebenso klassischen wie vielkritisierten Kategorisierung von Forster (1927) lassen sich Figuren nach ihrer Komplexität grundsätzlich in &lt;em&gt;flat&lt;/em&gt; und &lt;em&gt;round characters&lt;/em&gt; einteilen. Trotz der zahlreichen Differenzierungsversuche späterer Forscher prägt dieses Modell noch heute gängige Annahmen über die literaturgeschichtliche Entwicklung von Figurenbeschreibungen. So gelten insbesondere die Figuren der mittelhochdeutschen Literatur klischeehaft als &lt;em&gt;flat characters&lt;/em&gt;, die in erster Linie auf ihre Funktion für das Handlungsgefüge hin konstruiert werden und nicht die Tiefe späterer Charakterdarstellung erreichen (vgl. zusammenfassend Schulz 2012). Zugleich ist aber in der mediävistischen Literaturwissenschaft selbst herausgestellt worden, dass die Konstruktion komplexer Charaktere zu den grundlegenden Gestaltungsprinzipien mittelhochdeutscher Texte gehören kann.&lt;/p&gt;
&lt;p&gt;Als Paradefall hierfür kann der um 1200/1210 geschriebene Artusroman ‚Parzival‘ angesehen &lt;em&gt;werden&lt;/em&gt;, dessen Autor Wolfram von Eschenbach im Prolog das Konzept eines ‚gemischten‘, nicht eindeutig gut oder böse bewerteten Menschentypus entwirft, von dem er im Folgenden handeln will. Dementsprechend lassen sich im ‚Parzival‘ immer wieder Figuren finden, die nicht dem höfisch-adeligen Ideal (moralisch gute Figuren sind zugleich auch schön, tapfer, mächtig etc.) entsprechen, sondern in ihrer Darstellung widersprüchlich erscheinen und daher als komplex angesehen werden können.&lt;/p&gt;
&lt;p&gt;Im Folgenden schlagen wir eine Methodik vor, mit der diese Einschätzungen der hermeneutisch vorgehenden Forschung digital modelliert und überprüft werden kann.&lt;/p&gt;
&lt;p&gt;Methode&lt;/p&gt;
&lt;p&gt;Mit dieser Pilotstudie möchten wir etablieren, ob man bereits mit extrem einfachen Methoden Vorhersagen zur Figurenkomplexität treffen kann, und verwenden dazu &lt;em&gt;distributionelle Analysen&lt;/em&gt; (Harris 1954, Miller und Charles 1991). Distributionelle Analysen repräsentieren die Bedeutung eines Zielwortes durch Eigenschaften der Kontexte seines Vorkommens in Textkorpora – im einfachsten Fall durch die Häufigkeiten aller Kontextwörter innerhalb eines Fensters um die Vorkommen des Zielwortes (‚Kontextvektor‘).&lt;/p&gt;
&lt;p&gt;Spezifisch betrachten wir zwei Arten von Kontextvektoren: (a), &lt;em&gt;lexikalische Kontextvektoren&lt;/em&gt;, berechnet über lemmatisierte Kontexte (b), &lt;em&gt;entitätsbasierte Kontextvektoren&lt;/em&gt;, berechnet über Eigennamen in den Kontexten. Dabei misst (a) die &lt;em&gt;lexikalische&lt;/em&gt; Vielfalt der Kontexte, in denen die jeweilige Figur vorkommt, und (b) entsprechend die &lt;em&gt;soziale&lt;/em&gt; Vielfalt der Kontexte. Beide Arten von Repräsentationen verwenden Frequenzen und greifen als Kontext auf die sogenannten Dreißiger-Abschnitte des ‚Parzival‘ zurück, Abschnitte von 30 Versen also, in die der ‚Parzival‘ nach dem Erstherausgeber Karl Lachmann gegliedert sein soll.&lt;/p&gt;
&lt;p&gt;Kontextvektoren werden typischerweise miteinander verglichen, um die semantische Ähnlichkeit der Zielwörter zu modellieren und werden in der Computerlinguistik breit eingesetzt (Pantel und Turney 2010). In unserer Studie betrachten wir stattdessen den &lt;em&gt;Informationsgehalt&lt;/em&gt; der Kontextvektoren als Prädiktor für die Figurenkomplexität und messen ihn per &lt;em&gt;Entropie&lt;/em&gt;. Entropie eruiert den Informationsgehalt &lt;em&gt;H(p)&lt;/em&gt; einer (Wahrscheinlichkeits-)Verteilung &lt;em&gt;p&lt;/em&gt;, definiert als &lt;em&gt;H(p) = –&lt;/em&gt;Σ&lt;em&gt;&lt;sub&gt;x&lt;/sub&gt; p(x) &lt;/em&gt;log &lt;em&gt;p(x)&lt;/em&gt; (Manning und Schütze 1999). Entropie kann informell verstanden werden als die Anzahl an Bits, die an Information in der Verteilung enthalten sind. Damit enthalten die Kontextvektoren eine höhere Entropie, die in gleichmäßiger verteilten Kontexten vorkommen, als die, die häufig in gleichen Kontexten auftreten.&lt;/p&gt;
&lt;p&gt;Dies entspricht der (aus theoretischer Perspektive natürlich übervereinfachenden) Annahme, dass Figuren, die in reicheren und verschiedeneren Kontexten vorkommen, als komplexer wahrgenommen werden. Bei lexikalischen Kontexten (s.o.) sagen wir also voraus, dass höhere lexikalische Variabilität auf Komplexität hinweist; bei entitätsbasierten Kontexten übernimmt diese Rolle das gemeinsame Auftreten einer Figur mit mehreren anderen Figuren.&lt;/p&gt;
&lt;p&gt;Anzumerken ist allerdings, dass Frequenz ein starker Störfaktor in der Interpretation von Entropie darstellt: Häufigere Zielwörter kommen – ceteris paribus – mit mehr verschiedenen Kontexten vor und erhalten eine höhere Entropie. Aus diesem Grund sind Entropiewerte nicht absolut interpretierbar. Eine belastbare Interpretation ergibt sich aber durch den Vergleich der Entropiewerte für Zielwörter mit möglichst ähnlicher Frequenz.&lt;/p&gt;
&lt;p&gt;Experiment&lt;/p&gt;
&lt;p&gt;Als Textgrundlage verwenden wir den ‚Parzival‘ nach der 5. Auflage der Ausgabe Lachmanns (1891) in der digitalisierten, mit Lemmata versehenen Fassung von Yeandle (2014).&lt;/p&gt;
&lt;p&gt;Für unser Hauptexperiment beschränken wir uns auf die sieben im Text am häufigsten namentlich genannten Frauenfiguren Cundrîe, Herzeloyde, Jeschûte, Cunnewâre, Arnîve, Bêne und Itonjê. Hinzu kommt Sigûne, als ein in der Forschung oft genanntes Beispiel für eine weniger komplexe Figur im ‚Parzival‘. Sieben der acht Figuren liegen in einem engen Frequenzband zwischen 30 und 40 Vorkommen, sind also gut vergleichbar, während Sigûne nur 14 Mal vorkommt. Wir präsentieren die Ergebnisse per Streudiagramm, mit Entropie und Frequenz als den beiden Achsen.&lt;/p&gt;
&lt;p&gt;Abbildung 1 und 2 zeigen die Ergebnisse für die wichtigen weiblichen Figuren. Diese lassen sich zwanglos mit den Einschätzungen der hermeneutischen Literaturwissenschaft in Einklang bringen. Die höchsten Werte zeigen Cundrîe und Herzeloyde, auf deren Komplexität die Forschung immer wieder hingewiesen hat: Bei Cundrîe handelt es sich um die Gralsbotin, die zwar kultiviert und nach höfischen Sitten gekleidet auftritt, jedoch monströs hässlich ist. Sie klagt Parzival zwar berechtigterweise an, fällt dabei aber aus dem höfischen Rahmen. Herzeloyde zieht ihren Sohn Parzival aus (übermäßiger?) Trauer um ihren im Kampf gefallenen Ehemann fern von der höfischen Welt in einer Walteinöde auf, um zu verhindern, dass auch er einst Ritter wird – ein Verhalten, das die Forschung unterschiedlich bewertet. Mittelwerte erreichen Cunneware und Jeschute, die in der Forschung auch kontrovers diskutiert werden. Bêne, Arnîve und Itonjê lassen sich hingegen als Nebenfiguren bezeichnen, für die keine hohe Komplexität zu erwarten war.&lt;/p&gt;
&lt;p&gt;Die in der Trauer um ihren Geliebten Schionatulander verharrende Sigûne, die gerade in ihrer Geradlinigkeit einen Gegenentwurf zu den auf Ruhm und Ehre versessenen Artusrittern darstellt, zeigt erwartungsgemäß keine hohen Entropiewerte. Allerdings erlaubt dieser Befund keine starke Interpretation, da für Sigûne aufgrund der niedrigeren Frequenz von vorneherein niedrigere Entropiewerte zu erwartet sind. Um dennoch eine Vorhersage zur Komplexität Sigûnes zu erhalten, vergleichen wir in den Abbildungen 3 und 4 ein breiteres Spektrum an Figuren im Frequenzbereich zwischen 10 und 20 bezüglich ihrer Entropie. Dies entspricht einer erweiterten Version unserer Hypothese: Figuren, die &lt;em&gt;verglichen mit anderen Figuren des gleichen Frequenzbereiches&lt;/em&gt; besonders hohe bzw. niedrige Entropien aufweisen, sind &lt;em&gt;relativ zu diesen &lt;/em&gt;mehr bzw. weniger komplex.&lt;/p&gt;
&lt;p&gt;Interessanterweise liefern die beiden Kontextdefinitionen für Sigûne abweichende Vorhersagen: Die unterdurchschnittliche soziale Kontextvielfalt entspricht durchaus der Isoliertheit der Figur, die sich aus Trauer von der Welt zurückzieht und wenn, dann fast nur noch mit Parzival interagiert. Die vergleichsweise hohe lexikalische Vielfalt könnte demgegenüber darauf zurückzuführen sein, dass Sigune neben ihrer Trauer auch die weitere Funktion hat, Parzival über seine Herkunft aufzuklären: Während Sigune als Figur eher statisch erscheint, sind ihre Erzählungen über die Gralswelt reichhaltig und komplex. Für weitere Untersuchungen wären daher weitere Faktoren der Figurendarstellung wie Figurenbeschreibung und Figurenrede mit einzubeziehen sowie das Verhältnis von lexikalischer und sozialer Kontextvielfalt näher zu bestimmen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Diskussion&lt;/p&gt;
&lt;p&gt;Unsere Ergebnisse legen nahe, dass sich die Analyse der lexikalischen und ‚sozialen’ Vielfalt von Figurenkontexten durchaus als taugliches Maß zu einer allerersten Annäherung an das Konzept der Figurenkomplexität eignen. Damit konnte mit erstaunlich einfachen Mitteln ein erster Zugang zum komplexen narratologischen Themenfeld der Figurencharakterisierung gewonnen werden. Von hier aus ließe sich der Ausgang zu einer differenzierteren Modellierung von Figurenkomplexität nehmen, die auch auf die traditionelle narratologische Theoriebildung zurückwirken kann: Durch die digitale Modellierung wird es nötig, die Faktoren, aus denen sich das Konzept der Figurenkomplexität zusammensetzt (etwa Figurenrede, -handlung oder -beschreibung), genauer und expliziter zu bestimmen und über ihre Gewichtung nachzudenken. Außerdem wäre genauer zu klären, was das Konzept der Komplexität meinen soll. Im Sinne einer kritisch reflektierten digitalen Methodik werden somit vielschichtige literaturwissenschaftliche Phänomene wie die Charakterisierung von Figuren durch den formalisierenden Zugang nicht nivelliert, sondern die Verbindung von quantitativen und qualitativen Methoden zur wechselseitigen Erhellung genutzt.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p1_abstract>
  <p2_paperID>273</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Nasarek, Robert
Moeller, Katrin</p2_authors>
  <p2_organisations>Martin-Luther-Universität Halle-Wittenberg, Deutschland
Martin-Luther-Universität Halle-Wittenberg, Deutschland</p2_organisations>
  <p2_emails>robert.nasarek@geschichte.uni-halle.de
katrin.moeller@geschichte.uni-halle.de</p2_emails>
  <p2_presenting_author>Nasarek, Robert</p2_presenting_author>
  <p2_title>Die Ontologie historischer deutschprachiger Berufs- und Amtsbezeichnungen.</p2_title>
  <p2_abstract>&lt;p&gt;Berufsbezeichnungen sind eine der häufigsten Angaben von individualspezifischen Quellen. Besonders in den Sozial- und Politikwissenschaften, den Geisteswissenschaften und einigen naturwissenschaftlichen Disziplinen bieten Berufsbezeichnungen einen wichtigen Bezugspunkt sozialstruktureller Analysen.&lt;/p&gt;
&lt;p&gt;Mit der Ontologie deutschsprachiger Berufs- und Amtsbezeichnungen möchten wir eine Berufssystematik für historische, deutschsprachige Berufe entwickeln, die sowohl an die (modernen) Klassifikationssysteme anknüpfen als auch die deutschen Berufsbezeichnungen für internationale, historische Klassifikationssysteme (HISCO, PST) anschlussfähig macht und die sowohl als maschinenlesbare Ontologie als auch mittels eines Webservice für einen individuellen Zugriff genutzt werden kann.&lt;/p&gt;
&lt;p&gt;Die erreichten Zwischenergebnisse ermöglichen anhand unseres Projekts eine Präsentation über die Vermeidung von epistemischen Fallstricken durch informationstechnische Automatisierung und eine kritische Betrachtung der Effizienzsteigerung digitaler Werkzeuge, aber auch über die neuen Möglichkeiten zur Bewältigung von Big Data und dem dazugehörigen Erkenntnisgewinn.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>285</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Bruder, Daniel
Teufel, Simone</p3_authors>
  <p3_organisations>University of Cambridge, Vereinigtes Königreich
University of Cambridge, Vereinigtes Königreich</p3_organisations>
  <p3_emails>dmb77@cam.ac.uk
sht25@cl.cam.ac.uk</p3_emails>
  <p3_presenting_author>Bruder, Daniel</p3_presenting_author>
  <p3_title>Data models for Digital Editions: Complex XML versus Graph structures</p3_title>
  <p3_abstract>&lt;p&gt;Today's digital editions – and with them – digital archives suffer from idiosyncrasy and ephemerality. &lt;br /&gt;Crafting, curating and re-using digital editions and their tools is tedious, expensive and error-prone. Disturbingly, digital editions still fall short of the advantages of the printed book in terms of longevity and interoperability. &lt;br /&gt;These problems aren't new to the Digital Humanities community. They have been, and they are still being discussed: A hotly debated topic and in most cases, the blame is put onto the TEI scheme. But these problems are not caused by having available a common encoding scheme (formulated as they "TEI abstract model"), but rather, they are the consequences of the chosen data model, which is independent of the TEI itself. &lt;br /&gt;XML and its tree model are the cause of the problem, but as the TEI abstract model is independent from a specific data model, any other data model could be applied. &lt;br /&gt;Today's digital editions and archives are deeply affected by the unsuitability of the data model of the ordered tree to record and archive complex text. &lt;br /&gt;The tree structure is not suitable to model many of the phenomena found in literary manuscripts and it forces editors to laboriously work around its limitations, setting in motion a vicious circle of staggering complexity in tools and data. &lt;br /&gt;This talk will critically point out the limitations of the tree model and contrast it with the data model of directed graphs. &lt;br /&gt;The TEI abstract model could equally be adapted to graph structures to enable digital archives with the longevity of the printed book as well as interoperable and durable digital editions.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>77</session_ID>
  <session_title>Mittagspause</session_title>
  <session_start>2018-03-01 12:30</session_start>
  <session_end>2018-03-01 14:00</session_end>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>181</session_ID>
  <session_short>Vernetzungstreffen 4</session_short>
  <session_title>CLARIN Legal Issues</session_title>
  <session_start>2018-03-01 12:30</session_start>
  <session_end>2018-03-01 14:00</session_end>
  <session_room_ID>15</session_room_ID>
  <session_room>S 22, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>175</session_ID>
  <session_title>Führung durch das VR-Labor des Regionalen Rechenzentrums Köln</session_title>
  <session_start>2018-03-01 13:00</session_start>
  <session_end>2018-03-01 13:30</session_end>
  <session_info>maximale Teilnehmerzahl: 10 &lt;br&gt;
Anmeldung nur vor Ort im Konferenzsekretariat. </session_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>176</session_ID>
  <session_title>Führung durch das VR-Labor des Regionalen Rechenzentrums Köln</session_title>
  <session_start>2018-03-01 13:30</session_start>
  <session_end>2018-03-01 14:00</session_end>
  <session_info>maximale Teilnehmerzahl: 10 &lt;br&gt;
Anmeldung nur vor Ort im Konferenzsekretariat. </session_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>183</session_ID>
  <session_short>AG 7</session_short>
  <session_title>AG Digitale Rekonstruktionen Treffen</session_title>
  <session_start>2018-03-01 14:00</session_start>
  <session_end>2018-03-01 15:30</session_end>
  <session_room_ID>9</session_room_ID>
  <session_room>S 12, Seminargebäude</session_room>
  <session_room_info>Seminargebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>53</session_ID>
  <session_short>Panel_6a</session_short>
  <session_title>Digitale Edition II</session_title>
  <session_start>2018-03-01 14:00</session_start>
  <session_end>2018-03-01 15:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Hadersbeck, Maximilian</chair1>
  <attendee_count>4</attendee_count>
  <chair1_name>Maximilian Hadersbeck</chair1_name>
  <chair1_organisation>Ludwig Maximilians Universität München</chair1_organisation>
  <chair1_email>maximilian@cis.uni-muenchen.de</chair1_email>
  <chair1_ID>1032</chair1_ID>
  <sessionID>53</sessionID>
  <presentations>1</presentations>
  <p1_paperID>228</p1_paperID>
  <p1_contribution_type>Panel</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Bosse, Anke
Fanta, Walter
Godler, Katharina
Brüning, Gerrit
Boelderl, Artur</p1_authors>
  <p1_organisations>Robert-Musil-Institut, AAU Klagenfurt
Robert-Musil-Institut, AAU Klagenfurt
Robert-Musil-Institut, AAU Klagenfurt
Goethe-Universität Frankfurt / Freies Deutsches Hochstift
Institut für Germanistik, AAU Klagenfurt</p1_organisations>
  <p1_emails>anke.bosse@aau.at
walter.fanta@aau.at
katharina.godler@aau.at
Bruening@em.uni-frankfurt.de
artur.boelderl@aau.at</p1_emails>
  <p1_presenting_author>Bosse, Anke
Fanta, Walter
Godler, Katharina
Brüning, Gerrit
Boelderl, Artur</p1_presenting_author>
  <p1_title>musilonline - integral lösen. Dialogfeld Digitale Edition</p1_title>
  <p1_abstract>&lt;p&gt;Im Rahmen des Panels wird das digitale Editionsprojekt &lt;em&gt;musilonline&lt;/em&gt; am Robert-Musil-Institut/Kärntner Literaturarchiv der Alpen-Adria-Universität Klagenfurt vorgestellt und das künftig geplante Interface präsentiert. Im Anschluss werden ein Problembericht mit Fallbeispielen zur Textauszeichnung der Manuskripte mit XML/TEI im Musil-Nachlass folgen und die Migration des Textdaten-Korpus aus dem Flatfile des Formats FolioViews in das Zielformat XML/TEI geschildert. Zuletzt sollen Überlegungen zum interdiskursiven Online-Kommentar auf &lt;em&gt;musilonline&lt;/em&gt; zu einer offenen Diskussion zu derzeit geltenden Standards zur Langzeitarchivierung, interoperablen Repräsentation und Online-Kommentierung von digitalen Textkorpora überleiten.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>149</session_ID>
  <session_short>Panel_6b</session_short>
  <session_title>Der sehende Computer III</session_title>
  <session_start>2018-03-01 14:00</session_start>
  <session_end>2018-03-01 15:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Howanitz, Gernot</chair1>
  <session_info>Moderation: Gernot Howanitz</session_info>
  <attendee_count>4</attendee_count>
  <chair1_name>Gernot Howanitz</chair1_name>
  <chair1_organisation>Universität Passau</chair1_organisation>
  <chair1_email>gernot.howanitz@uni-passau.de</chair1_email>
  <chair1_ID>1084</chair1_ID>
  <sessionID>149</sessionID>
  <presentations>1</presentations>
  <p1_paperID>211</p1_paperID>
  <p1_contribution_type>Panel</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Burghardt, Manuel
Heftberger, Adelheid
Müller-Birn, Claudia
Pause, Johannes
Walkowski, Niels-Oliver
Zeppelzauer, Matthias</p1_authors>
  <p1_organisations>Universität Regensburg
Brandenburgisches Zentrum für Medienwissenschaften, Potsdam
Freie Universität Berlin
Universität Luxemburg
Berlin-Brandenburgische Akademie der Wissenschaften
Fachhochschule St. Pölten</p1_organisations>
  <p1_emails>manuel.burghardt@ur.de
adelheidh@gmail.com
clmb@inf.fu-berlin.de
johannes.pause@hotmail.de
walkowski@nowalkowski.de
Matthias.Zeppelzauer@fhstp.ac.at</p1_emails>
  <p1_presenting_author>Burghardt, Manuel
Heftberger, Adelheid
Müller-Birn, Claudia
Pause, Johannes
Walkowski, Niels-Oliver
Zeppelzauer, Matthias</p1_presenting_author>
  <p1_title>Computergestützte Film- und Videoanalyse</p1_title>
  <p1_abstract>&lt;p&gt;Das Thema Bild- und Bewegtbildanalyse gewinnt auch in der – bis dato stark auf Text fokussierten – Digital Humanities-Community immer mehr an Bedeutung. Vor diesem Hintergrund wurde Anfang des Jahres eine dedizierte DHd-Arbeitsgruppe „Film und Video“ gegründet. Nachdem die AG im Juli 2017 bereits  ein erstes Symposium zum Thema „Film rechnen – Computergestützte Methoden in der Filmanalyse“ in Regensburg abgehalten hat, soll als nächster Schritt ein entsprechendes Panel auf der DHd 2018 in Köln ausgerichtet werden, um in einem größeren Kreis die Grenzen und Möglichkeiten computergestützter Analyseverfahren für Film und Video zu diskutieren.&lt;/p&gt;
&lt;p&gt;Im Rahmen des Panels sollen nach einer kurzen Vorstellung der entsprechenden DHd-AG in insgesamt fünf Impulsreferaten exemplarische Ansätze zur computerbasierten Analyse von Film und Video vorgestellt und mit dem Plenum diskutiert werden. Im Anschluss an die Panelvorträge soll dann eine allgemeine Diskussionsrunde mit den Vortragenden und dem Plenum stattfinden, in der ggf. weitere digitale Analyseansätze ergänzt und zur Diskussion gestellt werden können. Weiterhin sollen strategische Ziele der AG „Film und Video“ vorgestellt und diskutiert werden. Durch diesen Austausch soll die Möglichkeit methodischer Überschneidungen mit anderen Forschungsfeldern evaluiert und diskutiert werden um so einen Beitrag zur methodischen Profilierung computergestützter Analyseverfahren und zum Methodentransfer zwischen verschiedenen Forschungsfeldern insgesamt zu leisten.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>150</session_ID>
  <session_short>Panel_6c</session_short>
  <session_title>Wissenschaftsorganisation III</session_title>
  <session_start>2018-03-01 14:00</session_start>
  <session_end>2018-03-01 15:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Schulte, Stefan</chair1>
  <attendee_count>3</attendee_count>
  <chair1_name>Stefan Schulte</chair1_name>
  <chair1_organisation>Philipps-Universität Marburg</chair1_organisation>
  <chair1_email>stefan.schulte@uni-marburg.de</chair1_email>
  <chair1_email2>stefan.schulte@verwaltung.uni-marburg.de</chair1_email2>
  <chair1_ID>1966</chair1_ID>
  <sessionID>150</sessionID>
  <presentations>1</presentations>
  <p1_paperID>179</p1_paperID>
  <p1_contribution_type>Panel</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Mache, Beata
Trippel, Thorsten
Effinger, Maria
Gradl, Tobias
Haaf, Susanne
Hinrichs, Erhard
Horstmann, Wolfram
Müller, Lydia
Schrade, Torsten
Teich, Elke</p1_authors>
  <p1_organisations>Staats- und Universtitätsbibliothek Göttingen
Eberhard Karls Universität Tübingen, Deutschland
Universitätsbibliothek Heidelberg
Otto-Friedrich-Universität Bamberg
Berlin-Brandenburgische Akademie der Wissenschaften
Eberhard Karls Universität Tübingen, Deutschland
Staats- und Universtitätsbibliothek Göttingen
Universität Leipzig
Akademie der Wissenschaften und der Literatur Mainz
Universität des Saarlandes</p1_organisations>
  <p1_emails>mache@sub.uni-goettingen.de
thorsten.trippel@uni-tuebingen.de
effinger@ub.uni-heidelberg.de
tobias.gradl@uni-bamberg.de
haaf@bbaw.de
erhard.hinrichs@uni-tuebingen.de
horstmann@sub.uni-goettingen.de
lydia@informatik.uni-leipzig.de
torsten.schrade@adwmainz.de
e.teich@mx.uni-saarland.de</p1_emails>
  <p1_presenting_author>Trippel, Thorsten
Effinger, Maria
Gradl, Tobias
Haaf, Susanne
Müller, Lydia
Schrade, Torsten</p1_presenting_author>
  <p1_title>Gute Forschungsdaten, bessere Forschung: wie Forschung durch Forschungsdatenmanagement unterstützt wird</p1_title>
  <p1_abstract>&lt;p&gt;&lt;strong&gt;Kurzzusammenfassung&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In diesem Panel geht es um die Förderung der geisteswissenschaftlichen Forschung durch eine planvolle Erhebung, Archivierung, Veröffentlichung und die dadurch ermöglichte Nachnutzung von Forschungsdaten, die sowohl zur Qualitätssicherung in der Forschung beitragen als auch nicht zuletzt neue Fragestellungen erlauben. Aus unterschiedlichen Perspektiven soll in dem Panel beleuchtet werden, welchen Mehrwert das Datenmanagement für die Forschung in den digitalen Geisteswissenschaften hat, wie man diesen Mehrwert erreicht und auch die Veröffentlichung der Forschungsdaten als ein selbstverständliches Element der Dissemination der Forschungsergebnisse etabliert und wie man gleichzeitig den Aufwand für die Forschung abschätzen kann.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ziele des Panels&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Vor dem Hintergrund der notwendigen Diskussion zum Management, zur Interoperabilität, Sicherung, Publikation und Zitation aber auch zur Auswahl und Kassation von Forschungsdaten wird das Panel Beteiligte aus der Forschung, Entwicklung, aus wissenschaftsgeleiteten Forschungsinfrastrukturen sowie dem Bibliotheks- und Universitäts-Verlagswesen zusammenbringen. Im Mittelpunkt steht die kritische oder auch kontroverse Auseinandersetzung, welche Aufgaben Forschende selbst übernehmen können und wollen, welche Aufgaben bei digitalen Infrastrukturen anzusiedeln sind, welche gemeinsamen Strategien aller Mitwirkenden nötig sind, wo eine Parallelisierung von Entwicklungen vermieden werden muss und wo eine Vielfalt von Ansätzen wünschenswert wäre.&lt;/p&gt;
&lt;p&gt;Dazu werden die Teilnehmenden des Panels anhand von vier Leitfragen aus ihrer jeweiligen Perspektive Stellung zum Thema nehmen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leitfragen&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Welche spezifischen Anforderungen aus der geisteswissenschaftlichen Praxis beeinflussen das Datenmanagement?&lt;/li&gt;
&lt;li&gt;Wie beeinflusst die Datenveröffentlichung herkömmliche Publikationen und welche Datenmanagementoptionen sind für die Publikationen von Daten relevant?&lt;/li&gt;
&lt;li&gt;Wieweit können Forschungsdatenmanagementpläne die Forschenden dabei unterstützen, schon vom Moment der Formulierung der Forschungsfrage und der Beantragung eines Projektes an für die sichere forschungsbegleitende und forschungsabschließende Veröffentlichung der gesammelten, erhobenen und erstellten Daten - möglichst den FAIR-Prinzipien konform - zu sorgen?&lt;/li&gt;
&lt;li&gt;Wie bestimmen wir die technischen Kriterien für die Sicherung und Interoperabilität der heterogenen Forschungsdaten, damit sie gesichert, gefunden und nachgenutzt werden können? Reichen die Kriterien zur Abschätzung des Aufwands aus?&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Hintergrund&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Geisteswissenschaftliche Forschung basiert darauf, Ideen und Konzepte anderer Forschender zu betrachten, zu kommentieren und weiter zu entwickeln. Im Rahmen des Paradigmenwechsels (siehe etwa Berry, 2011; Baum and Stäcker 2015 ; Thiel, 2012), der sich gerade auch in den digitalen Geisteswissenschaften durch teilautomatische Analysen (siehe bereits Busa, R., 1951), Visualisierungen und Verknüpfungen unterschiedlicher Datentypen manifestiert, ist die Nachnutzung von Daten, Informationen und Konzepten von Anfang an ein integraler Bestandteil der Forschung gewesen (siehe etwa den Impact von Busa beschrieben von Winter, 1999; die Daten von Busa unter &lt;http://www.corpusthomisticum.org/&gt;; arbeiten von Antonio Zampolli, 1973; etc.)&lt;/p&gt;
&lt;p&gt;Die Veröffentlichung von Forschungsdaten selbst ist kein Novum - in Anhängen, in Tabellen, in Abbildungen wurden “Daten” analog als Merkmale der Wissenschaftlichkeit einer Publikation veröffentlicht, und selbst Busa (ebd.) referenziert existierende Konkordanzen und Indizes, die allerdings noch nicht elektronisch vorlagen. Die elektronische Publikation und Weitergabe ist auch nicht neu: Bereits in den 1990er Jahren fanden sich elektronische Beilagen auf CD-ROM in Verlagspublikationen und auch bei Hausarbeiten; Webseiten enthielten schon früh Forschungsdaten, die - zumindest während der Projektlaufzeit - diese auch für andere Forschende verfügbar machten.&lt;/p&gt;
&lt;p&gt;Die Publikation über öffentlich zugängliche Datenrepositorien ist dagegen recht neu, d.h. in zentralen Einrichtungen, die auch über Projektlaufzeiten hinaus Forschungsdaten vorhalten können, dafür aber eine Übergabe durch Datenbereitsteller nach bestimmten Qualitäts- und Beschreibungsrichtlinien verlangen. Dabei erlauben Repositorien, extrem große Mengen von (vernetzten) Daten zu veröffentlichen: alle erhobenen Daten können publiziert werden. Darunter können auch Daten sein, die für die eigene Forschungsfrage letztendlich nicht relevant waren, aber den Forschungsweg und die Ergebnisse nachvollziehbar machen, also sogenannte “null-results”. Innerhalb von Enhanced Publications können sie referenziert werden und z.B. mittels Linked Data als dynamisch vernetzende Daten aufeinander verweisen. Ihre Nachnutzung erlaubt es, Analysen, die in der Vergangenheit nur als Gedankenexperimente möglich waren, praktisch durchzuführen.&lt;/p&gt;
&lt;p&gt;Auch deswegen setzen Forschungsförderer zunehmend voraus, dass ein Konzept zur Nachnutzung der Daten, auf denen Forschungsergebnisse beruhen, zusammen mit Projektanträgen eingereicht wird, und dass zudem bereits existierende Daten und Werkzeuge gegebenenfalls nachgenutzt werden (siehe Senat der DFG, 2015; Deutsche Forschungsgemeinschaft, 2013; Deutsche Forschungsgemeinschaft, 2009; Deutsche Forschungsgemeinschaft, undatiert; H2020 Programme, 2013; Allianz der deutschen Wissenschaftsorganisationen, 2010). Gleichzeitig wird durch die Attribution der Datenaufbereitung die Leistung derjenigen sichtbar, die die Daten bereitgestellt haben, so dass die Veröffentlichung “infrastrukturbezogene[r] wissenschaftliche[r] Leistungen” und Forschungsdaten “in Qualifikationsverfahren ergänzend anerkannt werden” können (vgl. Wissenschaftsrat, 2011). Die Bereitstellung von Forschungsdaten folgt dabei den sogenannten FAIR-Prinzipien, ist Teil der sich entwickelnden Open-Science-Landschaft und wird etwa von wissenschaftsgeleiteten Forschungsinfrastrukturen wie CLARIN-D (siehe Hinrichs und Trippel, 2017) und DARIAH-DE (Gradl und Henrich, 2016) unterstützt.&lt;/p&gt;
&lt;p&gt;Auch wenn der Mehrwert des Forschungsdatenmanagements in diesem Bereich dokumentiert ist, als Anforderung formuliert wird und positiv konnotiert ist (siehe Bargheer u.a. 2017), erscheint es einigen Forschenden trotzdem eher als aufgezwungene, lästige Notwendigkeit denn als Mehrwert für die Forschung in den DH. Auch deshalb sollen in dem Panel die Vertreter der Forschungsinfrastrukturen und die WissenschaftlerInnen reflektieren, ob die bisher an die Forschenden herangetragenen Angebote ausreichen, um ihre Anforderungen zu erfüllen und ihr Vertrauen zu gewinnen, und, was die Forschungsinfrastukturen tun können, um hier weitere Fortschritte zu erzielen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Panel Format&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Das Panel besteht aus drei Teilen:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Kurze Impulsvorträge von ca. 5 Minuten von den Panel-Teilnehmenden unter Einnahme ihrer jeweiligen Perspektive,&lt;/li&gt;
&lt;li&gt;Diskussion der Leitfragen zwischen den Panelteilnehmenden moderiert aus der Anwenderperspektive&lt;/li&gt;
&lt;li&gt;Öffnung der Diskussion für die Zuhörenden durch die Moderation&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Panel-Teilnehmende:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Moderatorin&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Elke Teich (Saarbrücken): Als Sprecherin eines interdisziplinären SFB verfügt Prof. Teich über eine hervorragende Kenntnis der Erfordernisse und Hindernisse im Forschungsdatenmanagement in großen Forschungsverbünden. Sie wird das Panel aus der spezifischen Perspektive der Geisteswissenschaften moderieren.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Teilnehmende auf dem Panel und Impulsvorträge&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Lydia Müller (Mind Research Repository, Universität Leipzig): In den Kognitionswissenschaften ist die Verwaltung von Daten im Zusammenhang mit der Publikation von Artikeln zusammen mit den zugrundeliegenden Daten teilweise gelebte Praxis. Lydia Müller ist als Maintainerin des Mind Research Repository (http://openscience.uni-leipzig.de/) an einer Schlüsselstelle, da dieses Verzeichnis die Integration von Daten und Publikationen ermöglicht. Vor dem Hintergrund der dort gehosteten Daten und Publikationen kennt sie die Fallstricke bei der Integration von Daten und Publikationen und weiß, wie die Daten nachhaltig und gesichert abgelegt werden können und hat Erfahrungen damit, auf diese Daten zu verweisen. Sie steht damit für einen Aspekt der FAIR-Prinzipien für Forschungsdaten im Rahmen des Forschungsdatenzyklus in den Geisteswissenschaften: der Aufbewahrung und Zugänglichmachung von Daten.&lt;/li&gt;
&lt;li&gt;Susanne Haaf (BBAW) ist am Aufbau und Betrieb des Deutschen Textarchivs (DTA) beteiligt und hat die Spezifikation des standardisierten DTA-Basisformats maßgeblich mit beeinflusst, das Eingang in die einschlägigen DFG-Richtlinien gefunden hat. Damit war sie an der Erstellung von geisteswissenschaftlichen Forschungsdaten selbst beteiligt und entwickelte aktiv Standards und Richtlinien, die für große Datenmengen eingesetzt wurden. Daneben vermittelte sie die zugrundeliegenden Workflows, Richtlinien und Technologien im Rahmen von Workshops und Lehrveranstaltungen innerhalb der DH-Nutzercommunity. Das DTA vertritt mehr als 2000 Nutzende, die zur Weiterentwicklung seines bislang einzigartigen Korpus interoperabler Forschungsdaten des historischen Deutschen für die Geisteswissenschaften beitragen.&lt;/li&gt;
&lt;li&gt;Torsten Schrade (Leiter der Digitalen Akademie der Akademie der Wissenschaften und der Literatur Mainz, Professor für Digital Humanities an der Hochschule Mainz): Torsten Schrade ist ausgewiesener Experte für Forschungsdatenmanagement und Webtechnologien für die geisteswissenschaftliche Grundlagenforschung, für historische Fachinformationssysteme, webbasierte Arbeitsumgebungen, digitale Editionen, Webservices für geisteswissenschaftliche Fachdaten und Semantic Web Anwendungen.&lt;/li&gt;
&lt;li&gt;Thorsten Trippel: Im Rahmen von CLARIN-D und dem europäischen Projekt Parthenos beschäftigt sich Thorsten Trippel mit der Erstellung und Umsetzung von Datenmanagementpläne für die geisteswissenschaftliche Forschung. Zusammen mit Antragstellern entwickelt er dazu Datenmanagementpläne, wie sie von Drittmittelgebern zunehmend gefordert sind, und begleitet die Projekte über alle Phasen der geisteswissenschaftlichen Forschung - vom Erstellen oder Auffinden von Forschungsdaten, über die Aufbereitung, Analyse, Bereitstellung bis zur Archivierung, um die FAIR-Prinzipien umzusetzen.&lt;/li&gt;
&lt;li&gt;Tobias Gradl: Im Rahmen seiner Forschung hat sich Tobias Gradl innerhalb von DARIAH mit der nachhaltigen Aufbereitung von Forschungsdaten beschäftigt, besonders mit Daten, die nicht standardkonform aufbereitet wurden und im Rahmen spezifischer Forschungsfragestellungen anfallen. Daher bearbeitet er Modelle, um Daten in einer nachhaltig erschlossenen Form bereitzustellen.&lt;/li&gt;
&lt;li&gt;Maria Effinger (UB Heidelberg): Im Rahmen des DFG-Projekts OA-Monos, des Universitätsverlags "Heidelberg University Publishing" und der Fachinformationsdienste "arthistoricum.net" und "Proyplaeum" hat Maria Effinger XML-basiertes Publizieren implementiert und in der Universitätsbibliothek in Heidelberg umgesetzt. Die Veröffentlichungsmöglichkeiten über universitätseigene Verlage nehmen für offene Publikationen und damit verbundene Daten eine wichtige Funktion ein, indem sie in einer engen Zusammenarbeit mit den Forschenden die prinzipielle Nachvollziehbarkeit und Überprüfbarkeit wissenschaftlicher Ergebnisse sichern. Diesen Aspekt wird Frau Effinger im Panel vertreten.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Alle Panelisten haben ihre Teilnahme zugesagt.&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>121</session_ID>
  <session_title>Kaffeepause</session_title>
  <session_start>2018-03-01 15:30</session_start>
  <session_end>2018-03-01 16:00</session_end>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>151</session_ID>
  <session_short>Panel_7a</session_short>
  <session_title>Digitale Edition III</session_title>
  <session_start>2018-03-01 16:00</session_start>
  <session_end>2018-03-01 17:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Baillot, Anne</chair1>
  <attendee_count>1</attendee_count>
  <chair1_name>Anne Baillot</chair1_name>
  <chair1_organisation>Le Mans Université</chair1_organisation>
  <chair1_email>anne.baillot@gmail.com</chair1_email>
  <chair1_email2>anne.baillot@univ-lemans.fr</chair1_email2>
  <chair1_ID>1009</chair1_ID>
  <sessionID>151</sessionID>
  <presentations>1</presentations>
  <p1_paperID>249</p1_paperID>
  <p1_contribution_type>Panel</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Stadler, Peter
Kepper, Johannes
Capelle, Irmlind
Oberhoff, Andreas</p1_authors>
  <p1_organisations>Universität Paderborn
Universität Paderborn
Universität Paderborn
Universität Paderborn</p1_organisations>
  <p1_emails>stadler@weber-gesamtausgabe.de
kepper@edirom.de
irmlind.capelle@uni-paderborn.de
oberhoff@upb.de</p1_emails>
  <p1_presenting_author>Stadler, Peter
Kepper, Johannes
Capelle, Irmlind
Oberhoff, Andreas</p1_presenting_author>
  <p1_title>„Storied Collections“? Ein kritischer Blick auf die Arbeit an digitalen (Musik)-Editionen</p1_title>
  <p1_abstract>&lt;p&gt;Der Aufbau von digitalen (Musik)-Editionen und den entsprechenden Online-Publikationen hat sich im wissenschaftlichen Umfeld etabliert und damit Fakten und Muster geschaffen, die nicht notwendigerweise mit den zeitlich parallel entwickelten Theorien kongruent gehen. Das mag zum einen daran liegen, dass die zeitgenössische Theoriebildung selbst nicht einheitlich ist – man vergleiche nur die Idee einer „sozialen Edition“ (Siemens 2011) mit der Debatte um „documentary editing“ (Robinson 2013; Gabler 2010; Pierazzo 2011) oder mit einem „multiplen Textbegriff“ als Grundlage der Edition (Sahle 2013) – zum anderen aber auch an den ganz praktischen Rahmenbedingungen der allermeist als drittmittelgeförderten, zeitlich begrenzten Projekte. &lt;br /&gt;Es gilt daher, kritisch rückzublicken und zu reflektieren, in welcher Weise die Art der digitalen Erschließung die Erkenntnismöglichkeiten des Nutzers steuert, und ob sich die mit solchen Webpublikationen häufig verbundenen Ideen von "Offenheit", "Erweiterbarkeit", "Vernetzung" und "Nutzerbeteiligung" in der täglichen Arbeit der Projekte überhaupt realisieren lassen – oder ob diese Ideen nicht sogar teilweise auf zu sehr simplifizierten Voraussetzungen beruhen?&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>152</session_ID>
  <session_short>Panel_7b</session_short>
  <session_title>Der sehende Computer IV</session_title>
  <session_start>2018-03-01 16:00</session_start>
  <session_end>2018-03-01 17:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Rehbein, Malte</chair1>
  <attendee_count>6</attendee_count>
  <chair1_name>Malte Rehbein</chair1_name>
  <chair1_organisation>Universität Passau</chair1_organisation>
  <chair1_email>malte.rehbein@uni-passau.de</chair1_email>
  <chair1_ID>1013</chair1_ID>
  <sessionID>152</sessionID>
  <presentations>1</presentations>
  <p1_paperID>271</p1_paperID>
  <p1_contribution_type>Panel</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Donig, Simon
Handschuh, Siegfried
Radisch, Erik
Rehbein, Malte
Hastik, Canan
Kohle, Hubertus
Ommer, Björn</p1_authors>
  <p1_organisations>Universität Passau, Deutschland
Universität Passau, Deutschland
Universität Passau, Deutschland
Universität Passau, Deutschland
Technische Universität Darmstadt, Deutschland
Ludwig-Maximilians-Universität München, Deutschland
Ruprecht-Karls-Universität Heidelberg, Deutschland</p1_organisations>
  <p1_emails>simon.donig@uni-passau.de
Siegfried.Handschuh@uni-passau.de
erik.radisch@uni-passau.de
malte.rehbein@uni-passau.de
hastik@linglit.tu-darmstadt.de
hubertus.kohle@gmail.com
ommer@uni-heidelberg.de</p1_emails>
  <p1_presenting_author>Handschuh, Siegfried
Rehbein, Malte
Hastik, Canan
Kohle, Hubertus
Ommer, Björn</p1_presenting_author>
  <p1_title> Der ferne Blick. Bildkorpora und Computer Vision in den Geistes- und Kulturwissenschaften - Stand - Visionen - Implikationen</p1_title>
  <p1_abstract>&lt;p&gt;Der ferne Blick. Bildkorpora und Computer Vision in den Geistes- und Kulturwissenschaften - Stand - Visionen - Implikationen&lt;/p&gt;
&lt;p&gt;Ausgangslage&lt;/p&gt;
&lt;p&gt;Getrieben von einer fortschreitenden Computerisierung ihrer wissenschaftlichen Methodik und einer wachsenden Verfügbarkeit von digitalen Bildwerken ist in den letzten Dekaden in zahlreichen Disziplinen der Geistes- und Kulturwissenschaften ein erneuertes Interesse am Bild als Forschungsgegenstand und Erkenntnisinstrument zu beobachten.&lt;/p&gt;
&lt;p&gt;So begreift sich etwa die (digitale) Kunstgeschichte in der Erweiterung explizit als digitale Bildwissenschaft (Kohle 2013), hat die Geschichtswissenschaft als “Visual History” das lange vernachlässigte Medium Bild als Quellengattung neben dem Text wiederentdeckt (Paul 2014), erfindet sich die Architekturgeschichte nicht zuletzt mit digitalen Instrumenten als Disziplin neu und haben sich junge Fächer wie die Material-Culture-Studies oder die Designgeschichte etabliert.&lt;/p&gt;
&lt;p&gt;Alle diese Disziplinen begegnen sich in den Digital Humanities als einem geteilten Wissensraum, dessen Erkundung ganz wesentlich von den Möglichkeiten der Informatik mitbestimmt wird.&lt;/p&gt;
&lt;p&gt;Die automatisierte Erschließung großer und größter Bildkorpora im Sinne eines effektiven Information Retrieval prägt dabei nach Jeffrey Schnapp die erste Welle der Digitalen Revolution (Schnapp 2011). Die zweite Welle - so wiederum Schnapp - müsse nun “qualitativ, interpretativ, erfahrungsbewusst, intuitiv erfassbar und schöpferisch” sein, um digitale Instrumente in den Dienst der Kernkompetenzen der Geistes- und Kulturwissenschaften stellen zu können: Achtsamkeit gegenüber Komplexität, Medienspezifik, historischem Kontext, analytischer Tiefe, Kritik und Interpretation.&lt;/p&gt;
&lt;p&gt;Ziele des Panels&lt;/p&gt;
&lt;p&gt;Mit dem hier vorgeschlagenen Panel verorten wir den Stand der digitalen Bildforschung in dieser idealtypischen Abfolge. Forschende aus unterschiedlichen Disziplinen beleuchten den State-of-the-art im Bereich automatisierter Verfahren der Computer Vision sowie gegenwärtige Wege und Visionen ihrer zukünftigen Anwendung in den Geistes- und Kulturwissenschaften. Wir erkunden Herausforderungen, die neue Werkzeuge wie die automatisierte Bildanalyse an Instrumentenkritik und Methodentransparenz stellen und tragen zur Konkretisierung von Best Practices in diesem Bereich bei.&lt;/p&gt;
&lt;p&gt;Gegenstand Verfahren, Praktiken und Projekte&lt;/p&gt;
&lt;p&gt;Von Merkmalserkennung (Pattern Matching) bis hin zu Ansätzen aus dem Bereich des maschinellen Lernens wie etwa Deep Learning haben Verfahren der Computer Vision in den letzten Jahren das Potential entwickelt, sich als disruptive Technologie in den Geistes- und Kulturwissenschaften zu erweisen. Dieser Umstand verdankt sich unter anderem raschen Fortschritten bei der verfügbaren Rechenleistung, aber auch einem zunehmend vereinfachten Zugang zu geeigneter Software sowie einer wachsenden Verfügbarkeit digitaler Bilddaten.&lt;/p&gt;
&lt;p&gt;Im Rahmen des Panels fragen wir zunächst nach den sich rasch entwickelnden Instrumenten und werfen einen Blick auf Bereiche und Projekte in denen diese zur Anwendung kommen. Wir wollen erkunden, welche Möglichkeiten computerbasierte automatisierte Verfahren etwa der Objekt- und Merkmalserkennung für die Digitalen Geisteswissenschaften eröffnen (zu einer einführenden Diskussion transdisziplinärer Potentiale von Computer Vision und Kunstgeschichte vgl. (Bell &amp; Ommer (2015) sowie dies. (2016)).&lt;/p&gt;
&lt;p&gt;Zwischen datengetriebenen und interpretativ hermeneutischen Zugängen&lt;/p&gt;
&lt;p&gt;Wir explorieren, ob und wie die anfangs postulierte Dichotomie von datengetriebenen und interpretativ-hermeneutischen Zugängen zur Generierung von Wissen (Drucker 2011) fortbesteht oder einem differenzierteren Modell Raum gegeben hat. Inwieweit lässt sich der am „Close Viewing“ geschulte Methodenapparat direkt auf Verfahren eines „Distant-Viewing“ (oder „Distant-Watching“ im Fall von Bewegtbildern) übertragen, oder inwieweit wird hier eine Wissenschaftstheorie des Digitalen noch zu entwickeln sein? Dies schließt die Frage einer adäquaten Instrumentenkritik ebenso ein, wie die nach Selektionsverfahren, Korpusbildung oder den Auswirkungen der Freigabepraxis von Bildmaterial durch Kulturinstinstitutionen (GLAM).&lt;/p&gt;
&lt;p&gt;Die semantische Lücke, Annotation, Multimodalität&lt;/p&gt;
&lt;p&gt;Die Analyse von “Big Image Data” auch im Bereich der Forschung wirft die Frage auf, wie Forschungsansätze in den letzten Jahren versucht haben, die “semantische Lücke” (Semantic Gap) im Umgang mit diesem Datenmaterial zu schließen.&lt;/p&gt;
&lt;p&gt;Welche Rolle kommt etwa Normdaten (GND), Thesauri (ICONCLASS, AAT oder ULAN) oder Ontologien, die spezifisch für die Dokumentation kultureller Artefakte entwickelt wurden (CIDOC-CRM) bzw. Domäneontologien wie Neoclassica (Donig, Christoforaki, Handschuh 2016) zu?&lt;/p&gt;
&lt;p&gt;Welche Rolle könnte alternativ die semantische Annotation (Oren, Möller, Scerri, Handschuh, &amp; Sintek 2006) durch Zugänge etwa aus dem Bereich der Gamification und des Crowdsourcing wie in ARTIGO (Wieser, Bry, Bérard, Lagrange 2013) bieten?&lt;/p&gt;
&lt;p&gt;Kann die Informatik hier mit neuen Verfahren zur Verknüpfung verschiedener digitaler Analyseansätze in multimodalen Artefakten – also etwa die Untersuchung der Verbindung von Bild und Text oder Bewegtbild und Ton – digitales Sehen ergänzen und verbessern? (Bruni, Tran, Baroni 2014), (Hiippala 2013).&lt;/p&gt;
&lt;p&gt;Methodentransparenz &amp; Best Practices&lt;/p&gt;
&lt;p&gt;In der Tat wirft ein digitales Sehen, analog zu kritischen Debatten um “Close” und “Distant Reading” (Bonfiglioli &amp; Nanni 2015) grundlegende epistemologische Fragen nach den Gemeinsamkeiten oder Unterschieden zwischen computergestützter Bildanalyse und in spezifischen Praktiken des Sehens geschulten Menschen auf. Erweitert etwa der ferne Blick nur die Geschwindigkeit und Menge dessen, was wir wahrnehmen können, oder verändert er unsere Wahrnehmung selbst?&lt;/p&gt;
&lt;p&gt;Wir erkunden wie digitale Instrumente beschaffen sein müssen, um den Erfordernissen der Geistes- und Kulturwissenschaften nach Methodentransparenz zu genügen. (Für eine grundsätzliche Diskussion der Konsequenzen des Designs von Klassifizierungs- und Rankingmechanismen sowie verschiedene Formen von Opacity siehe Burell 2016). Wie kann etwa der Black Box Charakter von Neuronalen Netzen sinnvoll akkommodiert werden? Nicht zuletzt stellt sich die Frage nach dem Zusammenspiel von Instrument und Forschungspraxis. Reproduziert etwa ein (in sich vollständig transparenter) Klassifikator, der aber auf der Basis eines bestimmten Korpus trainiert worden ist, nicht letztlich spezifische kulturelle Konzepte und konfirmiert damit einen bestehenden Kanon?&lt;/p&gt;
&lt;p&gt;Wir wollen uns diesen Fragen nicht verschließen, aber sie gleichermaßen rekontextualisieren wie über ihre forschungspraktischen Implikationen nachdenken. Wir werden dabei gerade mit jenen, die an der Weiterentwicklung dieser Instrumente arbeiten, diskutieren, wie Technologien gebaut und eingesetzt werden können, um ein emanzipatorisches und exploratives Potential zu entfalten.&lt;/p&gt;
&lt;p&gt;In diesem Sinn wollen wir abschließend Ideen für Best Practices im Sinne eines methodisch angeleiteten Umgangs mit großen Bildkorpora zusammentragen, indem wir etwa nach der Rolle und Modellierbarkeit von Kontextualisierung oder der Rolle von Ontologien und Normdaten dabei fragen.&lt;/p&gt;
&lt;p&gt;Impulsvorträge&lt;/p&gt;
&lt;p&gt;Siegfried Handschuh, Universität Passau: Bild, mutimodaler Verbund und Automatisierung&lt;/p&gt;
&lt;p&gt;Bildanalyse stellt eine besondere Herausforderung an die Fähigkeit von computerbasierten Verfahren Wissen zu generieren. Dass Bilder vollkommen ohne einen Bezug zu anderen Modi – hier besonders Text – vorkommen ist eher eine Ausnahme als die Regel. Der multimodale Verbund von Bild und Text kann so durch Verfahren aus dem Bereich des maschinellen Lernens ausgewertet werden, um Wissen zu erzeugen, das einen breiteren Kontext berücksichtigt. Das Impulsstatement thematisiert diesen Zusammenhang insbesondere eine Methodik für die Repräsentation, Annotation und (teil)automatisierte Entdeckung multimodaler Wissensbestände in großen digitalen Materialkorpora und fragt wie Techniken der Computer Vision gewinnbringend mit Verfahren aus dem Bereich der Verarbeitung Natürlicher Sprache zusammengebracht werden können.&lt;/p&gt;
&lt;p&gt;Canan Hastik, Technische Universität Darmstadt: &lt;br /&gt; Wie viel Semantik ist nötig und wie viel Automatisierung ist möglich?&lt;/p&gt;
&lt;p&gt;Computerbasierte Verfahren sind ein wichtiges Instrument, um die digitale Bildforschung effizienter zu gestalten und zu verstetigen. Es wäre jedoch ein großer Fehler, Verfahren des maschinellen Lernens lediglich dafür einzusetzen, bestehende theoretische und historische Kategorien zu definieren und die Automatisierung für Digital Humanities Experten voranzutreiben. Die Herausforderung besteht darin, den DH Forscher bei der Kuration und Entwicklung von aussagekräftigen, repräsentativen und authentischen Forschungskorpora zu unterstützen. Ontologien sind dabei elementar für die Semantifizierung von großen bildbasierten Korpora und unterstützen zudem die kollaborative Entwicklung und die Verknüpfung von Wissensbeständen. Automatisierte Verfahren gilt es insbesondere dafür einzusetzen, zeitgenössische Kultur neu zu sehen und zu interpretieren.&lt;/p&gt;
&lt;p&gt;Hubertus Kohle, Ludwig-Maximilians-Universität München: Ähnlichkeitsbestimmung in der digitalen Kunstgeschichte&lt;/p&gt;
&lt;p&gt;Das Statement widmet sich aus einer eher geisteswissenschaftlichen Perspektive heraus der digital gestützten Ähnlichkeitsbestimmung. Ähnlichkeit wird hier verstanden als Erkenntnismotor, der insbesondere in der Kunstgeschichte seinen Ort hat, die schon immer mit dem vergleichenden Sehen eine Methode des Erkenntisgewinns besaß. Abzuwägen wird sein, ob hierfür eher eine Metadatenanalyse oder die direkte Bildadressierung oder eine Mischung aus beidem zielführend ist.&lt;/p&gt;
&lt;p&gt;Björn Ommer, HCI/IWR Ruprecht-Karls-Universität Heidelberg:&lt;/p&gt;
&lt;p&gt;Aktuelle Entwicklungen im Bereich des Maschinellen Lernens und der Computer Vision haben Algorithmen hervorgebracht, die Visual Retrieval mit einer zuvor ungeahnten Performanz ermöglichen. Dadurch ergeben sich für die digitalen Bildwissenschaften ganz neue Möglichkeiten große Korpora zu erschließen und zu analysieren. Allerdings bringen diese neuen informatischen Verfahren auch ihre ganz eigenen Beschränkungen mit sich, die insbesondere bei ihrer Rekontextualisierung in den Geisteswissenschaften zutage treten. Dieser Vortrag wird das Potential einer interdisziplinären Kooperation von Informationswissenschaften und den Geisteswissenschaften diskutieren, grundlegende Beschränkungen beleuchten und mögliche Auswege präsentieren.&lt;/p&gt;
&lt;p&gt;Malte Rehbein, Universität Passau:&lt;br /&gt; Moderation&lt;/p&gt;
</p1_abstract>
 </session>

 <session>
  <session_ID>153</session_ID>
  <session_short>Panel_7c</session_short>
  <session_title>Wissenschaftsorganisation IV</session_title>
  <session_start>2018-03-01 16:00</session_start>
  <session_end>2018-03-01 17:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Wuttke, Ulrike</chair1>
  <chair2>Wettlaufer, Jörg</chair2>
  <attendee_count>6</attendee_count>
  <chair1_name>Ulrike Wuttke</chair1_name>
  <chair1_organisation>FH Potsdam</chair1_organisation>
  <chair1_email>ulrike.wuttke@gmx.net</chair1_email>
  <chair1_email2>wuttke@fh-potsdam.de</chair1_email2>
  <chair1_ID>1280</chair1_ID>
  <chair2_name>Jörg Wettlaufer</chair2_name>
  <chair2_organisation>Georg-August-Universität Göttingen</chair2_organisation>
  <chair2_email>jwettla@gwdg.de</chair2_email>
  <chair2_ID>1176</chair2_ID>
  <sessionID>153</sessionID>
  <presentations>1</presentations>
  <p1_paperID>182</p1_paperID>
  <p1_contribution_type>Panel</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Moeller, Katrin
Ďurčo, Matej
Lemaire, Marina
Rosenthaler, Lukas
Sahle, Patrick
Wuttke, Ulrike
Wettlaufer, Jörg</p1_authors>
  <p1_organisations>Historisches Datenzentrum Sachsen-Anhalt, Martin-Luther-Universität Halle-Wittenberg, Deutschland
Austrian Center for Digital Humanities, Österreichische Akademie der Wissenschaften, Österreich
Servicezentrum eSciences, Universität Trier, Deutschland
and Service Center for the Humanities DaSCH, Universität Basel (DHLab) und Schweizerische Akademie der Geistes- und Sozialwissenschaften, Schweiz
Data Center for the Humanities (DCH), Universität zu Köln, Deutschland
Stellvertretende Vorsitzende der AG Datenzentren des DHd, Fachbereich Informationswissenschaften, Fachhochschule Potsdam, Deutschland
Göttingen Centre for Digital Humanities, Georg-August Universität Göttingen, Deutschland</p1_organisations>
  <p1_emails>katrin.moeller@geschichte.uni-halle.de
matej.durco@oeaw.ac.at
esciences@uni-trier.de
lukas.rosenthaler@unibas.ch
sahle@uni-koeln.de
Ulrike.Wuttke@gmx.net
jwettla@gwdg.de</p1_emails>
  <p1_presenting_author>Moeller, Katrin
Ďurčo, Matej
Lemaire, Marina
Rosenthaler, Lukas
Sahle, Patrick
Wuttke, Ulrike
Wettlaufer, Jörg</p1_presenting_author>
  <p1_title>Die Summe geisteswissenschaftlicher Methoden? Fachspezifisches Datenmanagement als Voraussetzung zukunftsorientierten Forschens</p1_title>
  <p1_abstract>&lt;p&gt;&lt;strong&gt;Zusammenfassung des Vorhabens: &lt;/strong&gt;Die AG Datenzentren des DHd möchte ein Panel mit insgesamt sechs Diskussionspartnern veranstalten, um die Herausbildung fachbezogenen Datenmanagements im deutschsprachigen Raum in den digitalen Geisteswissenschaften weiter zu fördern sowie den Stand und die Perspektiven dieses Forschungsbereichs zu diskutieren. Das Panel reiht sich damit in den fachlich übergreifenden Organisationsprozess zum Datenmanagement ein. Im Mittelpunkt der Diskussion soll hier die Veränderung der Datenkultur in der Geisteswissenschaft stehen und damit die Frage, wie eine Konsolidierung, Vernetzung und fachliche Abrundung von Diensten die Anreize zur wissenschaftlichen Nutzung des Datenmanagements erhöhen können. Dies soll mit zwei Schwerpunktsetzungen erfolgen:&lt;/p&gt;
&lt;p&gt;A. Zunächst möchten wir anhand eines Überblicks zu bestehenden Angeboten von Datenzentren informieren und aufzeigen, welche fachbezogenen Leistungen momentan oder in naher Zukunft von Datenzentren in einem Netzwerk verteilter DH-Strukturen eingebracht werden. Dieser Block macht die Schwerpunktsetzungen, Rollen und herausgehobene fachliche Dienste einzelner Datenzentren sichtbar, die über allgemeine Anforderungen an ein Datenzentrum hinausragen. Daran anschließend kann über die mögliche Ausgestaltung kooperativer Strukturen und die Zusammenarbeit innerhalb eines Netzwerkes der DH-Datenzentren diskutiert werden.&lt;/p&gt;
&lt;p&gt;B. Weiterhin fragt das Panel grundlegend nach zentralen Aufgabenstellungen im Bereich von  Standards aus den einzelnen Fachgebieten der Geisteswissenschaften: Welche fachlichen Standards müssen Datenzentren über die zuvor gezeigten Ansätze hinaus kooperativ entwickeln, um passfähige Daten für eine möglichst große Breite von Nachnutzungen interdisziplinär abzusichern? Wo liegen Möglichkeiten und wo Grenzen von fachlicher Standardisierung und Normalisierung über die einzelnen Fachdisziplinen der Geisteswissenschaft hinweg? Hierzu sollen Erkenntnisse und Erfahrungen aus abgeschlossenen bzw. weitgehend realisierten Projekten dazu dienen, abstrahierende Anforderungen aus fachwissenschaftlicher Perspektive zu benennen, die zugleich einen Mehrwert der Realisierung besitzen. Insgesamt geht es darum, die Notwendigkeit eines geisteswissenschaftlichen sowie fachspezifischen Angebots nachdrücklich unter Beweis zu stellen, zu etablierende Standards zu benennen und Desiderate zu erkennen. &lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1. Forschungsstand&lt;/p&gt;
&lt;p&gt;Der digitale Wandel, das Entstehen, Archivieren, Erschließen und Nachnutzen großer Datenbestände verändert wissenschaftliche Forschung in ihren Grundlagen, Arbeitsweisen und Methodiken fundamental (OECD 2007; HRK 2014; RfII 2016; Allianz 2010; DFG 2015). Die zahlreichen Aspekte dieses Wandels beschäftigen Fach-Communities und Fachverbände in umfassender Weise mit Fragen des Datenmanagements, der Lizenzen, Rechte und des Datenschutzes, der technischen und inhaltlichen Langzeitarchivierung sowie Nachnutzung von Daten. Die intensiven Diskussionen um das Datenmanagement sind verbunden mit einem grundlegenden Nachdenken über künftige Aufgaben, Strukturen und Workflows bestehender und sich neu formierender Institutionen oder Akteure und führen zu grundsätzlichen Reflexionen über die notwendige Schaffung verteilter nationaler infrastruktureller Angebote (RfII 2016: 39f.; RfII 2017). In der jüngeren Vergangenheit sind bevorzugt technische Aspekte des Datenmanagements diskutiert worden, wie sie etwa die interoperationale Metadatenhaltung (Bibliotheksparadigma), Probleme der Speicherformate, Speichermedien und Dokumentation in der technischen Langzeitarchivierung über einen Rahmen von zehn Jahren hinaus (Archivparadigma) betreffen (Nestor 2016). Solche Lösungen und Angebote können oft in übergreifenden technischen Strategien gefunden werden, die ein zentrales Aufgabengebiet von Bibliotheken und Archiven umfassen. Wie in den Arbeitsbereichen der digitalen Wissenschaft allgegenwärtig zu beobachten ist, lässt sich dabei eine sukzessive, aber dennoch intensive Spezialisierung und Differenzierung digitaler Erfordernisse und Techniken beobachten. War beispielsweise noch das erste große deutschsprachige Überblickswerk der nestor-Arbeitsgruppe zur Langzeitarchivierung fachübergreifend  technisch geprägt (Neuroth 2010), offenbaren die neueren Arbeitsergebnisse eine deutliche Hinwendung zur fachbezogenen Darstellung (Neuroth 2012). Allerdings blieben auch in dieser Hinsicht die Parameter der technischen Langzeitarchivierung aus der Sicht der Archive und Bibliotheken bestimmend.&lt;/p&gt;
&lt;p&gt;Gleiches lässt sich mit Blick auf die Wissenschaft erkennen, wenn man etwa auf die sich konstituierenden Fachgruppen im Rahmen des Verbandes der Digital Humanities im deutschsprachigen Raum&lt;sup&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/sup&gt; oder andere traditionelle Fachverbände mit ihren sich ausdifferenzierenden digitalen Arbeitsweisen/Methoden schaut.&lt;sup&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/sup&gt; Intensiv diskutiert wurden solche Themen der fachbezogenen Binnendifferenzierung auch in der Auftaktveranstaltung zum Förderprogramm "'Mixed Methods' in den Geisteswissenschaften?" der VW-Stiftung im Frühsommer 2017, die eine Vielzahl deutschsprachiger Forschungsprojekte im Bereich der Digital Humanities versammelte.&lt;sup&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/sup&gt; &lt;/p&gt;
&lt;p&gt;Um nachhaltig wirksam zu sein, braucht Forschungsdatenmanagement eine enge Anbindung an die Wissenschaft mit ihren spezifischen Problemen und fachlichen Erfordernissen (RfII 2016: 60; RfII 2017: 1f., AG DZ 2017: 3). Nicht in erster Linie der technische Service zur Langzeitarchivierung standardisierter Daten, sondern die fachbezogene Datenkuration und die Tiefenerschließung auf der Basis von Standards sichert die Güte und Qualität von Forschungsdaten und ihre Nachnutzbarkeit für spezifische wissenschaftliche Forschungsprojekte. Qualitätsgerechtes Forschungsdatenmanagement - das auch im analogen Zeitalter den Kern jeder Forschung ausmachte - sichert den wissenschaftlichen Mehrwert von Daten, schafft vielfältige Nutzungsanreize und innerwissenschaftliche Akzeptanz (RfII 2016: 38-40; AG DZ 2017: 2). Im gleichen Maße wie daher die übergreifende Basis technischer Lösungen zur Metadatenhaltung, Lizenzierung und Langzeitarchivierung entwickelt werden muss, sind fachbezogene Standards der Datenkuration, der kontrollierten Vokabularien, der fachbasierten Quellenkritik und Annotation sowie spezialisierte Werkzeuge der fachlichen Datenverarbeitung zur Erzeugung nachhaltiger Datenstrukturen notwendig. Während erstere eher in den Verantwortungsbereich der Bibliotheken und Archive fallen, sind letztere das zentrale Aufgabenfeld der Wissenschaft mit ihren spezifischen Fachgebieten.&lt;/p&gt;
&lt;p&gt;Hinzu treten die zahlreichen Herausforderungen des geisteswissenschaftlichen Datenmanagements im Unterschied zu natur- und sozialwissenschaftlichen Projekten. Geisteswissenschaftliche Daten bilden in vielen Anwendungsbereichen komplexe Textkorpora mit hochspezialisierten Formen der Transkription, Annotation und der Unterscheidung verschiedener Ebenen von Quellenbegriff, Lemmatisierung, Normalisierung, Standardisierung und Begriffserklärung. Ähnliche komplexe Formen der Erschließung gelten ebenso in eher auf Objekte und Gegenstände bezogene Geisteswissenschaften. Ansätze der einen Diziplin sind (bis heute) nicht ohne weiteres auf eine andere übertragbar bzw. nicht fachübergreifend bekannt. Gleichzeitig ist es Anliegen von Datenzentren übergreifende Standards bzw. Austauschformate zu definieren und zu etablieren, welche die Transparenz und Verschneidung von Daten überhaupt erst einmal ermöglichen.&lt;/p&gt;
&lt;p&gt;Ebenso muss auf der Ebene der Datenzentren intensiv über Kooperationsformen und Vorgehensweisen für neue heterogene Datenbestände mit einem Mix aus Quellen, Daten, Methoden, Medien und Rechten nachgedacht werden (AG DZ 2017: 3).  &lt;/p&gt;
&lt;p&gt;2. Fragestellung  und Aufbau des Panels&lt;/p&gt;
&lt;p&gt;Die DH ist angetreten, um computergestützte Ansätze und Methoden quer über die geisteswissenschaftlichen Fachdisziplinen zu entwickeln. Was jedoch macht die Summe geisteswissenschaftlicher Bedürfnisse aus, um Daten tatsächlich interdisziplinär und langfristig nachnutzbar zu machen? Die "Kritik der digitalen Vernunft" besteht für ein solches geisteswissenschaftliches Unterfangen darin, gemeinsame Anforderungen und Standards der Datenhaltung aus bestehenden Projekten und Erfahrungen zu formulieren, zusammenzutragen, zu systematisieren und zu abstrahieren. Zudem bedürfen solche Standards der Akzeptanz durch Forschende, sollen sie Wirksamkeit entfalten. Standards müssen daher einen klaren Mehrwert für wissenschaftliche Arbeit bieten.&lt;/p&gt;
&lt;p&gt;A. Viele Datenzentren haben mittlerweile ihre reguläre fachliche Arbeit aufgenommen,  projektbasiert Erfahrungen gesammelt und neben der regulären Arbeit oft spezifische Schwerpunkte gebildet. In Form einer Präsentation der wichtigsten Akteure, Dienste und Angebote möchten wir über das bestehende Leistungsangebot innerhalb der AG Datenzentren des DHd-Verbandes informieren, wobei bevorzugt die Schwerpunktsetzungen im Bereich der Standards thematisiert werden. Auf diese Weise können sich Tagungsteilnehmer einen Überblick zum Dienstleistungsspektrum von Datenzentren verschaffen, was angesichts der insgesamt unübersichtlichen (weil sich dynamisch entwickelnden) Forschungslandschaft einen erheblichen Vorteil darstellt. Diese ca. 15-20 Minuten dauernde Präsentation beruht auf einer Umfrage unter den im Verband organisierten Einrichtungen und Akteuren, die von der Organisatorin des Panels in Zusammenarbeit mit der AG Datenzentren vorstrukturiert und präsentiert wird. Bei Bedarf können weitere  zentrale fachwissenschaftliche Angebote in den Geisteswissenschaften einbezogen werden. Dieser Leistungskatalog ist einerseits Grundlage der Diskussion mit dem Publikum und andererseits für die Außendarstellung der AG Datenzentren der DHd förderlich.&lt;/p&gt;
&lt;p&gt;B. Nach dieser Präsentation leitet die Moderation in den Diskussionsteil über. Eingeladen werden Vertreter und Vertreterinnen geisteswissenschaftlicher Datenzentren mit unterschiedlichen Schwerpunktsetzungen und Funktionen, die Impulsreferate zu notwendigen Anforderungen fachwissenschaftlicher Standards im Datenmanagement zu geben. Diese Statements werden vor der Tagung allen Diskussionspartnern zur Verfügung gestellt, um eine angeregte Diskussion über fachliche Belange und Standards für das Datenmanagement der Geisteswissenschaften zu ermöglichen und auch Kritik zu formulieren. Die Panelorganisatorin und Moderatoren sichten die Beiträge zuvor redaktionell-moderierend. Hier sollen vor allem Möglichkeiten und Grenzen von bestehenden innerfachlichen Standards (z.B. existierende Ansätze wie GND, XML, TEI, Transkriptionsregeln; neue Entwicklungen wie z.B. "Ontologie historischer Berufe” etc.) benannt und diskutiert werden. An Einzelbeispielen soll kontrovers diskutiert werden, welche Standards die Geisteswissenschaft und das Datenmanagement tatsächlich voranbringen. Sind es eher technische Aspekte und Rahmenbedingungen (Tools, Formate, Annotationswerkzeuge), die formale Aspekte des Datenmanagements vereinheitlichen oder thematisch übergreifende Normansätze, Ontologien oder kontrollierte Vokabulare, die häufig aber sehr voraussetzungsvoll und arbeitsintensiv sind? Zielgerichtet sollen so  Ansätze, Möglichkeiten und Grenzen diskutiert werden. Fachwissenschaftliche  Aspekte sollen zusätzlich durch die Öffnung der Diskussion für das Publikum einbezogen werden. Gleichzeitig kann das Publikum  Hindernisse und Desiderate des Forschungsdatenmanagements formulieren. Insgesamt sollen zudem mögliche Kooperationen und Organisationsstrukturen zwischen verschiedenen Datenzentren angerissen werden (Wer darf Standards entwickeln? Wer muss sie nutzen?). Nach Möglichkeiten werden dazu Beiträge des Publikums bzw. die Use Cases der Diskussion aufgegriffen. Diese Diskussion soll die fachliche Positionierung und Ausdifferenzierung der Datenzentren schärfen.&lt;/p&gt;
&lt;p&gt;Es ist die Aufgabe der Moderation, entsprechend der Schwerpunktsetzungen in der Diskussion abstrahierende Aussagen zur Ansetzung von Standards zu finden, die aber zuvor innerhalb der AG und auch der eingeladenen Diskutanten bereits andiskutiert wurden, um ein konzentriertes und fachbezogen-klares Fazit der Diskussion zu ermöglichen.&lt;/p&gt;
&lt;br clear="all" /&gt;
&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/sup&gt; Siehe die Arbeitsgruppen des Verbands DHd: https://dig-hum.de/dhd-ags.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/sup&gt; Vergleiche hier etwa das CfP und die daraus resultierenden Ergebnisse der Tagung: Quellen und Methoden der Geschichtswissenschaft im digitalen Zeitalter. Neue Zugänge für eine etablierte Disziplin, DIGIMET 2017, 25./26.09.2017 in Berlin, URL: http://welt-der-kinder.gei.de/wp-content/uploads/2017/05/CfP-DH-Abschlusstagung-WdK-02.05.2017.pdf.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/sup&gt; Dabei nahmen zahlreiche Vertreter der DH-Community in den Geisteswissenschaften teil, die auch über das eigentliche Förderprogramm "Mixed Methods" in den Geisteswissenschaften Projekte durchführen: https://www.volkswagenstiftung.de/fileadmin/downloads/publikationen/20161221_Mixed_Methods_VolkswagenStiftung_Bewilligungen.pdf.&lt;/p&gt;

</p1_abstract>
 </session>

 <session>
  <session_ID>185</session_ID>
  <session_title>Posterslam</session_title>
  <session_start>2018-03-01 17:45</session_start>
  <session_end>2018-03-01 18:45</session_end>
  <session_room_ID>1</session_room_ID>
  <session_room>Hörsaal B, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Baillot, Anne</chair1>
  <chair2>Hermes, Jürgen</chair2>
  <attendee_count>0</attendee_count>
  <chair1_name>Anne Baillot</chair1_name>
  <chair1_organisation>Le Mans Université</chair1_organisation>
  <chair1_email>anne.baillot@gmail.com</chair1_email>
  <chair1_email2>anne.baillot@univ-lemans.fr</chair1_email2>
  <chair1_ID>1009</chair1_ID>
  <chair2_name>Jürgen Hermes</chair2_name>
  <chair2_organisation>Universität zu Köln</chair2_organisation>
  <chair2_email>hermesj@uni-koeln.de</chair2_email>
  <chair2_ID>1828</chair2_ID>
 </session>

 <session>
  <session_ID>60</session_ID>
  <session_short>Poster_1</session_short>
  <session_title>Postersession</session_title>
  <session_start>2018-03-01 18:45</session_start>
  <session_end>2018-03-01 19:45</session_end>
  <session_room_ID>20</session_room_ID>
  <session_room>Hörsaalgebäude</session_room>
  <attendee_count>12</attendee_count>
  <sessionID>60</sessionID>
  <presentations>67</presentations>
  <p1_paperID>277</p1_paperID>
  <p1_contribution_type>Poster</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Dumont, Stefan</p1_authors>
  <p1_organisations>Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland</p1_organisations>
  <p1_emails>dumont@bbaw.de</p1_emails>
  <p1_presenting_author>Dumont, Stefan</p1_presenting_author>
  <p1_title>Ein Brief – zwei Perspektiven. Stellenkommentare in digitalen Briefeditionen über APIs austauschen</p1_title>
  <p1_abstract>&lt;p&gt;Vernetzung von Texten und anderen Ressourcen war und ist ein großes Versprechen des digitalen Zeitalters im Allgemeinen, aber auch der digitalen Editionsphilologie im Besonderen. Während man um 2000 darunter noch vorwiegend das manuelle Setzen von einzelnen Links verstand, wurde spätestens seit Beginn dieses Jahrzehnts die automatisierte Verknüpfung in den Blick genommen. So wurde in Portalen, Websites und auch digitalen Editionen (vornehmlich des deutschsprachigen Raums) die automatisierte Verlinkung von Personenregistereinträgen eingeführt, die mit Hilfe der Gemeinsamen Normdatei und von BEACON-Schnittstellen ermöglicht wird. Diese Vernetzung auf Basis von BEACON-Schnittstellen gehört heute zum Standard digitaler Editionen und ist nicht mehr wegzudenken (Stadler 2012).&lt;/p&gt;
&lt;p&gt;Mit den Fortschritten im Internet allgemein wuchs aber seit einigen Jahren der Wunsch, digitale Editionen über solche basalen Verlinkungen hinaus zu verknüpfen. Im Bereich der Briefeditionen z.B. wurde schon recht früh der Wunsch geäußert, Briefe editionsübergreifend durchsuchbar zu machen und zu vernetzen. Basis dafür sollten die in TEI-XML kodierten Briefmetadaten, also die wichtigsten Kopfdaten eines Briefes, sein. Mit Entwicklung des Correspondence Metadata Interchange Format (TEI Correspondence SIG 2015) und des darauf aufbauenden Webservices correspSearch (http://correspSearch.net) konnte dies realisiert werden. Nicht nur können Briefeditionen jetzt digitale Briefverzeichnisse bereitstellen, die durch correspSearch zentral recherchierbar gemacht werden. Digitalen Briefeditionen ist es hier nun möglich, über die API von correspSearch Briefmetadaten zu beziehen und passend zu einem in der eigenen Edition vorliegenden Brief zu präsentieren (Dumont 2017). Damit werden schon einige methodische Probleme der Briefedition überwunden, andere bleiben hingegen noch offen.&lt;/p&gt;
&lt;p&gt;Insbesondere bei der Edition von Briefen kann es vorkommen, dass ein Brief mehrfach ediert vorliegt. Diese Situation kann dadurch entstanden sein, dass ältere Editionen (etwa des 19. Jahrhunderts) durch zeitgemäße abgelöst werden. Sie kann aber auch dadurch aufgekommen sein, dass der Brief nicht in einer Briefwechselausgabe, sondern in zwei Gesamtausgaben erschienen ist: Einerseits in derjenigen Gesamtausgabe, die die Korrespondenz des &lt;em&gt;Autors&lt;/em&gt; ediert, andererseits in derjenigen, die die Korrespondenz des &lt;em&gt;Empfängers&lt;/em&gt; ediert. In so einem Fall entstehen zum einen Kommentierungen, die in beiden Editionen ähnlich vorgenommen werden (z.B. Identifizierung von Personen etc.). Zum anderen werden aber auch Kommentierungen vorgenommen, die aus der spezifischen Perspektive der Edition entstehen – d.h. mit Fokus auf diejenige Person, deren Korrespondenz ediert wird. Dadurch entstehen zwei Annotationslayer, die sich im Idealfall ergänzen. Die Einzelstellen(-kommentierung) aus der jeweils anderen Edition kann also u.U. äußerst wertvoll für einen Nutzer sein.&lt;/p&gt;
&lt;p&gt;In digitalen Briefeditionen besteht nun prinzipiell die Möglichkeit, Daten aus anderen digitalen Editionen bzw. Portalen zu integrieren. Anstelle eines einmaligen, manuellen Importes bzw. Übernahme dieser Daten, ist heutzutage deren automatisierter Abruf über ein Application Programming Interface (API) zu favorisieren. Daten aus externen Quellen können so schneller aggregiert, aktualisiert und zusammen mit den eigenen Forschungsdaten – hier edierte Briefe – angeboten werden. Daher erscheint die Nutzung von APIs und standardisierten Datenformaten auch hier angeraten. Im Regelfall liegen Einzelstellenkommentare in digitalen Briefeditionen heutzutage als Bestandteil der TEI-XML-Dokumente vor (etwa im Element &lt;note&gt;). Sollen diese Kommentare allerdings über eine Schnittstelle zur freien Nachnutzung durch externe Editionen angeboten werden, müssen sie deutlich mehr Informationen tragen, als im Kontext der Edition – nämlich Metadaten (Autor des Kommentars, Bezugstext etc.). Auch eine Übermittlung des ganzen Briefes mitsamt seinen Einzelstellenerläuterungen ist nicht gewünscht, liegt der Text doch gerade im hier geschilderten Fall bereits vor.&lt;/p&gt;
&lt;p&gt;In Betracht kommt daher ein Datenformat, dass dediziert für den &lt;em&gt;Austausch&lt;/em&gt; und die Aggregation von Kommentaren, also vorwiegend textuell vorliegender Annotation, konzipiert ist. Für diesen Bereich gibt es bereits einen Standard, der auch schon weit verbreitet Anwendung findet: das Web Annotation Data Model (Sanderson, Ciccarese, und Young 2017). Es ist für genau dieses Nutzungsszenario – Austausch von Kommentaren – konzipiert und durch das W3C standardisiert. Es wird daher auch von DARIAH-DE empfohlen (Lordick u. a. 2016).&lt;/p&gt;
&lt;p&gt;Das Poster stellt nun anhand eines Beispiels diesen Ansatz exemplarisch vor und demonstriert seinen Nutzen. Als Beispiel ist der überlieferte Briefwechsel zwischen Alexander von Humboldt und Samuel Thomas Soemmerring gewählt, der zum einen in edition humboldt digital vorliegt, zum anderen in einer (derzeit noch unveröffentlichten) digitalen Edition zu Soemmerrings Biographica. Konkret sieht die Implementierung so aus: Einzelstellenerläuterungen aus der Soemmerring-Edition, die ursprünglich als TEI-XML vorliegen, werden über eine API im Web Annotation Data Model angeboten. Diese API wird von correspSearch abgefragt und die Daten aggregiert. Andere digitale Editionen, hier beispielhaft die edition humboldt digital, können anhand einer Kalliope-URI, also der eindeutigen und maschinenlesbaren Archivkennung, den Webservice correspSearch auf Annotationen zu einem bestimmten Brief hin abfragen. Im Erfolgsfall werden die Annotationen ausgeliefert und in der edition humboldt digital am Brief angezeigt (mit Quelle, Autor etc.).&lt;/p&gt;
</p1_abstract>
  <p2_paperID>145</p2_paperID>
  <p2_contribution_type>Poster</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Kath, Roxana
Keilholz, Franz
Pöckelmann, Marcus
Rücker, Michaela
Wöckener-Gade, Eva
Yu, Xiaozhou</p2_authors>
  <p2_organisations>Universität Leipzig, Deutschland
Technische Universität Dresden, Deutschland
Martin-Luther-Universität Halle-Wittenberg, Deutschland
Universität Leipzig, Deutschland
Universität Leipzig, Deutschland
Technische Universität Dresden, Deutschland</p2_organisations>
  <p2_emails>roxana.kath@me.com
franz.keilholz@tu-dresden.de
marcus.poeckelmann@informatik.uni-halle.de
mruecker1@me.com
woeckener-gade@uni-leipzig.de
xiaozhou.yu@tu-dresden.de</p2_emails>
  <p2_presenting_author>Pöckelmann, Marcus</p2_presenting_author>
  <p2_title> Entwicklungsstand im Projekt &lt;em&gt;Digital Plato&lt;/em&gt;</p2_title>
  <p2_abstract>&lt;p align="justify"&gt;Das interdisziplinäre Forschungsprojekt Digital Plato untersucht die Rezeption und Nachwirkung des platonischen Werkes in der griechischen Literatur bis in die Spätantike mit einem Fokus auf nicht-wörtlichen Referenzen. Der folgende Beitrag und das dazugehörige Poster geben einen Überblick über die wichtigsten Zwischenergebnisse, die während der zurückliegenden ersten Hälfte der Projektlaufzeit erzielt wurden. Neben der Organisation des Korpus sowie dessen linguistischer Anreicherung, dem Aufbau eines Wortnetzes und die Einbettung in einen Vektorraum, zählen dazu die Erfassung und Kategorisierung bekannter Referenzstellen, die Adaption der CTS zur wortgenauen Referenzierung, die theoretische Annäherung und schließlich die semi-automatische Suche nach neuen Paraphrasen.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>109</p3_paperID>
  <p3_contribution_type>Poster</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Brunner, Annelen
Engelberg, Stefan
Jannidis, Fotis
Tu, Ngoc Duyen Tanja
Weimer, Lukas</p3_authors>
  <p3_organisations>Institut für Deutsche Sprache, Deutschland
Institut für Deutsche Sprache, Deutschland
Universität Würzburg, Deutschland
Institut für Deutsche Sprache, Deutschland
Universität Würzburg, Deutschland</p3_organisations>
  <p3_emails>brunner@ids-mannheim.de
engelberg@ids-mannheim.de
fotis.jannidis@uni-wuerzburg.de
tu@ids-mannheim.de
lukas.weimer@uni-wuerzburg.de</p3_emails>
  <p3_presenting_author>Brunner, Annelen
Tu, Ngoc Duyen Tanja
Weimer, Lukas</p3_presenting_author>
  <p3_title>Projektvorstellung – Redewiedergabe. Eine literatur- und sprachwissenschaftliche Korpusanalyse</p3_title>
  <p3_abstract>&lt;p&gt;Das laufende DFG-Projekt „Redewiedergabe“ stellt einen Anwendungsfall quantitativer Sprach- und Literaturwissenschaft dar und beschäftigt sich mit dem Phänomen „Redewiedergabe“  auf der Grundlage großer Datenmengen. Zu diesem Zweck wird zum einen ein Korpus manuell mit Redewiedergabeformen annotiert, zum anderen werden Verfahren zur automatischen Erkennung des Phänomens entwickelt. Ziel ist es, Forschungsfragen nach der Entwicklung von Redewiedergabe vor allem im 19. Jahrhundert zu beantworten.&lt;/p&gt;
&lt;p&gt;Das Poster präsentiert einen Überblick über das Gesamtprojekt sowie erste Projektergebnisse.&lt;/p&gt;
</p3_abstract>
  <p4_paperID>188</p4_paperID>
  <p4_contribution_type>Poster</p4_contribution_type>
  <p4_acceptance>Akzeptiert</p4_acceptance>
  <p4_authors>Barzen, Johanna
Blumtritt, Jonathan
Breitenbücher, Uwe
Kronenwett, Simone
Leymann, Frank
Mathiak, Brigitte
Neuefeind, Claes</p4_authors>
  <p4_organisations>Universität Stuttgart, Deutschland
Universität zu Köln, Deutschland
Universität Stuttgart, Deutschland
Universität zu Köln, Deutschland
Universität Stuttgart, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p4_organisations>
  <p4_emails>johanna.barzen@iaas.uni-stuttgart.de
jonathan.blumtritt@uni-koeln.de
uwe.breitenbuecher@iaas.uni-stuttgart.de
simonekronenwett@gmail.com
frank.leymann@iaas.uni-stuttgart.de
bmathiak@uni-koeln.de
c.neuefeind@uni-koeln.de</p4_emails>
  <p4_presenting_author>Barzen, Johanna</p4_presenting_author>
  <p4_title>SustainLife - Erhalt lebender, digitaler Systeme für die Geisteswissenschaften</p4_title>
  <p4_abstract>&lt;p&gt;Der digitale Wandel verändert die Wissenschaft grundlegend (Kramp 2013). Das exponentielle Wachstum, die steigende Komplexität sowie der zunehmende Gebrauch von digitalen Forschungsdaten beeinflussen den Forschungsprozess signifikant. Um das Potential der fortschreitenden Digitalisierung optimal nutzen zu können, müssen entsprechende Infrastrukturen geschaffen werden, die das Management von Forschungsdaten, die Möglichkeit ihrer Vernetzung, ihre dauerhafte Verfügbarkeit und einen freien Zugang gewährleisten.&lt;/p&gt;
&lt;p&gt;Die Vielzahl von wissenschaftspolitischen Empfehlungen, Bestandsaufnahmen, Umfragen und institutionellen Richtlinien rund um Forschungsdaten, die in den vergangen Jahren veröffentlicht wurden, sind ein Zeichen zunehmenden Problembewusstseins, politischen Handlungswillens, aber eben auch anhaltenden Handlungsbedarfs (Pampel / Bertelmann 2011; Wissenschaftsrat 2012; DV-ISA 2016; Rat für Informationsinfrastrukturen 2016). Durch die großen europäischen und nationalen Infrastrukturprojekte in den Geisteswissenschaften (CLARIN, DARIAH), die Einrichtung von fachspezifischen Datenzentren und spezifisch geisteswissenschaftlichen Datenzentren, wie dem Data Center for the Humanities (DCH, s. http://dch.uni-koeln.de), hat sich die Versorgungslage für Forschungsdaten stetig verbessert. Doch längst werden nicht alle produzierten Daten und digitalen Forschungsergebnisse tatsächlich für die Nachnutzung verfügbar gemacht bzw. sind für eine dauerhafte Verfügbarkeit in einer hochdynamischen digitalen Welt gerüstet (Razum / Neumann 2014; Wissenschaftsrat 2012).&lt;/p&gt;
&lt;p&gt;Die Diskussion fokussiert bisher stark auf Forschungsinformationssysteme zur standardisierten Vorhaltung von Forschungsprimärdaten. Dabei bleibt weitgehend unberücksichtigt, dass ein Großteil der digitalen Produkte in den Geisteswissenschaften in einer Form vorliegen, die sich einer normierten Versorgung bislang entzieht. Gemeint sind Forschungsanwendungen, oder “lebende Systeme” (Sahle / Kronenwett 2013), die einen wesentlichen Bestandteil digitaler Ergebnissicherung darstellen: Präsentationssysteme, interaktive Visualisierungen, Recherche-Datenbanken, digitale Editionen und digitale Arbeitsumgebungen – um nur einige Formen zu nennen – sind Arbeitswerkzeuge, Plattformen der Dissemination und Aggregation von Forschungsergebnissen und sind aus dem Arbeitsalltag in Geisteswissenschaften nicht mehr wegzudenken. Ihre dauerhafte Erhaltung, Betreuung und Bereitstellung über den Zeitraum der Projektförderung hinaus stellt eine große organisatorische und letztlich finanzielle Herausforderung dar.&lt;/p&gt;
&lt;p&gt;Ein grundlegendes Problem ist die Sorge um softwaregestützte Präsentationen und Funktionalitäten, die in Forschungsprojekten entstehen (Sahle / Kronenwett 2013). Nicht selten sind diese die eigentlichen Träger von „Informationsgehalt beziehungsweise wissenschaftliche[m] Mehrwert“ der im Projekt erbrachten Forschungsleistung (Wuttke et al. 2016). Eine Nachhaltigkeitsstrategie, die auf eine Abtrennung und Archivierung allein der Datenbasis zurückfällt, führt unweigerlich zum Verlust von Information und reduziert im Extremfall den wissenschaftlichen Nutzen auf null (Blumtritt / Mathiak 2016). &lt;/p&gt;
&lt;p&gt;Forschungsanwendungen sind keineswegs statische Objekte, sondern unterliegen einem kontinuierlichen Veränderungszyklus. Viele Anwendungen fungieren als Plattformen, die User-Input entgegennehmen und damit ihre Datenbasis laufend verändern. Browser-Updates und Veränderungen der Nutzungsgewohnheiten können bestimmte Komponenten unbrauchbar oder obsolet machen und damit eine Überarbeitung des Codes anstoßen. Notwendige Sicherheitsupdates erfordern regelmäßiges Eingreifen und können Kaskaden von weiteren Updates und Softwareanpassungen nach sich ziehen. Der Verzicht auf kontinuierliche Wartung spart kurzfristig Kosten, verschärft mittelfristig aber das Problem. Erfahrungen aus dem “LAZARUS-Projekt” zeigen, dass Anwendungen, die über längere Zeit brach liegen, nur unter großem Ressourceneinsatz wieder revitalisiert werden können (Bingert et al. 2016). Ein erster Ansatz zur Archivierung von Forschungssoftware auf Basis des TOSCA-Standards wurde vorgestellt, löst jedoch nur das Problem der initialen automatisierten Bereitstellung, nicht der Revitalisierung terminierter Systeme (Breitenbücher et al. 2017).&lt;/p&gt;
&lt;p&gt;Darüber hinaus sehen sich Forschungseinrichtungen mit dem Dilemma konfrontiert, dass befristete Projektlaufzeiten einen dauerhaften Betrieb organisatorisch deutlich erschweren. Dies ist insbesondere in den Geisteswissenschaften fatal, da hier oftmals ein anderer Maßstab bezüglich der Nachhaltigkeit angelegt wird, als dies in den vergleichsweise schnelllebigen Naturwissenschaften üblich ist. Die institutionelle Vorhaltung von Forschungsdaten für einige Jahrzehnte erfüllt den Zweck der Überprüfbarkeit und Reproduzierbarkeit im Sinne der guten wissenschaftlichen Praxis, für die Vorhaltung von Gegenständen des kulturellen Erbes ist eine zeitliche Beschränkung nicht sinnvoll. Selbst dann, wenn sich eine Abschaltung aus Sicherheits- oder Kostengründen nicht vermeiden lässt, so ist zumindest zu gewährleisten, dass Anwendung und Datenbasis auf eine Art und Weise archiviert werden, dass sie sich jederzeit verlustfrei und benutzbar wiederherstellen lassen.&lt;/p&gt;
&lt;p&gt;Das Projekt „SustainLife“, das in Zusammenarbeit zwischen des Data Center for the Humanities (DCH) aus Köln und des Instituts für Architektur von Anwendungssystemen (IAAS) der Universität Stuttgart durchgeführt wird, adressiert diese Probleme. In diesem Poster stellen wir das Projekt vor und präsentieren den ersten Teil einer Anforderungsanalyse an lebende Systeme der Digital Humanities. Als zweiten Teil soll vor Ort eine Umfrage unter den Teilnehmern der Konferenz durchgeführt werden, um die Anforderungen an lebende Systeme aus unterschiedlichsten Domänen möglichst zielgenau zu analysieren. Diese Anforderungsanalyse stellt die Basis für im Projekt SustainLife geplante Erweiterungen der open-source Software „OpenTOSCA“ (Binz et al. 2013) dar, welche auf dem OASIS-Standard TOSCA (OASIS 2013) basieren. Dieser Standard ermöglicht die Automatisierung des Deployments und Managements von Anwendungen und stellt damit eine vielversprechende Grundlage zur kostengünstigen Sicherung der Nachhaltigkeit lebender Systeme dar.&lt;/p&gt;
</p4_abstract>
  <p5_paperID>199</p5_paperID>
  <p5_contribution_type>Poster</p5_contribution_type>
  <p5_acceptance>Akzeptiert</p5_acceptance>
  <p5_authors>Lordick, Harald
Mache, Beata</p5_authors>
  <p5_organisations>Steinheim-Institut, Deutschland
SUB Göttingen, Deutschland</p5_organisations>
  <p5_emails>lor@steinheim-institut.org
mac@steinheim-institut.org</p5_emails>
  <p5_presenting_author>Lordick, Harald
Mache, Beata</p5_presenting_author>
  <p5_title>Annotationen anhand der Gemeinsamen Normdatei aus einer anwendungsorientierten Perspektive historischer Forschung</p5_title>
  <p5_abstract>&lt;p&gt;xxx&lt;/p&gt;
</p5_abstract>
  <sessionID>60</sessionID>
  <p6_paperID>251</p6_paperID>
  <p6_contribution_type>Poster</p6_contribution_type>
  <p6_acceptance>Akzeptiert</p6_acceptance>
  <p6_authors>Forney, Christian
Rojas Castro, Antonio
Dängeli, Peter</p6_authors>
  <p6_organisations>Universität Bern, Historisches Institut
Universität zu Köln, Cologne Center for eHumanities
Universität Bern, Historisches Institut; Universität zu Köln, Cologne Center for eHumanities</p6_organisations>
  <p6_emails>christian.forney@hist.unibe.ch
arojasca@uni-koeln.de
p.daengeli@uni-koeln.de</p6_emails>
  <p6_presenting_author>Forney, Christian</p6_presenting_author>
  <p6_title>Vom geschützt zugänglichen Datenbankverbund zur offenen Editions- und Forschungsplattform: kritischer Rückblick auf halber Strecke</p6_title>
  <p6_abstract>&lt;p dir="ltr"&gt;Das Projekt &lt;em&gt;Haller Online&lt;/em&gt; (2016-2018) verfolgt das Ziel, eine seit 1991 aufgebaute Forschungsdatenbank aus einem proprietären Format in eine offene, TEI-konforme XML-Struktur zu überführen und öffentlich zugänglich zu machen. Parallel dazu wird an der Universität Bern und dem Cologne Center for eHumanities gemeinsam ein Onlineportal als Editions- und Forschungsplattform entwickelt.&lt;/p&gt;
&lt;p dir="ltr"&gt;Im Prozess der Datenkonversion tritt die Frage zutage, inwiefern Datenmodelle (und mit ihnen auch der Blick der Datenerzeuger und Forscher auf ihre eigene Forschung) durch bestimmte Werkzeuge beeinflusst werden. Mit dem Leitmotiv der Tagung gesprochen, stellen wir also die Frage nach der digitalen Vernunft, die mit bestimmten Datenmodellen in die wissenschaftliche Beschreibung von Wissensbeständen Einzug hält.&lt;/p&gt;
&lt;p dir="ltr"&gt;Auf dem Poster soll (v.a. auch graphisch) aufgezeigt werden, worin sich die in der bisherigen, ohne semantische Leitplanken operierenden Umgebung erfassten Datenarten vom neuen Datenmodell unterscheidet, das den in den TEI-Guidelines kodifizierten semantischen Einheiten folgt.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;em&gt;NB:&lt;/em&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;em&gt;Der Screenshot kann im endgültigen Abstract noch durch einen aktuellere Version ersetzt bzw. um ein graphisches Diagramm ergänzt werden, das Kernaspekte der Datentransformation illustriert.&lt;/em&gt;&lt;/p&gt;
</p6_abstract>
  <p7_paperID>247</p7_paperID>
  <p7_contribution_type>Poster</p7_contribution_type>
  <p7_acceptance>Akzeptiert</p7_acceptance>
  <p7_authors>Bigalke, Ben
Drach, Sviatoslav
Henny-Krahmer, Ulrike
Sepúlveda, Pedro
Theisen, Christian</p7_authors>
  <p7_organisations>Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität Würzburg, Deutschland
Neue Universität Lissabon, Portugal
Universität zu Köln, Deutschland</p7_organisations>
  <p7_emails>bbigalke@smail.uni-koeln.de
sdrach@smail.uni-koeln.de
ulrike.henny@uni-wuerzburg.de
pmpsepulveda@gmail.com
ctheise2@uni-koeln.de</p7_emails>
  <p7_presenting_author>Theisen, Christian</p7_presenting_author>
  <p7_title>Personen- und Figurennetzwerke in Fernando Pessoas Publikationsplänen</p7_title>
  <p7_abstract>&lt;p&gt;Im Nachlass des portugiesischen Dichters Fernando Pessoa (1888-1935) finden sich zahlreiche Listen geplanter Publikationen. Diesen stehen nur wenige zu Lebzeiten tatsächlich realisierte Veröffentlichungen gegenüber. Daraus ergibt sich ein Kontrast zwischen der Ebene des Möglichen und des Realen in der Literatur und Literaturproduktion. Verstärkt wird diese Spannung noch dadurch, dass Pessoa unter verschiedenen, insgesamt etwa 120 Autorennamen geschrieben hat - oder geplant hat zu schreiben.&lt;/p&gt;
&lt;p&gt;Die Notizzettel, Seiten aus Notizbüchern und andere Papiere aus Pessoas Nachlass, auf denen er die Pläne für seine Werke handschriftlich oder mit Schreibmaschine geschrieben festgehalten hat, werden in einer Kooperation zwischen dem Institut für Literatur und Tradition (IELT) der Neuen Universität Lissabon und dem Cologne Center for eHumanities (CCeH) der Universität zu Köln digital ediert. Die Dokumente werden in TEI codiert, wobei Namensvorkommen erfasst werden und eine Identifikation der hinter den Namen stehenden Personen und Figuren erfolgt.&lt;/p&gt;
&lt;p&gt;Die Vorkommen von Namen in Pessoas Publikationsplänen werden hier mit Hilfe einer interaktiven Netzwerkvisualisierung analysiert, um zu untersuchen, wie sich der von ihm in den Dokumenten entworfene Personen- und Figurenkosmos über die Zeit entwickelt.&lt;/p&gt;
&lt;p&gt;Die aus der Netzwerkinterpretation gewonnenen literaturhistorischen Erkenntnisse bestätigen, dass für Pessoa die Edition und Publikation des Werkes und dessen Planung nicht von der Bedeutungsebene des Werkes selbst zu unterscheiden ist.&lt;/p&gt;
&lt;p&gt;Methodisch eröffnet die Visualisierung durch das interaktive Element und den höheren Grad der Abstraktion gegenüber den edierten Dokumenten neue Interpretationsspielräume.&lt;/p&gt;
</p7_abstract>
  <p8_paperID>267</p8_paperID>
  <p8_contribution_type>Poster</p8_contribution_type>
  <p8_acceptance>Akzeptiert</p8_acceptance>
  <p8_authors>Gálffy, Andreas
Kamphausen, Julian
Kronenwett, Simone
Wieners, Jan G.</p8_authors>
  <p8_organisations>Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p8_organisations>
  <p8_emails>agalffy@smail.uni-koeln.de
julian.kamphausen@uni-koeln.de
simone.kronenwett@uni-koeln.de
jan.wieners@uni-koeln.de</p8_emails>
  <p8_presenting_author>Gálffy, Andreas
Kamphausen, Julian
Kronenwett, Simone
Wieners, Jan G.</p8_presenting_author>
  <p8_title>Die Macht der Daten - vom konsequenten Umgang mit Forschungsdaten</p8_title>
  <p8_abstract>&lt;p&gt;Theorie: Lehrveranstaltung in Kooperation mit nestor&lt;/p&gt;
&lt;p dir="ltr"&gt;Der vorgeschlagene Beitrag in der Kategorie "Poster" veranschaulicht ausgewählte Inhalte und Ergebnisse, welche im Rahmen einer Lehrveranstaltung zum Thema "Forschungsdatenmanagement und Langzeitarchivierung" (FDM und LZA) von den TeilnehmerInnen erarbeitet und präsentiert wurden. Die Übung wurde im Sommersemester 2017 am Institut für Digital Humanities (Historisch-Kulturwissenschaftliche Informationsverarbeitung) an der Universität zu Köln in Kooperation mit nestor (Network of Expertise in long-term Storage and availability of digital Resources in Germany), dem Kompetenznetzwerk für Langzeitarchivierung und Langzeitverfügbarkeit digitaler Ressourcen in Deutschland, durchgeführt (Lehrveranstaltung 2017; Nestor 2017).&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p&gt;Praxis: Experten aus dem FDM- und LZA-Sektor&lt;/p&gt;
&lt;p dir="ltr"&gt;In weiterer Zusammenarbeit mit insgesamt sieben externen FDM- und LZA-Experten aus der Praxis (u.a. von GESIS, Hochschulbibliothekszentrum NRW (HBZ), Data Center for the Humanities (DCH), Forschungsdatenzentrum IANUS, Stiftung Rheinisch-Westfälisches Wirtschaftsarchiv zu Köln (RWWA)) wurden anhand konkreter Beispiele und Anwendungsfälle Fragen erörtert und Probleme dargestellt, wie Forschungsdaten nachhaltig bereitgestellt und langfristig gesichert werden können und welche Arbeitsabläufe dabei notwendig sind. In diesem Kontext wurde besonders ein Blick auf die sich derzeit etablierenden Data Professional Berufe geworfen (z.B. Data Librarian, Data Curator, Data Archivist, Data Manager, Data Scientist, Data Journalist etc.) sowie ihre jeweiligen Aufgabenprofile mit den eingeladenen Fachleuten diskutiert.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Aufgabe: Kurzartikel für neue nestor-Publikationsreihe&lt;/p&gt;
&lt;p dir="ltr"&gt;Vor diesem Hintergrund wählten die TeilnehmerInnen im Laufe der Lehrveranstaltung ein theoretisches oder praxisnahes Thema aus dem FDM- bzw. LZA-Bereich und verfassten dazu einen Beitrag, welcher in der neu entstandenen studentischen Kurzartikelreihe von nestor veröffentlicht werden wird. Von den insgesamt 20 Artikeln, deren Themenbreite von der digitalen Sicherung archäologischer Rekonstruktionen bis hin zur Transformation XML-basierter Daten reicht, werden im vorgeschlagenen Tagungsbeitrag ausgewählte Beispiele vorgestellt und visualisiert.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Beispiel: ”Informationserlebnis als Reiseerlebnis? Ein Vergleich eines Informationsinfrastrukturverbundes mit einem öffentlichen Personennahverkehrsverbund”&lt;/p&gt;
&lt;p&gt;Sehr deutlich wird der derzeitige Zustand des Forschungsdatenmanagements, wenn man ihn mit einem öffentlichen Personennahverkehrsverbund (wie der Verkehrsverbund Rhein-Sieg, www.vrsinfo.de) vergleicht. Bis zum 1. September 1987 existierten im Nahverkehr um Köln redundante Verbindungen, die nur teilweise aufeinander abgestimmt waren - viele Verkehrsbetriebe leisteten nebeneinander ihren Dienst. Mit dem Verkehrsverbund war es möglich, Angebote aufeinander abzustimmen und Ortsverbindungen besser zu koordinieren. Eine ähnliche Entwicklung im Forschungsdatenmanagement wird in diesem Aufsatz vorgeschlagen, mit Verweis auf die Nationale Forschungsdateninfrastruktur (NFDI) (Rat 2016).&lt;/p&gt;
&lt;p dir="ltr"&gt;Zusammenfassung&lt;/p&gt;
&lt;p dir="ltr"&gt;Es bleibt festzuhalten, dass die Digital Humanities ein sehr weites und schwer ab- und eingrenzbares Feld darstellen. Hiervon sind Forschungsdatenmanagement und Langzeitarchivierung nur zwei - augenscheinlich kleine - Themenbereiche, die ihrerseits heterogener kaum sein können. Diese Heterogenität begründet sich in der Mannigfaltigkeit der Daten und kommt in der Vielfalt der Themenbereiche und Tätigkeitsfelder zum Ausdruck. Dies wurde in der Lehrveranstaltung an zahlreichen Fallbeispielen deutlich und spiegelt sich erneut in den eingereichten Studierendenbeiträgen wider.&lt;/p&gt;
&lt;p dir="ltr"&gt;Des Weiteren lässt sich eine hohe Dynamik in diesen Feldern spüren. Schon allein in der stetig steigenden Zahl von Stellenanzeigen, die nach qualifizierten Fachkräften werben, aber auch in der Zahl der Ansätze, Normierungsversuche und Leitlinien, die zu einem bewusst nachhaltigen Umgang mit Daten anhalten sollen. Diese werden sowohl von oben herab (top-down) vorgeschrieben, als auch seitens der betroffenen Institutionen (bottom-up) betrieben. Das Ziel ist allen gemeinsam: der stetig steigenden Zahl von Forschungsdaten Herr zu werden, Forschungsdaten langfristig zu sichern und nachhaltig bereitzustellen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Bibliographie (Auswahl)&lt;/p&gt;
&lt;p dir="ltr"&gt;Büttner, Stephan / Hobohm, Hans-Christoph / Müller, Lars (Hrsg.) (2011): “Handbuch Forschungsdatenmanagement”, Bad Honnef: Bock + Herchen, http://www.forschungsdatenmanagement.de/  [zuletzt aufgerufen am 20.09.2017]&lt;/p&gt;
&lt;p dir="ltr"&gt;Lehrveranstaltung “Forschungsdatenmanagement und Langzeitarchivierung” (2017), Universität zu Köln, Institut für Digital Humanities (Historisch-Kulturwissenschaftliche Informationsverarbeitung), Sommersemester 2017, Homepage, http://www.lehre.jan-wieners.de/sosem17-fdm-lza/ [zuletzt aufgerufen am 20.09.2017]&lt;/p&gt;
&lt;p dir="ltr"&gt;Nestor (Network of Expertise in long-term Storage and availability of digital Resources in Germany) (2017), Homepage, http://www.langzeitarchivierung.de/Subsites/nestor/DE/Home/home_node.html, [zuletzt aufgerufen am 22.09.2017]&lt;/p&gt;
&lt;p dir="ltr"&gt;Neuroth, Heike / Strathmann, Stefan / Oßwald, Achim et al. (Hrsg.) (2012): “Langzeitarchivierung von Forschungsdaten. Eine Bestandsaufnahme”, Version 1.0, Boizenburg: Verlag Werner Hülsbuch, http://nestor.sub.uni-goettingen.de/bestandsaufnahme/ [zuletzt aufgerufen am 20.09.2017]&lt;/p&gt;
&lt;p dir="ltr"&gt;Rat für Informationsinfrastrukturen (2016): “Leistung aus Vielfalt. Empfehlungen zu Strukturen, Prozessen und Finanzierung des Forschungsdatenmanagements in Deutschland”, Göttingen, http://www.rfii.de/?wpdmdl=1998 [zuletzt aufgerufen am 20.09.2017]&lt;/p&gt;
&lt;p dir="ltr"&gt;Ray, Joyce M.  (Hrsg.) (2014): “Research Data Management. Practical Strategies for Information Professionals”, West Lafayette/Indiana&lt;/p&gt;
</p8_abstract>
  <p9_paperID>160</p9_paperID>
  <p9_contribution_type>Poster</p9_contribution_type>
  <p9_acceptance>Akzeptiert</p9_acceptance>
  <p9_authors>Pollin, Christopher
Vogeler, Georg</p9_authors>
  <p9_organisations>Universität Graz, Österreich
Universität Graz, Österreich</p9_organisations>
  <p9_emails>christopher.pollin@uni-graz.at
georg.vogeler@uni-graz.at</p9_emails>
  <p9_presenting_author>Pollin, Christopher</p9_presenting_author>
  <p9_title> MEDEA: Datenkonsistenz mittels Ontologie</p9_title>
  <p9_abstract>&lt;p align="left"&gt;Daten werden laut der Vision der ‘High Level Expert Group on Scientiﬁc Data’ in der Zukunft einen Grad an Ausdrucksstärke und Formen der Selbstbeschreibung erhalten, dass sie in die Lage versetzt werden, ihre eigene Infrastruktur zu stellen. Auch die Idee des Semantic Web verspricht eine Zukunft in der Maschinen selbständig mit Daten agieren können. Die praktische Realität - gerade für die digitalen Geisteswissenschaften - ist noch eine Andere. Dennoch steckt in den Methoden des Semantic Webs ein Potenzial, das es mit vernünftiger Kritik zu nutzen gilt.&lt;br /&gt;Das Projekt MEDEA (Modeling semantically Enriched Digital Edition of Accounts) versucht dies zu verwirklichen, indem an einem kollektiven Standard zu semantischen Anreicherung digitaler Editionen von historischen Rechnungsbüchern gearbeitet wird. Es wird der Frage nachgegangen, inwieweit Methoden des Semantic Webs bei der Erschließung, Analyse und Darstellung historischer Rechnungsbücher helfen können. Im MEDEA Projekt formalisiert die "Bookkeeping" Ontologie eine grundlegende Wissensstruktur, um Einträge in Rechnungsbüchern, ihre Transaktionen von Gütern, Dienstleistungen oder Geldbeträgen von einem Akteur oder Konto zu einem anderen, standardisiert beschreiben zu können. Neben anderen Bereichen, wie dem Retrieval, wird untersucht hinwieweit mittels Reasoning unterschiedliche TEI Kodierungen von Rechnungsbüchern auf ihre Datenkonsistenz hin überprüft werden können.&lt;br /&gt;Ein grundlegendes Problem ist bereits durch den Widerspruch der hermeneutischen Arbeit der Historikerin und der Entscheidbarkeit von OWL gegeben. Sind Ontologien in OWL ausdrucksstark genug, um geisteswissenschaftliche Daten so beschreiben zu können, dass ein logisches Schlussfolgern Ergebnisse erzielt, das die Konsistenz und die Qualität der Daten abbildet?&lt;/p&gt;
</p9_abstract>
  <p10_paperID>265</p10_paperID>
  <p10_contribution_type>Poster</p10_contribution_type>
  <p10_acceptance>Akzeptiert</p10_acceptance>
  <p10_authors>Homburg, Timo</p10_authors>
  <p10_organisations>Hochschule Mainz, Deutschland</p10_organisations>
  <p10_emails>timo.homburg@gmx.de</p10_emails>
  <p10_presenting_author>Homburg, Timo</p10_presenting_author>
  <p10_title>Semantische Extraktion auf antiken Schriften am Beispiel von Keilschriftsprachen mithilfe semantischer Wörterbücher</p10_title>
  <p10_abstract>&lt;p&gt;Einleitung und Motivation&lt;/p&gt;
&lt;p&gt;Semantische Extraktionsmechanismen (z.B. Topic Modelling) werden seit vielen Jahren im Bereich des Semantic Web und Natural Language Processings sowie in den Digital Humanities als Verfahren zur Visualisierung und automatischen Kategorisierung von Dokumenten eingesetzt. Oft ergeben sich durch den Einsatz neue Aspekte der Interpretation von Dokumentensammlungen die vorher noch nicht ersichtlich waren. Als Beispiele solcher Verfahren kommen häufig Machine Learning Algorithmen zum Einsatz, welche eine Grobeinordnung von Texten vornehmen können. Gepaart mit Metadaten von Texten können anschließend beispielsweise thematische Übersichten von Dokumenten mit geographischem Bezug auf Kartenmaterialien in GIS Systemen oder mittels historischer Gazetteers zeitliche Zusammenhänge automatisiert dargestellt werden. In dieser Publikation möchten wir die Möglichkeiten der semantischen Extraktion nutzen und diese auf einer Sammlung von Texten in Keilschriftsprachen anwenden.&lt;/p&gt;
&lt;p&gt;Keilschriftsprachen&lt;/p&gt;
&lt;p&gt;Keilschriftsprachen haben in den letzten Jahren ein größeres Interesse in der Digital Humanities und Linguistik Community erfahren.Neben der andauernden Standardisierung in Unicode werden unter anderem Part Of Speech Tagger und Mechanismen der automatisierten Übersetzung erprobt um Keilschrifttexte besser mit dem Computer zu erfassen und zu interpretieren. Desweiteren wurde die Erlernbarkeit der Keilschriftsprachen durch digitale Tools wie Eingabemethoden oder Karteikartenlernprogramme verbessert.&lt;br /&gt;Trotz all der erreichten Fortschritte verbleiben jedoch zahlreiche Probleme bei der maschinellen Verarbeitung von Keilschriftsprachen, die unter anderem mit der geringen Verfügbarkeit annotierter Ressourcen und der fehlenden Verfügbarkeit maschinenlesbarer und semantisch sowie linguistisch annotierter Wörterbücher zusammenhängt. Diese Limitierungen hindern viele Natural Language Processing und semantische Extraktionsalgorithmen daran ein besseres Ergebnis zu erzielen. Wir möchten mit dieser Publikation einen Beitrag leisten diese Situation zu verbessern und stellen das "Semantic Dictionary for Ancient Languages" vor, welches ein Versuch ist durch Annotierung vorhandener in der Forschungscommunity anerkannter Wörterbuchressourcen mit Unicode Characters, Semantic Web Konzepten, etymologischen Daten, gemeinsamen Vokabularen und POSTags eine semantische Ressource in RDF für die Optimierung solcher Algorithmen auf Basis der Sprachen Hethitisch, Sumerisch und Akkadisch zu schaffen.&lt;br /&gt;Das Wörterbuch basiert auf dem Lemon-Standard, ein W3C Standard der es erlaubt ebenfalls multilinguale Resourcen abzubilden. So können Entwicklungen der Sprache und gemeinsame Vokabulare wie zum Beispiel Akkadogramme und Sumerogramme in Hethitisch mit erfasst werden.&lt;/p&gt;
&lt;p&gt;Semantisches Wörterbuch und Semantische Extraktion&lt;/p&gt;
&lt;p&gt;Wir testen die Performance des Wörterbuchs auf einer der größten Sammlungen von digitalen Keilschrifttexten, der CDLI, aus der wir repräsentative Texte in hethitischer, sumerischer und akkadischer Keilschrift aus verschiedenen Epochen extrahieren und mittels Machine Learning klassifizieren, sowie verschlagworten. Das Ergebnis der semantischen Extraktion ist eine Sammlung von Themen pro Keilschrifttafel, die sich wiederum in Überkategorien gruppieren lassen und in einen zeitlichen, sprachlichen, dialektischen, sowie örtlichen Kontext gestellt werden können. Anhand der verschiedenen Metadaten der CDLI war es uns möglich eine thematische Karte der Fundorte der Keilschrifttafeln sowie deren Inhalt pro Epoche darzustellen aus der das relevante Fachpublikum schließen kann welche Themen zu welcher Zeit an welchem Fundort relevant für die Schreiber der jeweiligen Epoche waren. Im Zuge einer Weiterentwicklung möchten wir diese Informationen mit weiteren Metadaten wie beispielsweise der Jurisdiktion, den Daten der jeweiligen Herrscher sowie rekonstruierten Orten aus der antiken Zeit vervollständigen um Rückschlüsse auf interessante historische Ereignisse zu ziehen.&lt;/p&gt;
&lt;p&gt;Aufbau des Posters&lt;/p&gt;
&lt;p&gt;Auf unserem Poster möchten wir gerne den Prozess des Aufbaus, sowie die Struktur des semantischen Wörterbuchs sowie die Karte die durch unsere semantische Extraktion entstanden ist präsentieren um die jeweiligen Fachwissenschaftler zur Diskussion über die Entwicklung eines Semantic Web von Keilschriftsprachen und Keilschriftartefakten einzuladen. Desweiteren soll unser Poster eine Reihe von Anwendungen demonstrieren die sich in Zukunft mit unserer semantischen Ressource entwickeln lassen können um einen Beitrag zu einem hoffentlich zukünftig existierenden LinkedData Datensatz für Keilschriftartefakte zur Dokumentation von Keilschrift zu leisten.&lt;/p&gt;
</p10_abstract>
  <sessionID>60</sessionID>
  <p11_paperID>133</p11_paperID>
  <p11_contribution_type>Poster</p11_contribution_type>
  <p11_acceptance>Akzeptiert</p11_acceptance>
  <p11_authors>Loebel, Jens-Martin
Hahn, Carolin</p11_authors>
  <p11_organisations>bitGilde IT Solutions UG, Deutschland
Universität Freiburg</p11_organisations>
  <p11_emails>loebel@bitgilde.de
kontakt@carolinhahn.de</p11_emails>
  <p11_presenting_author>Loebel, Jens-Martin
Hahn, Carolin</p11_presenting_author>
  <p11_title>Digitale Wissenschaft – Eine Podcastreihe</p11_title>
  <p11_abstract>&lt;p&gt;So vielfältig die Angebote, Projekte und Methoden der digitalen Geisteswissenschaften sind, so schwierig ist der Werkzeugkasten "DH" zu fassen. Wir wagen eine neue Herangehensweise an das Thema – und veröffentlichen ab Januar 2018 regelmäßig Podcast-Folgen zu Historie, Forschungsbereichen und -standards. Die primäre Zielgruppe unseres Projekts sind Studierende und interessierte Laien, die sich über grundlegende Werkzeuge und Praktiken der Digital Humanities informieren möchten. Unsere Ziele: Mehr Aufmerksamkeit für DH-Forschungsprojekte erreichen und einen wertvollen Beitrag zur Wissensvermittlung leisten.&lt;/p&gt;
</p11_abstract>
  <p12_paperID>268</p12_paperID>
  <p12_contribution_type>Poster</p12_contribution_type>
  <p12_acceptance>Akzeptiert</p12_acceptance>
  <p12_authors>Fischer, Frank
Kittel, Christopher
Milling, Carsten
Trilcke, Peer
Wolf, Jana</p12_authors>
  <p12_organisations>Higher School of Economics, Moskau
Karl-Franzens-Universität Graz; Open Knowledge Forum Österreich
Berlin
Universität Potsdam, Deutschland
Mittelbayerische Zeitung Regensburg</p12_organisations>
  <p12_emails>ffischer@hse.ru
contact@christopherkittel.eu
cmil@hashtable.de
trilcke@uni-potsdam.de
jana_a_wolf@hotmail.com</p12_emails>
  <p12_presenting_author>Fischer, Frank
Trilcke, Peer</p12_presenting_author>
  <p12_title>Dramenquartett – Eine didaktische Intervention</p12_title>
  <p12_abstract>&lt;p dir="ltr"&gt;Ziel dieses Posters ist es, anhand von 32 deutschsprachigen Dramen in die Netzwerkanalyse literarischer Texte einzuführen, eine didaktische Intervention für eine zwar mittlerweile etablierte Methode der literaturwissenschaftlichen Analyse, die aber nicht immer genügend reflektiert wird: Der Errechnung teils komplexer netzwerktheoretischer Maße entspricht nicht immer ein entsprechender Sprung zur Bedeutungsebene. Was bedeutet es zum Beispiel wirklich, dass die durchschnittliche Pfadlänge in Goethes »Faust. Der Tragödie erster Theil« genau 1,79 beträgt? Wenn man jedoch diesen Wert in Beziehung zu entsprechenden Werten anderer Stücke setzt, gewinnt er an komparatistischer Bedeutung.&lt;/p&gt;
&lt;p dir="ltr"&gt;Um den komparatischen Blick im Kontext der literaturwissenschaftlichen Netzwerkanalyse zu schulen, setzen wir mit unserem Poster auf einen Gamification-Ansatz. Anders als bei unserem ersten Experiment in dieser Richtung – der auf der DHd2016 präsentierten Android-App »Play(s)« (vgl. Göbel/Meiners 2016), in deren Mittelpunkt die spielerische Korrektur und Anreicherung unserer Korpusdaten stand –, handelt es sich diesmal um eine nicht-technische Anwendung, die auf spielerische Weise netzwerkanalytisches Datenmaterial explorierbar macht.&lt;/p&gt;
&lt;p dir="ltr"&gt;Dabei wird das Posterformat in zweierlei Hinsicht bespielt: Das Poster ist einerseits eine Datenvisualisierung auf Grundlage eines selbst gepflegten größeren Dramenkorpus. Andererseits ist es ein in 32 Teile zerlegbares Dramenquartett, das spielerisch mit den Bedeutungshorizonten verschiedener netzwerktheoretischer Größen bekannt macht und ein Bewusstsein für komparatistische Möglichkeiten trainiert. Dieser Ansatz ist in den Geisteswissenschaften nicht neu, verwiesen sei etwa auf das architekturgeschichtliche Quartettspiel »Plattenbauten. Berliner Betonerzeugnisse« (Mangold u. a. 2001), in dem technische Daten verschiedener Plattenbautypen gegenübergestellt werden (vgl. auch Richter 2006).&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Didaxe des Dramenquartetts bezieht sich auf mehrere Dimensionen: eine literaturgeschichtliche, eine quantitative, eine netzwerktheoretische. Die 32 Stücke bilden einen Minimalkanon, der von der Zeit der Gottschedischen Theaterreformen bis in die Moderne reicht. Statt der lexikonartigen Beschreibung eines solchen Kanons (wie etwa im »Dramenlexikon des 18. Jahrhunderts«, Hollmer/Meier 2001), besteht das Beschreibungsinstrument hier in visuellen und quantitativen Werten, die Vergleichbarkeit herstellen – erst dieser Umstand vereint die verschiedenen Karten zu einem kompetitiven Spiel.&lt;/p&gt;
&lt;p dir="ltr"&gt;Als visueller Catch der Quartettkarten dient eine Visualisierung des jeweiligen extrahierten sozialen Netzwerks (vgl. Fischer u. a. 2016). Die weiteren Informationen auf den Karten setzen sich aus Metadaten (Autor*in – Titel – Untertitel – Jahr) und statischen und dynamischen Netzwerkdaten zusammen (Anzahl von Subgraphen – Netzwerkgröße – Netzwerkdichte – Clustering-Koeffizient – Durchschnittliche Pfadlänge – Höchster Degreewert und Name der entsprechenden Figur, all-in index). Das Deckblatt enthält eine Einführung zum Projekt und seinen Hintergründen sowie Kurzdefinitionen der auf den einzelnen Karten enthaltenen netzwerktheoretischen Daten.&lt;/p&gt;
&lt;p dir="ltr"&gt;Das Poster wird mit unserer Skriptsammlung ›dramavis‹ generiert, die in der neuen Version 0.4 eine entsprechende Funktion erhalten hat (Kittel/Fischer 2017). Für das Konferenzposter haben wir einen Fallback-Kanon zusammengestellt (Stücke von Johann Christoph und Luise Adelgunde Victorie Gottsched, von Gellert, J. E. Schlegel, Caroline Neuber, Klopstock, Lessing, Gerstenberg, Goethe, Lenz, Klinger, Schiller, Kotzebue, Kleist, Zacharias Werner, Müllner, Grillparzer, Grabbe, Büchner, Hebbel, Gustav Freytag, Anzengruber, Arno Holz, Wedekind, Schnitzler, Erich Mühsam). Über eine individualisierbare Kanon-Datei können aber auch eigene Quartette zusammengestellt werden.&lt;/p&gt;
&lt;p&gt;Bibliografie&lt;/p&gt;
&lt;p dir="ltr"&gt;Fischer, F.; Göbel, M.; Kampkaspar, D.; Kittel, C.; Trilcke, P.: Distant-Reading Showcase. 200 Years of Literary Network Data at a Glance. DHd2016, Leipzig. DOI: &lt;https://dx.doi.org/10.6084/m9.figshare.3101203.v1&gt;.&lt;/p&gt;
&lt;p dir="ltr"&gt;Göbel, M.; Meiners, H.-L.: Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus. DHd2016, Leipzig.&lt;/p&gt;
&lt;p dir="ltr"&gt;Hollmer, H.; Meier, A. (Hg.): Dramenlexikon des 18. Jahrhunderts. München 2001.&lt;/p&gt;
&lt;p dir="ltr"&gt;Kittel C., Fischer F.: dramavis v0.4 (September 2017). Repo: &lt;https://github.com/lehkost/dramavis&gt;.&lt;/p&gt;
&lt;p dir="ltr"&gt;Mangold, C. u. a.: Plattenbauten. Berliner Betonerzeugnisse. Ein Quartettspiel. Berlin 2001.&lt;/p&gt;
&lt;p dir="ltr"&gt;Richter, P.: Der Plattenbau als Krisengebiet. Die architektonische und politische Transformation industriell errichteter Wohngebäude aus der DDR am Beispiel der Stadt Leinefelde. Hamburg, Univ., Diss., 2006.&lt;/p&gt;
</p12_abstract>
  <p13_paperID>231</p13_paperID>
  <p13_contribution_type>Poster</p13_contribution_type>
  <p13_acceptance>Akzeptiert</p13_acceptance>
  <p13_authors>Proisl, Thomas
Evert, Stefan</p13_authors>
  <p13_organisations>Friedrich-Alexander-Universität Erlangen-Nürnberg, Deutschland
Friedrich-Alexander-Universität Erlangen-Nürnberg, Deutschland</p13_organisations>
  <p13_emails>thomas.proisl@fau.de
stefan.evert@fau.de</p13_emails>
  <p13_presenting_author>Proisl, Thomas</p13_presenting_author>
  <p13_title>Delta vs. N-Gram-Tracing: Wie robust ist die Autorschaftsattribuierung?</p13_title>
  <p13_abstract>&lt;p&gt;Die Autorschaftsattribuierung, also die Zuweisung von Texten unbekannter oder umstrittener Autorschaft zu ihrem wahren Autor, hat vielfältige Anwendungen beispielsweise in der Literatur- und Geschichtswissenschaft oder der forensischen Sprachwissenschaft. Eine populäre Methode zur Autorschaftsattribuierung ist die Anwendung von Deltamaßen (Burrows 2002; Argamon 2008) wie zum Beispiel Cosine-Delta (Smith und Aldridge 2011). Deltamaße verwenden die n häufigsten Wörter im Korpus, standardisieren die Frequenzen auf z-Werte und wenden ein Abstandsmaß, im Fall von Cosine-Delta den Kosinusabstand, an. Typischerweise schließt sich die Anwendung eines (hierarchischen) Clusterverfahrens an, das Texte des selben Autors zusammengruppiert.&lt;/p&gt;
&lt;p&gt;Eine neue Methode zur Autorschaftsattribuierung ist das sogenannte N-Gram-Tracing (Grieve et al., in Begutachtung). Hierbei werden aus dem zu klassifizierenden Text alle Wort- oder Buchstaben-N-Gramme einer bestimmten Länge extrahiert. Der Text wird dann dem Autor zugewiesen, der im Vergleichskorpus die meisten dieser N-Grammtypen verwendet. Die Häufigkeit der N-Gramme spielt dabei keine Rolle, es geht nur darum, wie viele N-Gramme aus dem zu klassifizierenden Text auch im Vergleichskorpus auftauchen.&lt;/p&gt;
&lt;p&gt;Wenn Methoden zur Autorschaftsattribuierung angewandt werden sollen um tatsächlich eine strittige Autorschaftsfrage zu klären, ist es sehr wichtig die Zuverlässigkeit und Robustheit der Verfahren abschätzen zu können, schließlich gibt es eine ganze Reihe von Einflussfaktoren. Kritisch sind zum Beispiel die folgenden Fragen: Welchen Einfluss haben die Länge des zu klassifizierenden Textes und die Größe des Vergleichskorpus auf die Genauigkeit der Autorschaftsattribuierung? Gibt es für die beiden Verfahren eine Mindesttextlänge, die nicht unterschritten werden sollte? Wie stark werden die Verfahren durch autor- und werkspezifische Eigenheiten beeinflusst? Ist die Genauigkeit der Autorschaftsattribuierung robust in Bezug auf die Zusammensetzung des Vergleichskorpus oder kann die Auswahl der Autoren und Texte das Ergebnis beeinträchtigen?&lt;/p&gt;
&lt;p&gt;Um diese Fragen zumindest teilweise beantworten zu können, führen wir eine Reihe von Evaluationsexperimenten durch. Um die Ergebnisse des N-Gram-Tracings besser mit denen von Delta vergleichen zu können, führen wir auf den Deltaabständen zwischen den Texten kein Clustering sondern eine nearest-neighbor-Klassifikation durch, d.h. wir weisen den zu klassifizierenden Text dem Autor des Textes mit dem geringsten Abstand zu. Im Einzelnen handelt es sich um zwei Kürzungs- und zwei Samplingexperimente. Datengrundlage für die Kürzungsexperimente sind die deutschen, englischen und französischen Romankorpora, die unter anderem von Jannidis et al. (2015) und Evert et al. (2017) verwendet wurden. Jedes Korpus besteht aus je drei Romanen von 25 Autoren, also aus 75 Romanen. Für das erste Kürzungsexperiment wird die Größe des Vergleichskorpus stabil gehalten und nur der zu klassifizierende Text gekürzt. Für Delta wird zusätzlich die Anzahl der verwendeten häufigsten Wörter variiert. Im zweiten Kürzungsexperiment werden sowohl der zu klassifizierende Text als auch das Vergleichskorpus gekürzt. Über ein leave-one-out-Verfahren werden alle Texte im Korpus klassifiziert um die Genauigkeit der Verfahren zu ermitteln.&lt;/p&gt;
&lt;p&gt;Für die Samplingexperimente verwenden wir eine Sammlung von 1018 deutschen Romanen aus dem langen 19. Jahrhundert. Alle Texte wurden von Muttersprachlern verfasst. Für das erste Samplingexperiment ziehen wir 5000 zufällige Stichproben von 25 Autoren und je drei Romanen (die Zusammensetzung der einzelnen Stichproben ist also vergleichbar mit den oben erwähnten Romankorpora). Für das zweite Samplingexperiment beschränken wir uns auf die 25 Autoren, die in unserer Sammlung mit den meisten Romanen vertreten sind, und ziehen 5000 zufällige Stichproben von je drei Romanen pro Autor (also ebenfalls 25×3 Texte). Für jede Stichprobe ermitteln wir über ein leave-one-out-Verfahren die Genauigkeit der beiden Verfahren.&lt;/p&gt;
&lt;p&gt;Erste Ergebnisse deuten darauf hin, dass beide Verfahren weniger robust sind als bisher angenommen wurde.&lt;/p&gt;

&lt;p&gt;Bibliographie&lt;/p&gt;
&lt;p&gt;Argamon, Shlomo (2008): „Interpreting Burrows’ delta: Geometric and probabilistic foundations“. In: &lt;em&gt;Literary and Linguistic Computing&lt;/em&gt; 23/2: 131–47. https://doi.org/10.1093/llc/fqn003&lt;br /&gt;Burrows, John (2002): „‘Delta’—A measure of stylistic difference and a guide to likely authorship“. In: &lt;em&gt;Literary and Linguistic Computing&lt;/em&gt; 17/3: 267–87. https://doi.org/10.1093/llc/17.3.267.&lt;br /&gt;Evert, Stefan / Proisl, Thomas / Jannidis, Fotis / Reger, Isabella / Pielström, Steffen / Schöch, Christof / Vitt, Thorsten (2017): „Understanding and explaining Delta measures for authorship attribution.“ In: &lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt;. https://doi.org/10.1093/llc/fqx023.&lt;br /&gt;Grieve, Jack / Carmody, Emily / Clarke, Isobelle / Gideon, Hannah / Heini, Annina / Nini, Andrea / Waibel, Emily (in Begutachtung): „Attributing the Bixby Letter using n-gram tracing“. Eingereicht bei &lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt; am 26. Mai 2017.&lt;br /&gt;Jannidis, Fotis / Pielström, Steffen / Schöch, Christof / Vitt, Thorsten (2015): „Improving Burrows’ Delta – An empirical evaluation of text distance measures“. In: &lt;em&gt;Digital Humanities 2015: Conference Abstracts&lt;/em&gt;. http://dh2015.org/abstracts.&lt;br /&gt;Smith, Peter W. H. / Aldridge, W. (2011): „Improving authorship attribution: Optimizing Burrows’ delta method“. In: &lt;em&gt;Journal of Quantitative Linguistics&lt;/em&gt; 18/1: 63–88. https://doi.org/10.1080/09296174.2011.533591.&lt;/p&gt;
</p13_abstract>
  <p14_paperID>299</p14_paperID>
  <p14_contribution_type>Poster</p14_contribution_type>
  <p14_acceptance>Akzeptiert</p14_acceptance>
  <p14_authors>Burr, Elisabeth
Fußbahn, Ulrike</p14_authors>
  <p14_organisations>Universität Leipzig, Deutschland
Universität Leipzig, Deutschland</p14_organisations>
  <p14_emails>elisabeth.burr@uni-leipzig.de
uf28bope@studserv.uni-leipzig.de</p14_emails>
  <p14_presenting_author>Burr, Elisabeth
Fußbahn, Ulrike</p14_presenting_author>
  <p14_title>"Kinder des Buchdrucks" im Digitalen Zeitalter. Ein romanistisches Digital Humanities Modul</p14_title>
  <p14_abstract>&lt;p&gt;Schon länger wird die Frage nach der Lehre der Digital Humanities gestellt. So hat Brett Hirsch 2012 den Sammelband &lt;em&gt;Digital Humanities Pedagogy&lt;/em&gt;. vorgelegt. Im selben Jahr ist auch der Sammelband von Matthew K. Gold &lt;em&gt;Debates in the Digital Humanities&lt;/em&gt; sowie der Sammelband von Claire Warwick, Melissa Terras und Julianne Nyhan &lt;em&gt;Digital Humanities in Practice&lt;/em&gt; erschienen, in denen die Lehre ebenfalls eine Rolle spiel. Zudem sind Initiativen, wie die EU geförderte &lt;em&gt;#dariahTeach&lt;/em&gt; entstanden, die Materialien für die Lehre von Digital Humanities entwickelt und online zur Verfügung stellt. 2017 haben dann Fotis Jannidis, Hubertus Kohle und Malte Rehbein ein umfangreiches deutschsprachiges Lehrbuch zu &lt;em&gt;Digital Humanities&lt;/em&gt; vorgelegt. In keiner dieser Publikationen wird aber, soweit wir sehen können, der Frage nachgegangen, wie der Auftrag, den die neue Epistemologie &lt;em&gt;Digital Humanities&lt;/em&gt; als den ihren erkennt, nämlich mittels der Nutzung von computationellen Methoden ein umfassendes und kritisches Wissen von Artefakten sowie deren Relationen untereinander zu schaffen, in der Lehre umgesetzt werden kann. Einen Versuch in diese Richtung wollen wir mit unserem Poster präsentieren.&lt;/p&gt;
</p14_abstract>
  <p15_paperID>117</p15_paperID>
  <p15_contribution_type>Poster</p15_contribution_type>
  <p15_acceptance>Akzeptiert</p15_acceptance>
  <p15_authors>Hug, Marius</p15_authors>
  <p15_organisations>Staatsbibliothek zu Berlin - Preußischer Kulturbesitz, Deutschland</p15_organisations>
  <p15_emails>marius.hug@googlemail.com</p15_emails>
  <p15_presenting_author>Hug, Marius</p15_presenting_author>
  <p15_title> BeWeB-3D – Zur Digitalisierung interaktiver Buchobjekte</p15_title>
  <p15_abstract>&lt;p&gt;Im Rahmen des Projekts BeWeb-3D (http://sbb.berlin/beweb3d) und in Kooperation mit dem Zentrum für digitale Kulturgüter in Museen (ZEDIKUM) wird an der Staatsbibliothek zu Berlin – Preußischer Kulturbesitz ein Konzept zur Digitalisierung sogenannter Bewegungsbücher erarbeitet. Dabei handelt es sich um Buchobjekte, die bewegliche Teile enthalten und damit eine Interaktion erfordern, die über das reine Umblättern der Seiten hinausgeht. Damit ist diesen Buchtypen – seien es wissenschaftliche Bücher aus der Zeit des 12. Jahrhunderts oder Kinderbücher ab dem Anfang des 19. Jahrhunderts – eines gemein: Sie verfügen über eine zusätzliche (räumliche oder zeitliche) Dimension und verweigern sich so gängigen Digitalisierungspraktiken aus dem Bereich der Buchdigitalisierung. Im Rahmen einer Posterpräsentation soll einerseits das Datenmodell und andererseits die bis dahin replizierten Buchtypen unter Berücksichtigung der jeweiligen Digitalisierungstechnologien vorgestellt werden. Verschiedene digitalisierte Spielbilderbücher stehen browserbasiert oder als Augmented Reality &lt;em&gt;Hands-On&lt;/em&gt; zur Verfügung.&lt;/p&gt;
</p15_abstract>
  <sessionID>60</sessionID>
  <p16_paperID>201</p16_paperID>
  <p16_contribution_type>Poster</p16_contribution_type>
  <p16_acceptance>Akzeptiert</p16_acceptance>
  <p16_authors>Franzini, Greta
Fischer, Franz
Kestemont, Mike</p16_authors>
  <p16_organisations>Georg-August-Universität Göttingen
Universität zu Köln
Universiteit Antwerpen</p16_organisations>
  <p16_emails>gfranzini@etrap.eu
franz.fischer@uni-koeln.de
mike.kestemont@uantwerpen.be</p16_emails>
  <p16_presenting_author>Franzini, Greta</p16_presenting_author>
  <p16_title>Digital Medievalist: A Web Community for Medievalists working with Digital Media</p16_title>
  <p16_abstract>&lt;p&gt;&lt;em&gt;Digital Medievalist &lt;/em&gt;is an international web community for medievalists working with digital media. The poster outlines the activities of &lt;em&gt;Digital Medievalist&lt;/em&gt;, inviting German-speaking scholars to learn more about the initiative and to give feedback on how it can better address community needs.&lt;/p&gt;
</p16_abstract>
  <p17_paperID>113</p17_paperID>
  <p17_contribution_type>Poster</p17_contribution_type>
  <p17_acceptance>Akzeptiert</p17_acceptance>
  <p17_authors>Hess, Jan
Lebherz, Daniel
Zeyen, Christian</p17_authors>
  <p17_organisations>Trier Center for Digital Humanities (TCDH); Universität Trier
Center for Informatics Research and Technology (CIRT); Universität Trier
Center for Informatics Research and Technology (CIRT); Universität Trier</p17_organisations>
  <p17_emails>hess@uni-trier.de
lebherz@uni-trier.de
zeyen@uni-trier.de</p17_emails>
  <p17_presenting_author>Hess, Jan</p17_presenting_author>
  <p17_title>Text Mining und Computersimulation zur Analyse autobiographischer Texte: Einflüsse auf das literarische Schaffen Klaus Manns</p17_title>
  <p17_abstract>&lt;p&gt;„Ist das alles?“ (Mann 1995, S. 151) – Die vorwurfsvoll anmutende (rhetorische) Frage, die Klaus Mann Ende Juni 1933 unter einer Auflistung seiner Werke des zurückliegenden Halbjahrs in seinem Tagebuch notiert, gibt einen Hinweis darauf, wie selbstkritisch sich der Schriftsteller mit dem eigenen Schaffen auseinandersetzt. Angesichts der persönlichen und politischen Umstände sowie der Vielzahl an Kontakten und Aktivitäten, die er in seinem Journal verzeichnet, erscheint es fast etwas überraschend, dass die gut drei Monate nach seiner Emigration erstellte Werkliste immerhin 21 – wenn auch größtenteils kürzere – Texte umfasst. In Anbetracht seiner ständigen Rastlosigkeit stellt sich nicht nur die Frage, wann Klaus Mann überhaupt seiner eigentlichen schriftstellerischen Arbeit nachgeht, sondern auch, welche Faktoren zum Ge- oder Misslingen seines Schaffens beitragen. Wie wirken sich beispielsweise die beinahe allabendlichen Theater-, Kino- und Barbesuche oder die mehr oder weniger regelmäßig eingenommenen Rauschmittel auf seine literarische Produktivität aus? Welche Rolle spielt das tägliche Lektürepensum? Haben die zahlreichen Treffen, Telefonate und Korrespondenzen Einfluss auf seine schriftstellerische Arbeit? Oder erweisen sich die politischen Umstände, Angst, Verzweiflung und Hass gegenüber dem Nationalsozialismus als entscheidendes Vehikel literarischer Produktivität?&lt;/p&gt;
&lt;p&gt;Aufgrund des Umfangs und der Detailgenauigkeit seiner alltäglichen Schilderungen von Aktivitäten und Gedanken erscheinen Klaus Manns Tagebücher als idealer Untersuchungsgegenstand zur Beantwortung der aufgeworfenen Fragen. Gerade die Vielzahl und Komplexität der darin enthaltenen Informationen machen es jedoch schwierig, sich potentiellen Faktoren literarischer Produktivität tiefergehend analytisch zu nähern. Um diese große Anzahl an Informationen und deren Zusammenhänge besser bzw. überhaupt untersuchen zu können und die entsprechenden literaturwissenschaftlichen Forschungsprozesse zu unterstützen, sollen im Rahmen von &lt;em&gt;eXplore!&lt;/em&gt; am Fallbeispiel der Klaus Mann-Tagebücher verschiedene Methoden des Text Mining (Feldman/ Sanger 2006; Manning/ Schütze 1999) angewendet werden. Auf Basis der dadurch erzielten Ergebnisse sollen Methoden der Computersimulation eingesetzt werden, um die eingangs aufgeworfenen Fragen beantworten zu können (Abb. 1).&lt;/p&gt;
&lt;p&gt;Methoden der Computersimulation haben sich bereits seit längerer Zeit in verschiedenen  Wissenschaftsdisziplinen etabliert. Seit den 1990er Jahren halten diese auch vermehrt Einzug in sozialwissenschaftliche Forschungsbereiche und bieten dort verschiedene Möglichkeiten zur Darstellung und Analyse von gesellschaftlichen Systemzusammenhängen (Gilbert 2007). Eine ähnliche Entwicklung lässt sich auch in den Digital Humanities nachvollziehen, wenngleich diese Methoden – insbesondere im literaturwissenschaftlichen Bereich – eine eher untergeordnete Rolle spielen (Kohle 2017). Die zunehmende Anwendung von Computersimulation als Forschungsmethode liegt darin begründet, dass sie die Möglichkeit bietet, komplexe, unzugängliche Systeme zu untersuchen und experimentell zu analysieren. In solchen Experimenten lassen sich u.a. durch die Betrachtung verschiedener Szenarien Resultate erzielen, die Rückschlüsse auf Plausibilitäten von Handlungen, Strukturen und Zusammenhängen im zugrundeliegenden System erlauben. Der Weg zu verlässlichen Ergebnissen einer Simulationsstudie führt in einem ersten, wesentlichen Schritt über die Modellbildung und Sammlung dafür relevanter Daten (Law 2015). Zu diesem Zweck soll das zugrundeliegende System, also konkret Klaus Mann, seine sozialen Kontakte und sein Umfeld möglichst realitätsnah mit allen wesentlichen Eigenschaften, Aktivitäten und Zusammenhängen in einer agentenbasierten Sozialsimulation (Davidsson 2002) abgebildet werden (Abb. 2).&lt;/p&gt;
&lt;p&gt;Die dafür notwendige Generierung einer geeigneten Datengrundlage ist ein anspruchsvoller Prozess, in dem anhand von Methoden aus dem Bereich des Text Mining zunächst die wesentlichen, auf den Untersuchungsgegenstand einwirkenden Faktoren identifiziert und analysiert werden müssen. Konkret sollen dabei verschiedene, bereits etablierte Methoden wie Named Entity Recognition, Sentiment Analyse oder Worthäufigkeits- und Kookkurrenzanalysen angewendet werden, um vorab Kontakte, Aktivitäten, Aufenthalte, Tätigkeiten, persönliche oder politische Ereignisse zu identifizieren, welche mit Klaus Manns literarischer Produktivität in Zusammenhang stehen könnten. Anders als im Falle der Computersimulation kommen Text Mining-Methoden, -Werkzeuge und Programmbibliotheken im literaturwissenschaftlichen Kontext bereits ungleich häufiger zum Einsatz. Zur Komplexitätsreduktion werden die zugrundeliegenden Text Mining-Prozesse dabei jedoch oft als Black Box betrachtet, was den Nachteil birgt, dass das Zustandekommen der erzielten Ergebnisse oft nur schwer nachvollzogen werden kann. Um dem entgegenzuwirken sollen die im Rahmen von &lt;em&gt;eXplore!&lt;/em&gt; benötigten Text Mining-Prozesse explizit in Form von Scientific Workflows (Taylor et al. 2014) definiert und ausgeführt werden. Die in den Digital Humanities vergleichsweise wenig verbreiteten Scientific Workflows dienen der systematischen und transparenten Automatisierung wiederkehrender Datenverarbeitungsprozesse und haben sich in den Naturwissenschaften als geeignetes Mittel erwiesen, wissenschaftliche Prozesse zu strukturieren, zu dokumentieren und damit die Reproduktion und Validierung von Ergebnissen erheblich zu erleichtern.&lt;/p&gt;
&lt;p&gt;Im Kontext von &lt;em&gt;eXplore!&lt;/em&gt; sollen erfolgreich eingesetzte Workflows daher als prozedurales Erfahrungswissen gespeichert und mithilfe von Verfahren der künstlichen Intelligenz wiederverwendbar gemacht werden, sodass die Definition neuer Text Mining-Workflows auf Basis bereits bestehender Workflows erfolgen und damit vereinfacht werden kann. Letztlich sollen somit nicht nur die Forschungsprozesse im Fallbeispiel Klaus Mann, sondern zukünftig auch ähnliche Text Mining-Problemstellungen sowie die  Modellierungsarbeit in Simulationsstudien zur Erforschung weiterer autobiographischer Texte sinnvoll unterstützt werden (Abb. 2).&lt;/p&gt;
</p17_abstract>
  <p18_paperID>116</p18_paperID>
  <p18_contribution_type>Poster</p18_contribution_type>
  <p18_acceptance>Akzeptiert</p18_acceptance>
  <p18_authors>Kröger, Bärbel
Popp, Christian</p18_authors>
  <p18_organisations>Akademie der Wissenschaften zu Göttingen, Deutschland
Akademie der Wissenschaften zu Göttingen, Deutschland</p18_organisations>
  <p18_emails>bkroege@gwdg.de
cpopp@gwdg.de</p18_emails>
  <p18_presenting_author>Kröger, Bärbel
Popp, Christian</p18_presenting_author>
  <p18_title>Kleriker des Alten Reiches in der Digitalen Welt. Das Forschungsportal Germania Sacra Online</p18_title>
  <p18_abstract>&lt;p&gt;Das Poster soll die digitalen Angebote der Germania Sacra vorstellen und an einem konkreten Beispiel Möglichkeiten und Grenzen diskutieren, wie mithilfe der Netzwerkanalyse große Datenkorpora visualisiert und ausgewertet werden können, um neue Fragestellungen zu generieren.&lt;/p&gt;
</p18_abstract>
  <p19_paperID>252</p19_paperID>
  <p19_contribution_type>Poster</p19_contribution_type>
  <p19_acceptance>Akzeptiert</p19_acceptance>
  <p19_authors>Czmiel, Alexander
Grabsch, Sascha
Jürgens, Marco
Maiwald, Anke
Willenborg, Josef</p19_authors>
  <p19_organisations>Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland
Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland
Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland
Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland
Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland</p19_organisations>
  <p19_emails>czmiel@bbaw.de
grabsch@bbaw.de
juergens@bbaw.de
maiwald@bbaw.de
willenborg@bbaw.de</p19_emails>
  <p19_presenting_author>Czmiel, Alexander
Grabsch, Sascha
Jürgens, Marco</p19_presenting_author>
  <p19_title>erschließen - verknüpfen - finden: Forschungsdaten im Digitalen Wissensspeicher</p19_title>
  <p19_abstract>&lt;p&gt;Im Rahmen des DFG-geförderten Projektes „Digitaler Wissensspeicher“ wurde an der Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW) seit 2012 ein zentraler Zugang für sämtliche digitale Forschungsdaten und Ressourcen der Akademie geschaffen. Hauptziel war dabei die vollständige Erfassung und Volltext-Indexierung der technisch sowie inhaltlich äußerst vielfältigen und heterogenen Ressourcen der BBAW. Der Wissensspeicher bietet eine exemplarische Lösung für einen Katalog heterogener, digitaler geisteswissenschaftlicher Ressourcen, die zentral aggregiert und über eine Volltextsuche verfügbar gemacht werden.&lt;/p&gt;
&lt;p&gt;Durch die projektübergreifende Volltextsuche in allen verzeichneten Sammlungen sowie die miteinander vernetzten Metadaten verbessert der Digitale Wissensspeicher die Auffindbarkeit, Sichtbarkeit und damit die Nutzbarkeit von Digital-Humanities-Projekten und digitalen Forschungsergebnissen.&lt;/p&gt;
</p19_abstract>
  <p20_paperID>176</p20_paperID>
  <p20_contribution_type>Poster</p20_contribution_type>
  <p20_acceptance>Akzeptiert</p20_acceptance>
  <p20_authors>Wissik, Tanja
Resch, Claudia</p20_authors>
  <p20_organisations>Österreichische Akademie der Wissenschaften, Österreich
Österreichische Akademie der Wissenschaften, Österreich</p20_organisations>
  <p20_emails>Tanja.Wissik@oeaw.ac.at
Claudia.Resch@oeaw.ac.at</p20_emails>
  <p20_presenting_author>Wissik, Tanja</p20_presenting_author>
  <p20_title>MeuchelmörderInnen, KindsmörderInnen, DiebInnen und die dazugehörigen Tatbestände: Erstellung eines Thesaurus für das österreichische Strafrecht des 18. Jahrhunderts zur Erschließung einer Flugblattsammlung</p20_title>
  <p20_abstract>&lt;p&gt;In diesem Posterbeitrag beschreiben wir die Erstellung eines Thesaurus für das österreichische Strafrecht des 18. Jahrhunderts  und gehen auf die damit verbundenen Herausforderungen ein, sowohl seitens des Themengebietes aber auch seitens der Quellen.&lt;/p&gt;
&lt;p&gt;Als Quellen dienen kaum erforschte Flugblätter zur Bekanntmachung von Hinrichtungen im 18. Jahrhundert (vgl. Ammerer/Adomeit 2010), die derzeit mit einem modernen Methodeninventar als XML-Dokumente nach den Richtlinien der TEI annotiert und auf ihre digitale Verfügbarkeit im Netz vorbereitet werden. Die Darstellung der Sachverhalte, die zur Hinrichtung führen, sind zwar medial überformt (vgl. Peil 2002, Kosenina 2005, Wiltenburg 2009, Dainat 2009, Härter 2010), beruhen allerdings auf den Entscheidungen des damaligen Stadtgerichts.&lt;/p&gt;
&lt;p&gt;Bei der Erschließung zu berücksichtigen ist, dass im Laufe des 18. Jahrhunderts unterschiedliche Strafrechtsbestimmungen in Kraft waren. Vor 1768 gab es in den Ländern Österreichs und Böhmens kein einheitliches Straf- und Strafprozessrecht, jedoch galten die “Constitutio Criminalis Carolina Peinliche Gerichts- oder Peinliche Halsgerichtsordnung Kaiser Karls V.” und daneben unterschiedliche Landgerichtsordnungen, etwa die “Land-Gerichts-Ordnung. Deß Ertz-Hertzogthumbs Oesterreich unter der Ennß”. Erst im Jahre 1768 wurde durch die Constitutio Criminalis Theresiana, auch “Theresiana” genannt, ein einheitliches Straf- und Strafprozessgesetz eingeführt, welches aber bereits 1787 vom Allgemeines Gesetzbuch über Verbrechen und derselben Bestrafung, dem sogenannten “Strafgesetzbuch Josephs II” oder “Josephina” abgelöst wurde.&lt;/p&gt;
&lt;p&gt;All diesen Strafrechtsbestimmungen ist gemein, dass sie unterschiedliche Tatbestände definieren, die mit der Todesstrafe belegt sind. Im Laufe der Zeit wurden aber zum Teil neue Tatbestände ergänzt und mit neuen Definitionen versehen und stattdessen andere Delikte abgeschafft. Diese Delikte werden auch in den zu erschließenden Quellen beschrieben: Wie erwähnt, handelt es sich dabei nicht eigentlich um Rechtstexte, sondern um Flugblätter, die über öffentliche Hinrichtungen im 18. Jahrhundert berichten. Die Quellen werden zwar als “Todesurteile” bezeichnet, aber da sich die Flugblätter an ein breites Publikum wenden, werden die Delikte allgemeinverständlich beschrieben, kaum aber unter Verwendung der damals zeitgenössischen Rechtsterminologie. DelinquentInnen werden als &lt;em&gt;UrphedsbrecherInnen&lt;/em&gt; oder &lt;em&gt;DiebInnen&lt;/em&gt; bezeichnet, ohne aber den eigentlichen Tatestand zu nennen - eine Referenz auf die jeweilige Gesetzesstelle fehlt oft gänzlich. Im gesamten Quellenmaterial konnte nur ein einziger Beleg gefunden werden, bei dem direkt auf den Gesetzestext referenziert wurde.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Ein weitere Herausforderung für die Erschließung der Quellen stellen die Schreibvarianten dar, vgl. etwa &lt;em&gt;Diebstahl&lt;/em&gt; vs. &lt;em&gt;Diebstall &lt;/em&gt;oder &lt;em&gt;Urfehde&lt;/em&gt; vs. &lt;em&gt;Urphed&lt;/em&gt;, &lt;em&gt;Vrphed&lt;/em&gt; oder &lt;em&gt;Vrphedt&lt;/em&gt;. Aus diesen Gründen ist die toolgestützte Analyse und die Zuordnung der Flugblätter zu den einzelnen Straftatbeständen erschwert. In ähnlich gelagerten Projekten, wie z.B. den “Proceedings of the old Bailey online”, die eine Zeitspanne von ca. 240 Jahren abdecken, wurden die Delikte für die Erschließung des Materials bewusst nicht nach den gesetzlichen Bestimmungen definiert (vgl. Hitchcock, Tim et al.), sondern allgemeine Definitionen erarbeitet. In dem vorliegenden Projekt haben wir uns für eine quellennahe Definition entschieden. Aus diesem Grund ist es für den Thesaurus auch essentiell, nicht nur die Definitionen bereitzustellen, sondern auch die Angabe der Quelle, aus der die Definition stammt, sowie die Erfassung von Varianten. Als Framework für die Erstellung und mögliche Darstellung des Thesaurus im Semantic Web wird als formale Sprache auf SKOS (Simple Knowledge Organisation System) mit der Erweiterung SKOS-XL (SKOS Simple Knowledge Organization System eXtension for Labels) zurückgegriffen.&lt;/p&gt;
&lt;p&gt;Um eine verallgemeinernde Typologisierung aller in den Quellen vorkommenden Delikte zu vermeiden und eine quellennahe und differenzierte Zuordnung zu ermöglichen, möchten wir den von uns erstellten Thesaurus präsentieren, der auf der Web-Applikation eine von mehreren Möglichkeiten des Zugriffs darstellen wird (andere Zugriffsmöglichkeiten ergeben sich aus biographischen Angaben zu den DelinquentInnen wie Alter, Geschlecht, Familienstand, Religionszugehörigkeit, Herkunft bzw. der Volltextsuche im Text).&lt;/p&gt;
&lt;p&gt;Anhand des Posterbeitrags wollen wir erörtern, wie die einzelnen Tatbestände aus den unterschiedlichen Rechtsnormen miteinander in Relation gesetzt werden können, wie weiters eine Verbindung zu den heutigen Tatbeständen hergestellt werden wird und worin letztlich der Mehrwert für zukünftige UserInnen innerhalb und außerhalb der Academia besteht. Schließlich sollen auch mögliche Nachnutzungsszenarien für den Thesaurus thematisiert werden.&lt;/p&gt;
</p20_abstract>
  <sessionID>60</sessionID>
  <p21_paperID>151</p21_paperID>
  <p21_contribution_type>Poster</p21_contribution_type>
  <p21_acceptance>Akzeptiert</p21_acceptance>
  <p21_authors>Koch, Carina</p21_authors>
  <p21_organisations>Universität Graz, Österreich</p21_organisations>
  <p21_emails>carina.koch@uni-graz.at</p21_emails>
  <p21_presenting_author>Koch, Carina</p21_presenting_author>
  <p21_title>Die illustrierte Postkarte und die digitalen Geisteswissenschaften – (Kulturerbe)objekt oder (Nachrichten)text</p21_title>
  <p21_abstract>&lt;p&gt;Die Postkarte wird als Quelle für unterschiedliche geisteswissenschaftliche Disziplinen herangezogen. Wurde sie zunächst in erster Linie für bildwissenschaftliche Forschungen genutzt, wird in den letzten Jahren vermehrt auf die Relevanz der Postkarte als Bild-Text-Medium hingewiesen. Die digitalen Geisteswissenschaften werden dem gerecht und ermöglichen neue Forschungsperspektiven. Aufbauend auf digitalisierte und strukturiert erfasste Sammlungen können mit digitalen Methoden statistische Auswertungen, quantitative Analysen und qualitative Untersuchungen durchgeführt werden. Anhand des Projektes “GrazMuseum Online” werden Optionen, Herausforderungen und Ergebnisse einer digitalen Bearbeitung der illustrierten Postkarte, die gleichsam Bild und Text ist, exemplarisch vorgestellt.&lt;/p&gt;
</p21_abstract>
  <p22_paperID>136</p22_paperID>
  <p22_contribution_type>Poster</p22_contribution_type>
  <p22_acceptance>Akzeptiert</p22_acceptance>
  <p22_authors>Klaffki, Lisa
Steyer, Timo</p22_authors>
  <p22_organisations>Herzog August Bibliothek Wolfenbüttel, Deutschland
Herzog August Bibliothek Wolfenbüttel, Deutschland; Forschungsverbund Marbach Weimar Wolfenbüttel, Deutschland</p22_organisations>
  <p22_emails>klaffki@hab.de
steyer@hab.de</p22_emails>
  <p22_presenting_author>Klaffki, Lisa</p22_presenting_author>
  <p22_title>“Kann man da eben mal was eintragen und visualisieren?” Digitaler Praxistest für die DARIAH-DE-Infrastruktur</p22_title>
  <p22_abstract>&lt;p dir="ltr"&gt;&lt;strong&gt;Die Ausgangslage: Diskrepanz zwischen Anspruch und Wirklichkeit&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;In den letzten Jahren wurden vermehrt digitale Services und Ressourcen entwickelt, die EinzelforscherInnen, aber auch Projektgruppen in der Arbeit mit digitalen Methoden, der Generierung, dem Forschungsdatenmanagement und in toto bei der Transformation von Arbeitsprozessen in das digitale Medium unterstützen sollen. Signifikante Bestandteile des Research Data Lifecycles können bereits über digitale Angebote abgedeckt werden (Puhl et. al. 2015). Ein wesentliches Ziel, welches mit dem Aufbau der digitalen Infrastrukturen verfolgt wird, ist, dass auch diejenigen Forschungsvorhaben davon profitieren sollen, in denen keine oder nur geringe technischen Kenntnisse oder eine DH-Unterstützung vorhanden sind. Aktuell besteht zwischen Anspruch und Wirklichkeit aber noch eine Diskrepanz: Auf der einen Seite scheint mittlerweile die Mehrheit der WissenschaftlerInnen den Nutzen von Normdaten, Integration ihrer Forschungsdaten in Suchmaschinen oder die Visualisierungen ihrer Daten in Forschungsumgebungen zu erkennen. Auf der anderen Seite steht aber die Kritik, dass die oftmals propagierte einfache und intuitive Nutzung der digitalen Angebote nicht vorhanden ist. Der Mehrwert, den die Services den eher auf ein analoges Projektergebnis ausgerichteten Projekten bieten, stehen in keinem effizienten Verhältnis von Aufwand und Ertrag. Auch Schulungsvideos oder Workshops sind häufig zu unspezifisch oder nicht auf die vorhandenen Daten anwendbar.&lt;/p&gt;
&lt;p dir="ltr"&gt;Diesem Kritikpunkt möchte sich diese Postereinreichung annehmen und exemplarisch Forschungsdaten aus einem nicht genuinen DH-Projekt in verschiedene Komponenten der Infrastruktur von DARIAH-DE, einem digitalen Forschungsinfrastrukturprojekt für die Geistes- und Kulturwissenschaften, integrieren.&lt;/p&gt;
&lt;p dir="ltr"&gt;Ziel ist es, dabei den Aufwand und die erforderlichen Kenntnisse zu evaluieren, die eine erfolgreiche Integration der Forschungsdaten bedingen. Die DARIAH-DE-Dienste eignen sich besonders für diesen Test, da sich die DARIAH-DE-Angebote ausdrücklich an EinzelforscherInnen richten (DARIAH-DE in Kürze).&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Der Usecase: ein frühneuzeitlicher Auktionskatalog&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Als Usecase dient das Forschungsprojekt “Erschließung frühneuzeitlicher Auktionskataloge” des Forschungsverbundes Marbach Weimar Wolfenbüttel (MWW) (Autorenbibliotheken). Die aus einem Auktionskatalog von 1670 rekonstruierte Bibliothek gehörte dem Chiliasten Benedikt Bahnsen (gest. 1669). Der aus Norddeutschland stammende und nach Amsterdam emigrierte Bahnsen war als Verleger, Buchhändler und Bücheragent, Mathematiker und Rechenmeister tätig. Das Projekt erfasst in einer Excel-Tabelle alle Losnummern, d.h. Titel des Katalogs, die Tiefenerschließung umfasst die bibliographischen Angaben der verzeichneten Bücher, Normdaten zu Personen und Werken, Geodaten für die Druckorte sowie Links zu Digitalisaten und in Nachweissysteme. Mit der Erschließung wird dem Alleinstellungsmerkmal dieser Bibliothek Rechnung getragen, bei der es sich um das umfangreichste Einzelrepositorium für nonkonformes und heterodoxes Schrifttum handeln dürfte (Beyer et. al. 2017, 43–70).&lt;/p&gt;
&lt;p dir="ltr"&gt;Zwar verfügt das Projekt durch die Zusammenarbeit mit den DH-MitarbeiterInnen des Forschungsverbundes über eine eigene Datenbank und eine Präsentation im WWW (Auktionskataloge). Aber begonnen hatte das Projekt ohne deren Unterstützung und genau die in dieser Phase in einer Exceltabelle erhobenen Daten sollen für den Usecase mit DARIAH-DE verwendet werden. Zwar ist es keine empirische Studie, aber der mit den Projektbeteiligten durchgeführte Usecase hat dennoch aufgrund seines prototypischen Charakters richtungsweisende Bedeutung.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Die Werkzeuge: DARIAH-DE-Dienste&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Innerhalb von DARIAH-DE wurde eine Data Federation Architecture (DFA) entwickelt (Gradl / Henrich / Plutte 2015; Gradl / Henrich 2016). Unter diesem Begriff sind mehrere modulare Komponenten gebündelt, die für sich alleine oder im Zusammenspiel genutzt werden können. Davon nutzt der hier skizzierte Workflow das DARIAH-DE-Repository zum Speichern und persistenten Adressieren der Forschungsdaten, die Collection Registry zum Verzeichnen der Datensammlung mitsamt Schnittstelle, um die Daten in die Generische Suche zu integrieren und wiederauffindbar und somit nachnutzbar zu machen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Für viele der in den verschiedenen Schritten eines Data Research Lifecycles anfallenden Aufgaben können also Komponenten der DFA zum Einsatz kommen. Die DFA deckt aber nicht alle Schritte eines Forschungsprozesses und damit letztlich auch des Research Data Lifecycles ab. So bleibt der Abschnitt der Analyse oder Visualisierung offen, allerdings gibt in den digitalen Geisteswissenschaften hinreichend andere Tools, die diese Lücke zielgerichtet füllen können.&lt;/p&gt;
&lt;p dir="ltr"&gt;Für den Praxistest wird zur Visualisierung der räumlich-zeitlichen Daten ein ebenfalls aus dem DARIAH-DE-Projekt stammendes Tool verwendet, der Geo-Browser (Kollatz 2016).&lt;/p&gt;
</p22_abstract>
  <p23_paperID>193</p23_paperID>
  <p23_contribution_type>Poster</p23_contribution_type>
  <p23_acceptance>Akzeptiert</p23_acceptance>
  <p23_authors>Zirker, Angelika
Bauer, Matthias
Kirchhoff, Leonie
Lahrsow, Miriam</p23_authors>
  <p23_organisations>Humboldt Universität zu Berlin; Eberhard Karls Universität Tübingen
Eberhard Karls Universität Tübingen
Eberhard Karls Universität Tübingen
Eberhard Karls Universität Tübingen</p23_organisations>
  <p23_emails>angelika.zirker@uni-tuebingen.de
m.bauer@uni-tuebingen.de
leonie.kirchhoff@uni-tuebingen.de
miriam.lahrsow@uni-tuebingen.de</p23_emails>
  <p23_presenting_author>Zirker, Angelika
Bauer, Matthias
Kirchhoff, Leonie
Lahrsow, Miriam</p23_presenting_author>
  <p23_title>TEASys: Kollaboratives digitales Annotieren als Lehr- und Lernprozess</p23_title>
  <p23_abstract>&lt;p&gt;Das Poster präsentiert TEASys (Tübingen Explanatory Annotations System; Bauer/Zirker 2015) und die Möglichkeiten, die es als heuristisches &lt;em&gt;tool&lt;/em&gt; in Lehr- und Lernprozessen bietet. Im Projekt annotieren Studierende kollaborativ Texte der englischsprachigen Literatur. Nachdem TEASys und seine Struktur wie auch Funktionen bereits bei der DHd 2016 und 2017 vorgestellt wurden, liegt der Schwerpunkt nun auf dem &lt;em&gt;Prozess&lt;/em&gt; der kollaborativen Annotation im digitalen Medium.&lt;/p&gt;
&lt;p&gt;Die Praxis des erklärenden Annotierens, d.h. der Anreicherung eines Textes mit Zusatzinformationen um ihn verständlicher zu machen, wird durch die Digitalisierung grundlegend beeinflusst. Während die Erstellung von annotierten Buch-Editionen vor allem Einzelforschern vorbehalten war (und ist), eröffnen sich durch Digitalität neue Möglichkeiten der Generierung von &lt;em&gt;social knowledge&lt;/em&gt;: Viele digitale Editionen bzw. Plattformen (z.B. &lt;em&gt;The Readers‘ Thoreau&lt;/em&gt;, &lt;em&gt;Infinite Ulysses&lt;/em&gt;, &lt;em&gt;PyncheonWiki, Genius.com&lt;/em&gt;) und Tools (z.B. &lt;em&gt;Annotation Studio, A.nnotate, hypothes.is&lt;/em&gt;) ermöglichen es Lesern, zur Erläuterung von Wörtern und Passagen beizutragen. Dadurch löst sich die Grenze zwischen Leser und Annotierendem auf (Sahle 2013: 177, 258). Gleichzeitig gibt es in einer digitalen Edition keine zeitlichen oder räumlichen Einschränkungen, so dass Annotatoren kontinuierlich neue Informationen zu einer Annotation hinzufügen können. Damit ist Digitalität im Sinne des Tagungsthemas auch der Kritik zu unterziehen. Denn sie kann dazu führen, dass endlose Abhandlungen zu jedem einzelnen Wort eines Textes entstehen, was zu Informationsflut, irrelevanten oder unstrukturierten Informationen und infolgedessen zu einem Qualitätsverlust der digitalen Annotation führen kann. Damit laufen digitale erklärende Annotationen Gefahr, den Nutzer bei seinem Anliegen, einen Text zu verstehen, zu verwirren oder in die Irre zu führen (s. Bauer/Zirker 2017b).&lt;/p&gt;
&lt;p&gt;TEASys steuert dem entgegen, indem sowohl der Prozess des Annotierens wie auch der Nutzen der entstandenen Annotationen kritisch reflektiert werden. Dies geschieht in studentischen &lt;em&gt;peer&lt;/em&gt;-&lt;em&gt;learning&lt;/em&gt;-Gruppen und in Lehrveranstaltungen, die häufig den Anstoß für die selbständige Weiterarbeit in den Gruppen geben. In den Lehrveranstaltungen dient das Annotieren als Methode zur Erarbeitung historisch und/oder kulturell distanter Texte ebenso wie zum Erwerb textanalytischer Fähigkeiten. Dabei lernen Studierende, ihre eigene Vorgehensweise zu reflektieren und sie überprüfbar zu machen. TEASys wird als Lehrmethode eingesetzt, indem es Studierende dazu anregt, ihr eigenes Unverständnis eines Textes zu reflektieren: welche Elemente eines Textes tragen dazu bei, dass er ‚schwierig‘ ist (oder so empfunden wird), und welche (Art von) Informationen werden benötigt, um diese Schwierigkeiten zu überwinden? Hier kommt die Heuristik von TEASys ins Spiel: Annotationen sind strukturiert nach Kategorien der Information (z.B. Sprache, Form, Intertextualität etc.; s. Bauer/Zirker 2015) sowie nach Ebenen der Komplexität (insgesamt drei) und zielen damit auf konkrete Leserbedürfnisse (Bauer/Zirker 2017b).&lt;/p&gt;
&lt;p&gt;Die Strukturierung der Annotationen hilft Studierenden somit auch, Fragen zu formulieren, auf die sie andernfalls vielleicht nicht gestoßen wären (z.B. „Konnte das Wort „travel“ Wort im 16. Jahrhundert nicht noch etwas anderes bedeuten?“). Häufig wird dabei deutlich, dass die Erläuterungsbedürftigkeit eines Textes oder von Textteilen oft nur aufgrund eines gewissen Expertenwissens erkannt wird. Dieses wird oftmals von den Lehrenden beigesteuert; aufgrund der Strukturierung können aber auch Studierende leichter zu Experten werden. Die Kombination von Expertise und Kritik führt zu Generierung von &lt;em&gt;social knowledge&lt;/em&gt; durch den Austausch innerhalb der &lt;em&gt;peer&lt;/em&gt;-Gruppen, wodurch in effizienter Weise qualitativ verifizierte Arbeitsergebnisse erzeugt werden (vgl. Jannidis, Kohle, Rehbein 2017: 211). Die Publikation der Annotationen online stellt eine zusätzliche Motivation für die Studenten dar, Output auf hohem fachlichem Niveau zu produzieren (Stroud 2006: 215), sowie ihre Annotation zu verbessern. Die kontinuierliche Weiterarbeit (vgl. Ralle 1016: 147) birgt aber auch Risiken in Hinsicht auf die Verwend- und Zitierbarkeit des kollaborativ entstandenen Wissens.&lt;/p&gt;
&lt;p&gt;Das Poster stellt die einzelnen Lehr- und Lernprozesse des kollaborativen Annotierens dar und soll die Nachhaltigkeit des Projektes und sowie den Mehrwert der Heuristik von TEASys im Hinblick auf digitale Methoden im Literaturunterricht veranschaulichen.&lt;/p&gt;
</p23_abstract>
  <p24_paperID>124</p24_paperID>
  <p24_contribution_type>Poster</p24_contribution_type>
  <p24_acceptance>Akzeptiert</p24_acceptance>
  <p24_authors>Steyer, Timo
Dogunke, Swantje
Mayer, Corinna
Neumann, Katrin
Cremer, Fabian
Wübbena, Thorsten</p24_authors>
  <p24_organisations>Forschungsverbund Marbach Weimar Wolfenbüttel; Herzog August Bibliothek Wolfenbüttel
Forschungsverbund Marbach Weimar Wolfenbüttel; Klassik Stiftung Weimar
Forschungsverbund Marbach Weimar Wolfenbüttel; Deutsches Literaturarchiv Marbach
Deutsches Forum für Kunstgeschichte Paris
Deutsches Forum für Kunstgeschichte Paris
Deutsches Forum für Kunstgeschichte Paris</p24_organisations>
  <p24_emails>steyer@hab.de
swantje.dogunke@klassik-stiftung.de
corinna.mayer@dla-marbach.de
Neumann@MaxWeberStiftung.de
Cremer@MaxWeberStiftung.de
twuebbena@dfk-paris.org</p24_emails>
  <p24_presenting_author>Steyer, Timo
Dogunke, Swantje
Mayer, Corinna
Neumann, Katrin
Cremer, Fabian
Wübbena, Thorsten</p24_presenting_author>
  <p24_title>Peer-to-Peer statt Client-Server: Der Mehrwert kollegialer Beratung und agiler DH-Treffen</p24_title>
  <p24_abstract>&lt;p&gt;Ein wesentliches Kennzeichen der Digital Humanities ist ihr hoher Grad an Interdisziplinarität, Vernetzung und Kommunikation. Angesichts ihrer Schnittstellenfunktion und des schnellen technologischen Wandels ist der regelmäßige Austausch zwischen DH-Vorhaben ebenso unerlässlich wie impulsgebend. Aus diesen Gründen finden seit drei Jahren regelmäßige Treffen zwischen den DH-MitarbeiterInnen des Forschungsverbundes Marbach Weimar Wolfenbüttel und der Max Weber Stiftung statt.[1] Beide Institutionen vereinen strukturelle Gemeinsamkeiten: Als Zusammenschlüsse geografisch verteilter Forschungs- und Infrastruktureinrichtungen, die innerhalb der Geisteswissenschaften unterschiedliche wissenschaftliche Schwerpunkte verfolgen, streben beide Verbünde im Bereich der digital gestützten Forschung nach Synergien und Vernetzung. Die Treffen werden als Peergroup-Treffen auf operativer Ebene nach dem Ansatz der kollegialen Beratung mit wechselnden Rollen und Formaten, wie z.B. Partnerinterviews, Impulsreferate, Buzzgroups oder Think-Pair-Share, durchgeführt.[2]  Ursprünglich begonnen als offener Austausch über Fragen des Aufbaus digitaler Infrastrukturen, des Wissenstransfers von fachlicher Expertise und zu aktuellen DH-Einwicklungen rückten durch den systemischen Ansatz zunehmend andere, unerwartete Themenfelder in den Fokus: Bei den DH-Vorhaben in den Verbünden werden vor allem Projektvorhaben initiiert, bei denen eine Konvergenz zwischen digitalen und analogen Forschungsmethoden angestrebt wird. Hieraus entsteht ein Spannungsverhältnis zu den etablierten Organisationstrukturen der Institutionen, die in ihren Abläufen und Strukturen nicht auf multidisziplinäre Projekte mit einem digitalen Anteil ausgerichtet sind. Bei der Übertragung des traditionellen Organisations- und Projektmanagements auf die DH-Projekte ergeben sich folgende Herausforderungen:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Dissonanz zwischen analogen und digitalen Projektworkflows: Der DH-Anteil in Forschungsprojekten ist nicht selten durch Förderrichtlinien motiviert und viele GeisteswissenschaftlerInnen kommen in diesen Projekten erstmalig in Kontakt zu DH.[3] Technische Möglichkeiten werden dadurch sowohl unter- als auch überschätzt. Zu Projektbeginn existieren daher in den DH-Anteilen häufig nur abstrakte Vorstellungen über die Umsetzung der Projektinhalte in technische Verfahren oder Werkzeuge. Bei DH-Projekten werden  die notwendigen konzeptionellen Arbeiten zu Projektbeginn – etwa die Evaluierung von Software oder die Identifizierung von Best-Practice-Modellen zu Kernaufgaben – nur unzureichend in den Projektplänen berücksichtigt.[4]&lt;/li&gt;
&lt;li&gt;Strukturelle Isolation: In vielen Forschungsprojekten arbeitet häufig nur eine einzelne Person als DHler/in; diese fungiert als singuläre Wissensressource, die schließlich zu den übrigen ProjektmitarbeiterInnen entweder in einem dienstleistungsähnlichen Verhältnis steht oder für die Vermittlung und Legitimation neuartiger digitaler Methoden gegenüber den technisch weniger versierten Kollegen viel Engagement aufbringen muss. Da in Gedächtniseinrichtungen und -organisationen nur wenige bis keine Planstellen für DH vorhanden sind, können die Projektstellen nur bedingt auf institutionelles Wissen und Kompetenzen, z. B. in Bereichen wie Softwareentwicklung oder Datenmodellierung, zurückgreifen.&lt;/li&gt;
&lt;li&gt;Diversität der Aufgaben: Alle technischen Fragen landen in den Forschungsprojekten in der Regel auf dem Tisch der DH. Förderanträge enthalten selten Anforderungsanalysen oder validierte Zielsetzungen bezüglich der DH-Anteile, die sich auch in den überlangen Kompetenzanforderungen widerspiegeln. Die DH-MitarbeiterInnen müssen dadurch Mehrfachfunktionen erfüllen: Einerseits sollen sie Dienstleistungen für Forschungsprojekte erbringen, andererseits eigene Forschungsprojekte durchführen und (implizit) Projektmanagement übernehmen.[5] Durch diese Situation gehen einerseits die Kernkompetenzen der DH, wie die Gestaltung, Moderation und Begleitung von digitalen Prozessen weitgehend unter während der Rechtfertigungsdruck steigt, wenn Anforderungen nicht kurzfristig realisiert werden können.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Basierend auf den ersten Ergebnissen hat die Gruppe konkrete Ziele der Treffen ausgearbeitet. Diese umfassen neben dem eingangs anvisierten Austausch und gegenseitiger Beratung auch Qualitätssicherung, Innovationsleistung und Fortbildung. Neben diesen Funktionen zählen außerdem die Stärkung eigener Entscheidungen durch Verifikation des jeweiligen Partners, durch Impulse beschleunigte Entwicklungspfade und direkter Technologietransfer zu den Resultaten des Kooperationsmodells. Die Peergroup-Treffen können selbstverständlich die genannten Herausforderungen nicht lösen, aber die projektbedingten Dissonanzen der Arbeitspläne werden offengelegt, die strukturelle Isolation zeitweise aufgelöst und die Diversität der Aufgaben durch Wissens-, Erfahrungs- und Technologietransfer erleichtert. Das  Poster soll zum einen die Funktionen und Impulse präsentieren, die sich durch das Peer-to-Peer Format ergeben haben. Ziel ist es, mit anderen KollegInnen ins Gespräch zu kommen und auch unangenehme Fragen zu diskutieren, etwa warum die DH in bestimmten Projektstrukturen primär mit Sensibilisierung und Legitimation beschäftigt sind. Dabei soll im Sinne der Tagung auch kritisch hinterfragt werden, inwieweit die DH nicht auch selber zu einer Wahrnehmung beitragen, die sie in die Rolle eines technischen Dienstleisters oder der digitalen Wollmilchsau drängt. --- [1] Vgl. http://www.mww-forschung.de/digitale-forschungsinfrastruktur/ und  http://www.maxweberstiftung.de/ueber-uns/virtuelle-infrastrukturen.html. [2] Vgl.: Kim-Oliver Tietze: Wirkprozesse und personenbezogene Wirkungen von kollegialer Beratung – Theoretische Entwürfe und empirische Forschung, Wiesbaden 2010. [3] Vgl. Daniel V. Pitti: Designing Sustainable Projects and Publications, in: A Companion to Digital Humanities, hg. v. Susan Schreibman, Ray Siemens, John Unsworth, Oxford 2004, http://www.digitalhumanities.org/companion/. [4] Projektformen für DH bei: Edin Tabak: A Hybrid Model for Managing DH Projects, in: DHQ: Digital Humanities Quarterly, 11.1 (2017). [5] Vgl. Ashley Reed: Managing an established digital humanities project: Principles and practices from the twentieth year of the William Blake archive, in: Digital Humanities Quarterly, 8.1 (2014).&lt;/p&gt;
</p24_abstract>
  <p25_paperID>262</p25_paperID>
  <p25_contribution_type>Poster</p25_contribution_type>
  <p25_acceptance>Akzeptiert</p25_acceptance>
  <p25_authors>Guhr, Svenja
Pannach, Franziska
Ziehe, Stefan
Knauth, Jürgen
Kauf, Carina
Sporleder, Caroline</p25_authors>
  <p25_organisations>Universität Göttingen, Deutschland
Universität Göttingen, Deutschland
Universität Göttingen, Deutschland
Universität Göttingen, Deutschland
Universität Göttingen, Deutschland
Universität Göttingen, Deutschland</p25_organisations>
  <p25_emails>svenjasimone.guhr@stud.uni-goettingen.de
franziska.pannach@stud.uni-goettingen.de
stefan.ziehe@stud.uni-goettingen.de
jknauth@uni-goettingen.de
carina.kauf@stud.uni-goettingen.de
csporled@gwdg.de</p25_emails>
  <p25_presenting_author>Guhr, Svenja
Ziehe, Stefan</p25_presenting_author>
  <p25_title>Chancen und Grenzen von Digitalen Methoden zur Analyse der politischen Meinungsbildung in Sozialen Medien</p25_title>
  <p25_abstract>&lt;p&gt;Soziale Medien spielen weltweit im politischen Meinungsbildungsprozess eine immer wichtigere Rolle. Sowohl Onlineangebote von Zeitungen als auch Twitteraccounts von  Organisationen oder Personen können quasi in Echtzeit über aktuelle Geschehnisse informieren. Die Kommentar- und Replyfunktionen bieten zudem einen digitalen Ort für den öffentlichen Austausch. Damit können auch ‚normale’ Nutzer sozialer Medien ganz gezielt Nachrichten wie auch persönliche Kommentare oder Gerüchte in einem Ausmaß verbreiten wie es im vor-digitalen Zeitalter kaum möglich war. Das Entstehen eines vollkommen neuen digitalen Kommunikationsraumes, der sowohl Grenzen überschreitet als auch potenziell neue Grenzen schafft („Echokammern“) kann zum einen positiv im Sinne einer Demokratisierung öffentlicher Meinungsbildung gewertet werden, birgt aber auch Risiken.&lt;br /&gt;Die Analyse der Rolle von Sozialen Medien im öffentlichen Meinungsbildungsprozess ist inzwischen ein aktives Forschungsfeld. Für eine umfassende Auswertung des Datenmaterials sind jedoch mächtige Analyseverfahren notwendig (Sentimentanalyse, Netzwerkanalyse, Dialog-/Diskursanalyse, Bot-Erkennung etc.), die zur Zeit noch nicht in adäquatem Maß zur Verfügung stehen. So ist zum Beispiel die Sentimentanalyse relativ gut erforscht, beschränkt sich aber in der Regel auf das Englische sowie auf die Textsorte „Rezension“. In drei Pilotstudien haben wir untersucht, (i) inwieweit sich Prozesse der öffentlichen Meinungsbildung in sozialen Medien mit den zur Zeit zur Verfügung stehenden Verfahren nachvollziehen lassen und (ii) wo die Grenzen dieser Verfahren liegen und wie diese im Hinblick auf die Frage nach der Rolle sozialer Medien im Meinungsbildungsprozess weiterentwickelt werden können. Der thematische Fokus liegt dabei auf dem öffentlichen Diskurs im Kontext von nationalen Wahlen. Die Pilotstudien decken dabei zwei verschiedene Sprachen (französisch, deutsch) und Textsorten (Kommentare auf Nachrichtenseiten, Tweets) ab und variieren hinsichtlich der ausgewerteten Datenmenge und Herangehensweise (gemischt qualitativ-quantitative stilistische Auswertung vs. primär quantitative Polaritätsanalyse).&lt;/p&gt;
</p25_abstract>
  <sessionID>60</sessionID>
  <p26_paperID>104</p26_paperID>
  <p26_contribution_type>Poster</p26_contribution_type>
  <p26_acceptance>Akzeptiert</p26_acceptance>
  <p26_authors>Neovesky, Anna
von Vlahovits, Frederic</p26_authors>
  <p26_organisations>Akademie der Wissenschaften und der Literatur | Mainz
Akademie der Wissenschaften und der Literatur | Mainz</p26_organisations>
  <p26_emails>Anna.Neovesky@adwmainz.de
Frederic.vonVlahovits@adwmainz.de</p26_emails>
  <p26_presenting_author>Neovesky, Anna
von Vlahovits, Frederic</p26_presenting_author>
  <p26_title>IncipitSearch - Vernetzung musikwissenschaftlicher Vorhaben</p26_title>
  <p26_abstract>&lt;p dir="ltr"&gt;Die digitalen Musikwissenschaften konzentrieren sich bis dato vor allem auf die Erstellung  digitaler Editionen sowie die Entwicklung von Werkzeugen und Standards für deren Erarbeitung und Publikation. Während die Daten vieler Edition mittlerweile nachhaltig und nachnutzbar zur Verfügung gestellt werden, mangelt es noch an übergreifenden und vernetzenden Datenaggregatoren, die die musikwissenschaftlichen Vorhaben vernetzen. Anders gesagt: Vor lauter Edieren, hat man sich noch zu wenig auf die Zusammenführung der bereits existenten Daten konzentriert. Gerade das wäre jedoch mithilfe der über die Summe der recht heterogenen Datenbestände hinweg normalisierbaren Metadaten gut möglich.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Die IncipitSearch der Akademie der Wissenschaften und der Literatur | Mainz setzt genau hier an, indem sie musikalische Incipits durchsuchbar macht. Da Incipits einen pragmatischen Ansatz zur Katalogisierung von notierter Musik darstellen, bei dem wenige Anfangstakte einer Partitur transkribiert werden, lassen sich hierüber Editionen, Werkverzeichnisse und Quellenkataloge digitalen wie gedruckten Ursprungs zusammenführen. Ziel eines solchen Ansatzes ist nicht nur die Möglichkeit einer repositorienübergreifenden Suche, denn es lassen sich mithilfe von Incipits sehr wohl auch rudimentäre Aussagen über Spezifika der jeweils referenzierten Musik treffen. Ein erster vergleichender analytischer Blick auf eine breite Datengrundlage ist also ebenfalls möglich.&lt;/p&gt;
&lt;p dir="ltr"&gt;In erster Linie versteht sich die IncipitSearch jedoch als ein Pendant zu konventionellen thematisch-bibliographischen Katalogen, wie sie in gedruckter Form schon seit Jahrhunderten vorgelegt werden. Man stelle sich vor, man sucht eine Sonate in C auf einem bestimmten Anfangston oder mit einer bestimmten Anfangsmelodie, ohne den Komponisten zu kennen. Nur allein mit dem Titel „Sonate in C“ dürfte man eine unüberschaubare Anzahl an Treffern erzielen. Mittels einer simplen Eingabe der Anfangstöne auf einer Klaviatur jedoch, lässt sich das gesuchte Stück leicht eruieren. Mit RISM hat man ja bereits ein hervorragendes Beispiel für einen solchen Suchansatz, der mit Notenincipits arbeitet . Gerade eine Zusammenführung verschiedener  Datenrepositorien – von Digitalen Werkverzeichnisse, Editionen über Quellenlexika und -sammlungen –  wäre mehr als wünschenswert.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Eine wichtige Voraussetzung für den Erfolg eines solchen Unterfangens ist sowohl die freie Verfügbarkeit der Daten auf Seite der Datenlieferanten als auch die freie Verfügbarkeit der Software auf Seite der Anbieter des Dienstes. Die IncipitSearch ist somit neben ihrer bibliographischen Funktion gleichsam auch ein an die musikwissenschaftlichen Fachcommunity gerichteter Aufruf, ihre Daten nach einem einheitlichen Schema offen zu legen, um Sie für eine Vernetzung in diesem wie auch in weiteren Diensten nachnutzbar zu machen. Dafür wird auf einen bewährten Standard für die Codierung der Incipits gesetzt, nämlich Plaine &amp; Easie Code. Es handelt sich um eine simple, schlanke Transkriptionssprache, die von der International Association of Music Libraries, Archives and Documentation Centres kuratiert wird.&lt;/p&gt;
&lt;p dir="ltr"&gt;Der Dienst IncipitSearch selbst ist als Microservice konzipiert und in Gänze sowie bezogen auf seine einzelnen kapselbaren Bestandteile selbst nachnutzbar. Sein Quellcode ist vollständig MIT-lizensiert offen gelegt auf Github verfügbar. Das, eine gute Dokumentation und ein zukünftiges Workshop-Angebot sollen die Zugangsschwelle für potenzielle Nutzer, vor allem seitens der Datenhalter so niedrig wie möglich legen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Mit der IncipitSearch wird den best practices moderner DH folgend, das Potenzial einer Nische genutzt, indem zahlreiche große und kleine, bisher weitestgehend als Inseln dastehende Datenrepositorien der Musik mittels kompakt-gekapselter Software, einer Datenschnittstelle und eines simplen Transkriptionsstandards von Noten-Incipts zusammengeführt werden. Bei der Implementierung weiterer Repositorien setzen wir „community driven“ bewusst auf einen bottom-up-Ansatz, um die Akzeptanz des Dienstes zu stärken und auf Bedürfnisse aus der Community angemessen reagieren zu können. Das Kernpotenzial der Anwendung liegt aber auch in seinen Anforderungen an die Community: Mehr Mut Datentransparenz und mehr Mut zur Vernetzung.&lt;/p&gt;
</p26_abstract>
  <p27_paperID>209</p27_paperID>
  <p27_contribution_type>Poster</p27_contribution_type>
  <p27_acceptance>Akzeptiert</p27_acceptance>
  <p27_authors>Adelmann, Benedikt
Andresen, Melanie
Begerow, Anke
Gaidys, Uta
Gius, Evelyn
Koch, Gertraud
Menzel, Wolfgang
Orth, Dominik
Topp, Sebastian
Vauth, Michael
Zinsmeister, Heike</p27_authors>
  <p27_organisations>Universität Hamburg
Universität Hamburg
Hochschule für angewandte Wissenschaften, Hamburg
Hochschule für angewandte Wissenschaften, Hamburg
Universität Hamburg
Universität Hamburg
Universität Hamburg
Bergische Universität Wuppertal
Universität Hamburg
Technische Universität Hamburg
Universität Hamburg</p27_organisations>
  <p27_emails>adelmann@informatik.uni-hamburg.de
Melanie.Andresen@uni-hamburg.de
anke.begerow@haw-hamburg.de
uta.gaidys@haw-hamburg.de
evelyn.gius@uni-hamburg.de
gertraud.koch@uni-hamburg.de
menzel@informatik.uni-hamburg.de
dominik.orth@uni-wuppertal.de
sebastian.topp@uni-hamburg.de
michael.vauth@tuhh.de
heike.zinsmeister@uni-hamburg.de</p27_emails>
  <p27_presenting_author>Andresen, Melanie
Gius, Evelyn</p27_presenting_author>
  <p27_title>hermA. Zur Rolle von Annotationen in hermeneutischen Prozessen</p27_title>
  <p27_abstract>&lt;p&gt;Im Forschungsverbund „Automatisierte Modellierung hermeneutischer Prozesse“ (hermA) geht es um die Frage, ob und inwieweit hermeneutisches Arbeiten im Bereich der sozial- und geisteswissenschaftlichen Textanalyse computergestützt automatisiert werden kann. Das Poster stellt erste Ergebnisse aus dieser Zusammenarbeit vor.&lt;/p&gt;
&lt;p&gt;Anhand einer Darstellung der fünf Teilprojekte aus Literaturwissenschaft, Pflegewissenschaft, Kulturanthropologie, Computerlinguistik und Informatik und ihres Forschungszugangs (deduktiv, induktiv und/oder abduktiv) wird die Rolle von manuellen und automatischen Annotationen im Kontext hermeneutischer Fragestellungen betrachtet.&lt;/p&gt;
</p27_abstract>
  <p28_paperID>131</p28_paperID>
  <p28_contribution_type>Poster</p28_contribution_type>
  <p28_acceptance>Akzeptiert</p28_acceptance>
  <p28_authors>Kessler, Linda
Braun, Tamara
Preuß, Tanja</p28_authors>
  <p28_organisations>Universität Stuttgart, Deutschland
Universität Stuttgart, Deutschland
Universität Stuttgart, Deutschland</p28_organisations>
  <p28_emails>st150918@stud.uni-stuttgart.de
st151509@stud.uni-stuttgart.de
st102459@stud.uni-stuttgart.de</p28_emails>
  <p28_presenting_author>Kessler, Linda
Braun, Tamara
Preuß, Tanja</p28_presenting_author>
  <p28_title>Entitäten im Fokus am Beispiel von Captivity Narratives</p28_title>
  <p28_abstract>&lt;p&gt;Eigennamenerkennung (NER) ist im Bereich der maschinellen Sprachverarbeitung bereits&lt;strong&gt;&lt;/strong&gt;viel behandelt  worden. Eine Übersicht hierzu findet sich bei Nadeau und Sekine (2007). In den Digital Humanities dient die Erkennung von benannten Entitäten der Identifikation zentraler Akteure und Elemente in Texten, welche unter anderem die Grundlage für tiefergehende Analysen bezüglich Beziehungen, Strukturen und Emotionen in diesen Texten bilden. Jannidis et al. (2015) thematisieren allerdings, dass die reine NER  beispielsweise für eine Analyse von Figurennetzwerken in literarischen Texten unzureichend ist, da dabei nur Figurenreferenzen durch konkrete Namensnennung erfasst werden. Um spezifisch auf die Bedürfnisse von Textanalysen im Kontext der Digital Humanities einzugehen, wurden im „Center for Reflected Text Analytics“ (CRETA) (Kuhn et al. 2016) der Universität Stuttgart Annotationsrichtlinien entworfen, die über die Annotation reiner Eigennamen hinausgehen und sich auf verschiedenartige Entitätsreferenzen in deutschsprachigen Texten unterschiedlicher Genres fokussieren. So wird beispielsweise das Appellativ &lt;em&gt;the indians&lt;/em&gt; als Entität erfasst, obwohl die Referenz nicht mit Namen spezifiziert wird. Ein Bespiel für den Mehrwert der Annotation solcher Entitätsreferenzen findet sich bei Blessing et al. (2017). Um die Übertragbarkeit der Richtlinien nicht nur zwischen verschiedenen Textsorten, sondern auch sprachübergreifend zu evaluieren, stellen wir unser Projekt mit dem Ziel der Annotation von Erzähltexten in englischer Sprache vor. Ausgehend von der durch CRETA geschaffenen Grundlage teilt sich unser Projekt in drei Phasen auf: die manuelle Annotation und Überprüfung der Übertragbarkeit der CRETA-Richtlinien auf die gegebene Textsorte, die Automatisierung der Entitätserkennung und die Einbindung der Entitäten in eine literaturwissenschaftliche Analyse.&lt;/p&gt;
&lt;p&gt;Als Textgrundlage dient eine Sammlung von englischsprachigen Captivity Narratives. Diese Erzählungen aus dem 18. Jahrhundert handeln von Erfahrungen weißer Siedler in Nordamerika, die in indianische Gefangenschaft geraten. Zunächst wurden in sieben Texten im Gesamtumfang von 71.526 Wörtern 5.163 Entitäten identifiziert und mit den von CRETA erarbeiteten Kategorien (Personen, Orte, Organisationen, Ereignisse, Werke und abstrakte Konzepte) annotiert. Im Verlauf dieser Annotationsphase wurden die CRETA Richtlinien an die speziellen Gegebenheiten der Textsorte angepasst, die in Bezug auf die Erwähnung von Personen und Orten einige Besonderheiten aufweist. Auffällig ist beispielsweise, dass Personen in vielen Fällen in Gruppen, oftmals auch ohne spezifische Namen, erwähnt werden. Um diese Nennungen dennoch zu erfassen, wird die  Begrenzung auf Eigen- und Gattungsnamen aufgehoben und um Formen wie &lt;em&gt;some, others &lt;/em&gt;oder &lt;em&gt;a few &lt;/em&gt;erweitert. Zudem werden Orte häufig anhand von Landschaftsmerkmalen und nur selten mit konkreten Ortsnamen benannt. Dementsprechend bilden solche Nennungen (z.B. &lt;em&gt;the river &lt;/em&gt;oder&lt;em&gt; the mountain&lt;/em&gt;) den Großteil der annotierten Ortsentitäten. Die Erfassung von vollständigen Nominalphrasen als Entitäten erweist sich stellenweise als problematisch, da die Captivity Narratives verschachtelte Nominalphrasen enthalten, sodass sehr umfangreiche Entitäten zu annotieren sind.&lt;/p&gt;
&lt;p&gt;Der so entstandene Goldstandard dient als Trainingsdatensatz zur Entwicklung eines maschinellen Lernverfahrens. Ein Naive Bayes Classifier wurde mit Features trainiert, die sich u.a. auf die äußere Gestalt (z.B. Großschreibung), die Wortart und die  Zugehörigkeit zu Wortlisten (Namen und amerikanische Orte) beziehen. Im Kreuzvalidierungsverfahren kann damit ein Micro-Fscore von 0,29 erzielt werden. Für die am häufigsten im Trainingsmaterial vorhandene Klasse PER wurde ein Precision-Wert von 0,45 erzielt. Dies bedeutet, dass fast die Hälfte der automatisch mit PER annotierten Entitäten wirklich Personen sind. Der Recall von 0,3 zeigt, wie unvollständig die Erkennung mit einem knappen Drittel aller relevanten Personen noch ist. Eine Auswertung der Ergebnisse zeigt, dass die Länge und Verschachtelung vieler Entitäten die automatische Klassifizierung erschwert. Da sich im manuellen Annotationsprozess der Kontext häufig als Entscheidungshilfe herausstellte, sollte dieser bei der automatischen NER zukünftig berücksichtigt werden. Darüber hinaus könnte die Erweiterung der verwendeten Features durch syntaktische Informationen und die Verwendung einer größeren Menge an Trainingsdaten zu Verbesserungen führen.&lt;/p&gt;
&lt;p&gt;Um den Mehrwert der Entitätsreferenzen für eine inhaltliche Fragestellung bezüglich der Captivity Narratives zu veranschaulichen, zeigen wir die textstatistische Analyse von Emotionen im Umfeld bestimmter Entitäten bzw. Entitätsgruppen. Basierend auf den manuell annotierten Texten, lassen sich Personenentitäten mithilfe von Clusteranalysen gruppieren. Anhand von positiven und negativen Wortlisten lassen sich zwei Gruppen bilden, die sich grob als &lt;em&gt;Indianer&lt;/em&gt; und &lt;em&gt;Andere&lt;/em&gt; gegenüber stehen. Eine auf denselben Wortlisten basierende Sentiment Analyse ergab einen deutlich negativeren Emotionswert für Personenentitäten, die der Gruppe der Indianer zuzuordnen sind, als für die Gruppe der anderen Personen.&lt;/p&gt;
&lt;p&gt;Abschließend lässt sich festhalten, dass auf Grundlage unserer Annotationen eine Abgrenzung der im Text auftretenden Gruppen anhand von emotionsgeladenen Wörtern möglich ist, die der erwarteten negativen Haltung der Verfasser gegenüber den Eingeborenen Nordamerikas entspricht.&lt;/p&gt;
&lt;p&gt;Die von CRETA entwickelten Annotationsrichtlinien sind grundsätzlich auf die von uns analysierten Texte anwendbar, trotz abweichender Sprache und spezifischer Erzählweise. Um die Breite der enthaltenen Entitätsreferenzen vollständig abbilden zu können, bedarf es allerdings einzelner Spezifizierungen der Annotationsrichtlinien für diese Textsorte.&lt;/p&gt;
</p28_abstract>
  <p29_paperID>180</p29_paperID>
  <p29_contribution_type>Poster</p29_contribution_type>
  <p29_acceptance>Akzeptiert</p29_acceptance>
  <p29_authors>Gerhards, Simone
Gülden, Svenja A.
Konrad, Tobias
Leuk, Michael
Verhoeven-van Elsbergen, Ursula
Rapp, Andrea</p29_authors>
  <p29_organisations>Johannes Gutenberg-Universität Mainz, Deutschland; Akademie der Wissenschaften und der Literatur Mainz, Deutschland
Johannes Gutenberg-Universität Mainz, Deutschland; Technische Universität Darmstadt, Deutschland; Akademie der Wissenschaften und der Literatur Mainz, Deutschland
Johannes Gutenberg-Universität Mainz, Deutschland; Technische Universität Darmstadt, Deutschland; Akademie der Wissenschaften und der Literatur Mainz, Deutschland
Akademie der Wissenschaften und der Literatur Mainz, Deutschland
Johannes Gutenberg-Universität Mainz, Deutschland; Akademie der Wissenschaften und der Literatur Mainz, Deutschland
Technische Universität Darmstadt, Deutschland; Akademie der Wissenschaften und der Literatur Mainz, Deutschland</p29_organisations>
  <p29_emails>gerhards@uni-mainz.de
sguelden@uni-mainz.de
tokonrad@uni-mainz.de
michael.leuk@adwmainz.de
verhoeve@uni-mainz.de
rapp@linglit.tu-darmstadt.de</p29_emails>
  <p29_presenting_author>Gerhards, Simone
Gülden, Svenja A.
Konrad, Tobias</p29_presenting_author>
  <p29_title>Aus erster Hand – 3000 Jahre Kursivschrift der Pharaonenzeit digital analysiert</p29_title>
  <p29_abstract>&lt;p&gt;Das Projekt &lt;em&gt;Altägyptische Kursivschriften&lt;/em&gt; (AKU 2015) an der Akademie der Wissenschaften und der Literatur Mainz unter der Leitung von Prof. Dr. Ursula Verhoeven-van Elsbergen (JGU Mainz) in Kooperation mit Prof. Dr. Andrea Rapp (TU Darmstadt) besteht seit April 2015. Ziel ist es, in verschiedenen Modulen im Verlauf von maximal 23 Jahren eine digitale Paläographie zum Hieratischen und zu den Kursivhieroglyphen zu erstellen sowie verschiedene Aspekte der Kursivschrift-Kultur systematisch unter Einbeziehung digitaler Methoden zu untersuchen.&lt;/p&gt;
&lt;p&gt;Im Alten Ägypten gab es neben den monumentalen und detailliert ausgeführten Hieroglyphen auch kursive (Hand-)Schriften, die als Hieratisch, Kursivhieroglyphen, Kursivhieratisch und Demotisch bezeichnet werden. Sie wurden mit Pflanzenstengeln und Rußtusche auf Papyrus, Leinen, Leder, Holz, Ton oder Stein geschrieben oder eingeritzt. Die Kursivschriften spielten unter Gelehrten, Priestern, Beamten und Schreibern eine wesentliche Rolle in den Bereichen der Kommunikation und Verwaltung, aber auch der Dichtung, Wissensgebiete sowie religiösen und funerären Texte. Das Hieratische war über 3000 Jahre lang in Gebrauch und wurde von den Schülern als erste Schriftart noch vor den Hieroglyphen erlernt.&lt;/p&gt;
&lt;p&gt;Bis heute ist die knapp 100 Jahre alte &lt;em&gt;Hieratische Paläographie&lt;/em&gt; von Georg Möller das Standardwerk für die ägyptologische paläographische Forschung (Möller 1909–1912). Er hat aus nur 32 gut datierten Schriftzeugnissen (vor allem Papyri) alle identifizierbaren Einzelzeichen faksimiliert und in übersichtlichen Listen erfasst, die die Zeitspanne von der 5. Dynastie (ca. 2500 v. Chr.) bis zur römischen Kaiserzeit (3. Jh. n. Chr.) abdecken; die drei Bände bestehen aber zusammengenommen aus nur etwa 220 Seiten. Da er für manche Epochen nur sehr wenige oder gar keine Schriftquellen zur Verfügung hatte, sind einige Zeiträume nicht oder nur unzureichend dokumentiert. Möller selbst betrachtete diese Listen als Vorarbeiten für weitergehende Untersuchungen, was er aber aufgrund seines frühen Todes nicht realisieren konnte. Erst ca. 70 Jahre später formulierte Posener (1973) seine Anforderungen an eine zukünftige paläographische Forschung und einen &lt;em&gt;nouveau Möller&lt;/em&gt;. Mit den damaligen technischen Voraussetzungen hätten die komplexen Anforderungen in Verbindung mit der Materialfülle selbst von einer Forschergruppe nicht erfüllt werden können. So erklären sich die zahlreichen Teilpaläographien, die in den nachfolgenden Jahrzehnten im Rahmen ägyptologisch-paläographischer Forschung entstanden sind (z. B. Goedicke 1988, Verhoeven 2001, Allen 2002, Lenzo 2011). Diese halten sich bis heute an das Prinzip von Möller, ordnen die Zeichen allerdings nach der Standardliste (&lt;em&gt;Sign-list&lt;/em&gt;), die Gardiner in seiner &lt;em&gt;Egyptian Grammar&lt;/em&gt; (Gardiner 1927, &lt;sup&gt;3&lt;/sup&gt;1973: 438-548) publiziert hat. Da bei Gardiner aber nicht alle hieroglyphischen Entsprechungen zu den &lt;em&gt;Hieratogrammen&lt;/em&gt; (Verhoeven 2001: 1) zu finden sind, kam es in den verschiedenen Teilpaläographien zu diversen Erweiterungen, die keinem einheitlichen Prinzip folgen und somit nicht eindeutig referenzierbar sind (Gülden 2016: 3).&lt;/p&gt;
&lt;p&gt;Digitale Ansätze zur Analyse von Handschriften beschreiben beispielsweise Stokes (2009) für das europäische Mittelalter und Quirke (2011) für die hieratische Schrift des Alten Ägypten. Das AKU-Projekt entwickelt erstmals eine digitale Paläographiedatenbank, in der nach und nach das gesamte Zeichenrepertoire der altägyptischen Kursivschriften erfasst wird: ca. 600 Einzelzeichen (sowohl Laut- als auch Deutzeichen), Zahlen, Maße und Korrekturzeichen sowie Ligaturen, Zeichengruppen und besondere Orthographien (Verhoeven 2015: 32). Für die Auswertung des Datenmaterials sind zudem umfassende Metadaten der Schriftträger (z. B. Herkunft, Datierung, Genre, Materialität, Beschreibstoffe und Schreibgerät) notwendig, die ebenfalls in der Datenbank erfasst werden (Gülden, Krause, Verhoeven 2017 und im Druck). Somit wird die Analyse verschiedener Aspekte ermöglicht, z. B. die Entwicklungen und Diversität der Kursiven, die Bezüge zur Hieroglyphenschrift sowie kontextuelle und funktionelle Anpassungen. Interessant sind auch Fragen zu Schriftökonomie, Schreibrichtung, Layout, Abkürzungen, Diakritika und Ligaturen. Außerdem können Datierungen, Schreiberpersonen und regionale Unterschiede ausgemacht werden.&lt;/p&gt;
&lt;p&gt;Zunächst werden die einzelnen Schriftzeichen (&lt;em&gt;Hieratogramme&lt;/em&gt;) auf der Basis hochauflösender Digitalisate der Textträger digital faksimiliert (umgezeichnet). Diese werden sowohl als Vektor- und Rastergrafiken gespeichert, um die digitale Auswertung in unterschiedlichen Tools möglich zu machen. In der Datenbank, die in den nächsten Jahren als&lt;em&gt; open access online tool&lt;/em&gt; zur Verfügung stehen soll, werden sie kategorisiert und mit Metadaten versehen. Für ein langfristiges Repositorium, aber auch für Auswertungen, Visualisierungen und &lt;em&gt;linked open data&lt;/em&gt;, sollen die Daten in verschiedene Formate übertragen werden, z. B. in ein TEI konformes XML-Schema und als csv-Files.&lt;/p&gt;
&lt;p&gt;Während für alphabetische Schriften bereits zahlreiche Vorarbeiten im Bereich der Handschriftenerkennung vorliegen, muss dies für die Handschrift des Alten Ägypten erst entwickelt werden, um eine Grundlage für automatisierte Prozesse bei der Zeichenerkennung, -erfassung und -auswertung dieser komplexen Schrift zu ermöglichen.&lt;/p&gt;
</p29_abstract>
  <p30_paperID>150</p30_paperID>
  <p30_contribution_type>Poster</p30_contribution_type>
  <p30_acceptance>Akzeptiert</p30_acceptance>
  <p30_authors>Faynberg, Veronika
Fischer, Frank
Lashchuk, Svetlana
Orlova, Tatyana
Palchikov, German
Shlosman, Evgenia</p30_authors>
  <p30_organisations>Higher School of Economics, Moskau, Russland
Higher School of Economics, Moskau, Russland
Higher School of Economics, Moskau, Russland
Higher School of Economics, Moskau, Russland
Higher School of Economics, Moskau, Russland
Higher School of Economics, Moskau, Russland</p30_organisations>
  <p30_emails>berenis0102@gmail.com
frafis@gmail.com
svetalashch@gmail.com
taorkon.tootta@gmail.com
rebel368@gmail.com
zhenya96@gmail.com</p30_emails>
  <p30_presenting_author>Lashchuk, Svetlana</p30_presenting_author>
  <p30_title>Netzwerkanalytischer Blick auf die Dramen Anton Tschechows</p30_title>
  <p30_abstract>&lt;p dir="ltr"&gt;Die literarische Netzwerkanalyse hat sich in den letzten Jahren zu einer gefragten Methode der digitalen Literaturwissenschaft entwickelt. Dabei rangiert die Größe der Arbeitskorpora im Sinne des »scalable reading« (Martin Mueller) von der Betrachtung von Einzeltexten (Schweizer/Schnegg 1998, Moretti 2011) über kleinere Korpora bis hin zur Untersuchung hunderter oder gar tausender Dramen (Fischer u. a. 2016, Trilcke u. a. 2016, Algee-Hewitt 2017). Dabei zeigt sich auch immer wieder Interesse an bestimmten, etwa autorzentrierten Subkorpora (Wade 2017).&lt;/p&gt;
&lt;p dir="ltr"&gt;In diesem Kontext siedelt sich auch unser Posterprojekt an, in dessen Mittelpunkt die extrahierten Netzwerkdaten zu den Stücken des russischen Dramatikers Anton Tschechow (1860–1904) stehen. Die Datengrundlage bildet das von uns aufgebaute und betriebene Russian Drama Corpus (RusDraCor), das es sich zur Aufgabe gestellt hat, russischsprachige Stücke in der Zeitspanne zwischen den 1740er-Jahren (Sumarokow, Lomonossow u. a.) und den 1930er-Jahren (mit Texten von Autoren wie Majakowski oder Gorki) im TEI-Format zur Verfügung zu stellen (Fischer u. a. 2017). Neben Large-Scale-Analysen zur strukturellen Evolution des russischen Dramas ergibt sich so auch die Möglichkeit zur Betrachtung von nach verschiedenen Kriterien portionierten Teilkorpora, etwa der Stücke einzelner Autoren.&lt;/p&gt;
&lt;p dir="ltr"&gt;Anton Tschechow gehört zu den meistgespielten russischen Dramatikern, dessen Werke bis heute inszeniert werden, gerade auch an deutschsprachigen Bühnen, vor allem seine vier letzten Stücke, »Die Möwe«, »Onkel Wanja«, »Drei Schwestern« und »Der Kirschgarten«. Von der Figurenkonstellation her haben diese Werke einen hohen Wiedererkennungswert: Es gibt keine wirklichen Protagonisten; die Redeanteile und Gesprächssituationen sind relativ gleichmäßig über eine Gruppe von Figuren verteilt. Dies zeigt sich sofort auch in den Netzwerkgraphen: Die Knoten (von denen jeder für eine Figur des jeweiligen Dramas steht) bilden einen character space, der bei der Visualisierung einem Polyeder gleicht. Die einzigen Figuren, die nicht am gemeinsamen Gesprächskreis teilhaben, sind die Diener und sonstige Gehilfen, deren Redeanteile sich auf Dialoge mit ihren direkten Weisungsbefugten beschränken. Dieses Sichtbarwerden der sozialen Zweiteilung des Dramenpersonals ist eine der Leistungen der Netzwerkvisualisierung. Zieht man die Werkchronologie als Größe hinzu, wird außerdem deutlich, wie sich die für Tschechow typischen Personenkonstellationen allmählich herausbilden, ab den frühen Stücken »An der Landstraße« (1884), »Iwanow« (1887) und »Der Waldteufel« (1889), über mehrere Kurzdramen oder Etüden wie »Der Bär« (1888), »Tragödie wider Willen« (1889) oder »Das Jubiläum« (1891), bis 1895 mit der »Möwe« die typische Tschechow’sche Charakterkonstellation gefunden ist.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Beschaffenheit des Russian Drama Corpus erlaubt es, quantitative Analysen auch zugeschnitten auf bestimmte Figurengruppen zu beschränken, etwa gesondert nach Geschlecht oder sozialem Status. Bereits eine simple Worthäufigkeitsanalyse kann so etwa zeigen, dass weibliche und männliche Rollen in Tschechow-Stücken von den Redeanteilen und dem Vernetzungsgrad her vergleichbar sind (anders als etwa bei allen anderen Autoren im Korpus). Diese Verteilungsdiagramme sowie netzwerktheoretische Werte wie Dichte, Diameter, Clustering-Koeffizient und Average Path Length ergänzen die chronologisch sortierten Netzwerkvisualisierungen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die im Poster geschaffene Übersicht über alle Tschechow-Dramen hat auch enzyklopädischen Charakter, enthält sie doch etwa alle Figuren im Kontext ihres Auftretens im Tschechow’schen Dramenkosmos. Der netzwerkanalytische Blick ist somit durchaus geeignet, als Brücke zur inhaltlichen Auseinandersetzung mit den Werken Tschechows zu dienen.&lt;/p&gt;
</p30_abstract>
  <sessionID>60</sessionID>
  <p31_paperID>272</p31_paperID>
  <p31_contribution_type>Poster</p31_contribution_type>
  <p31_acceptance>Akzeptiert</p31_acceptance>
  <p31_authors>Simmler, Severin
Vitt, Thorsten
Pielström, Steffen</p31_authors>
  <p31_organisations>Universität Würzburg, Deutschland
Universität Würzburg, Deutschland
Universität Würzburg, Deutschland</p31_organisations>
  <p31_emails>severin.simmler@stud-mail.uni-wuerzburg.de
thorsten.vitt@uni-wuerzburg.de
pielstroem@biozentrum.uni-wuerzburg.de</p31_emails>
  <p31_presenting_author>Simmler, Severin
Pielström, Steffen</p31_presenting_author>
  <p31_title>LDA Topic Modeling über ein graphisches Interface</p31_title>
  <p31_abstract>&lt;p dir="ltr"&gt;LDA (Latent Dirichlet Allocation) Topic Modeling ist ein computergestütztes Verfahren zur semantischen Analyse digitaler Textsammlungen. Hierbei werden mit Hilfe eines probabilistischen Verfahrens aus Texten eine Reihe sogenannter “Topics” generiert: Gruppen semantisch ähnlicher Begriffe, die über mehrere Texte gemeinsam auftreten und im Modell als Wahrscheinlichkeitsverteilungen über die Gesamtheit des analysierten Vokabulars repräsentiert werden. Das heißt, daß zum Beispiel in einem Topic zum Thema Seefahrt nautische Begriffe besonders hohe Wahrscheinlichkeiten haben (Blei 2012, Steyvers und Griffiths 2006). &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;In den letzten Jahren ist das Interesse an LDA als Verfahren für die Analyse literarischer Textcorpora auf Seiten der digitalen Geisteswissenschaften stark gestiegen. Im Kontrast zu diesem gesteigerten Interesse ist die Anwendung der Methode allerdings nicht wesentlich leichter geworden. Gängige Implementierungen des LDA-Algorithmus werden entweder über ein kommandozeilenbasiertes Java-Programm (MALLET von McCallum 2002) oder über Skripte in der Programmiersprache Python (Gensim von Rehurek und Sojka 2010) angesprochen. Die Aufbereitung der Daten vor dem Topic Modeling, das sog. “Preprocessing” und die Analyse der Ergebnisse hinterher geschieht dann zumindest in Teilen häufig unter Verwendung weiterer Programme bzw. Arbeitsumgebungen. Alles in allem erfordert die Durchführung einer LDA-basierten Inhaltsanalyse damit zur Zeit relativ umfangreiche technische Kenntnisse.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Um den Zugang zu dieser Methode zu erleichtern entwickeln wir im Rahmen von DARIAH-DE (https://de.dariah.eu/) zur Zeit eine ausführlich dokumentierte Python-Programmbibliothek, die es ermöglichen soll, den gesamten Arbeitsprozess einer LDA-basierten Analyse in einer einzigen Umgebung durchzuführen (https://github.com/DARIAH-DE/Topics). Neben der Schaffung integrierter, flexibler Arbeitsabläufe, die vollständig in einer Programmiersprache und Umgebung stattfinden können,  wollen wir auch Forscherinnen und Forschern ohne vorherige Programmierkenntnisse eine Möglichkeit zu bieten, Topic Modeling als Vefahren kennen zu lernen und an eigenen Daten auszuprobieren.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Um einen leichtgewichtigen Einstieg in diese Thematik zu bieten haben wir auf Basis unserer Programmbibliothek, der Python-nativen LDA-Implementierung von Allan Riddell (https://pypi.python.org/pypi/lda) und dem Python-Microframework “Flask” (http://flask.pocoo.org/) einen sogenannten GUI-Demonstrator entwickelt (Abb. 1). Dabei handelt es sich um eine browserbasierte graphische Benutzeroberfläche für die DARIAH-Topics Bibliothek, mit der sich ein basaler Topic-Modeling Analysevorgang lokal, mit eigenen Daten, aber eben ohne jegliche Programmierkenntnisse durchführen lässt.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Der GUI-Demonstrator übernimmt und erklärt hierbei exemplarisch alle Arbeitsschritte einer einfachen Analyse. Zunächst werden Textdateien über ein Auswahlmenü eingelesen und tokenisiert. Nutzerinnen und Nutzer können zur Reduktion des Vokabulars auf die Funktionswörter vorgeben, wie viele der häufigsten Wörter aus den Texten entfernt werden sollen, oder alternativ über ein weiteres Auswahlmenü eine externe Stopwortliste einbinden. Die Anzahl der zu berechnenden Topics und die Zahl der Iterationen, über die die Berechnung durchgeführt werden soll, ein Faktor, der die Qualität der Ergebnisse entscheidend beeinflusst, können ebenfalls über das Interface gesteuert werden. In der derzeitigen Form generiert das Programm als Output eine Tabelle mit den zehn am stärksten gewichteten Wörtern in jedem Topic, sowie ein Heatmap als Übersicht über die Verteilung der Topics über die Texte.&lt;/p&gt;
&lt;p dir="ltr"&gt;Im Fokus der gegenwärtigen Weiterentwicklung steht die Gestaltung interaktiver Outputs mit Hilfe von Bokeh (https://bokeh.pydata.org/), die einen flexibleren Zugriff auf eine größere Zahl von Aspekten der Modellierungsergebnisse ermöglichen sollen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p dir="ltr"&gt;Das Ziel dieser Entwicklung bleibt aber in erster Linie ein didaktisches: Der GUI-Demonstrator führt die  grundsätzlichen Möglichkeiten der Methode vor und informiert gleichzeitig über die Abläufe im Hintergrund, so dass der Schritt hin zur Verwendung der gleichen Funktionalitäten in einem vorbereiteten Jupyter Notebook, das schnell an die spezifischen Bedürfnisse eine bestimmten Forschungsfrage angepasst werden kann, nur noch klein ist.&lt;/p&gt;
</p31_abstract>
  <p32_paperID>134</p32_paperID>
  <p32_contribution_type>Poster</p32_contribution_type>
  <p32_acceptance>Akzeptiert</p32_acceptance>
  <p32_authors>Blessing, Andre
Kuczera, Andreas</p32_authors>
  <p32_organisations>Universität Stuttgart, Deutschland
Akademie der Wissenschaften und der Literatur, Mainz</p32_organisations>
  <p32_emails>andre.blessing@ims.uni-stuttgart.de
andreas.kuczera@adwmainz.de</p32_emails>
  <p32_presenting_author>Blessing, Andre
Kuczera, Andreas</p32_presenting_author>
  <p32_title>NLP meets RegNLP meets Regesta Imperii</p32_title>
  <p32_abstract>&lt;p&gt;Dieser Posterbeitrag veranschaulicht die Interaktion zwischen computerlinguistischen Methoden und Regestenforschung. Es wird eine Anwendung vorgestellt, die bereits in einem graphbasierten Format vorliegenden Regesten webbasiert anzeigt und es erlaubt, Registereinträge im Text zu verorten. Die daraus entstandene Datenbasis hilft dabei neues Wissen zu generieren, so können z.B. Verwandtschaftsbeziehung automatisch erkannt und in den Regesten-Graph integriert werden.&lt;/p&gt;
</p32_abstract>
  <p33_paperID>138</p33_paperID>
  <p33_contribution_type>Poster</p33_contribution_type>
  <p33_acceptance>Akzeptiert</p33_acceptance>
  <p33_authors>Schwengelbeck, Isabel
Wahl, Dominik
Foester, Karl
Friedl, Dennis
Fluss, Fabian
Mersch, Isabelle
Voss, Fabian
Dröge, Martin
Stadler, Peter
Voges, Ramon</p33_authors>
  <p33_organisations>Universität Paderborn, Deutschland
Universität Paderborn, Deutschland
Universität Paderborn, Deutschland
Universität Paderborn, Deutschland
Universität Paderborn, Deutschland
Universität Paderborn, Deutschland
Universität Paderborn, Deutschland
Universität Paderborn, Deutschland
Universität Paderborn, Deutschland
Universität Paderborn, Deutschland</p33_organisations>
  <p33_emails>schwengelbeck.isabel@gmail.com
dwahl@mail.uni-paderborn.de
foesterkarl@gmail.com
dennisfriedl@paderborn.com
fabian.fluss92@gmx.de
imersch@campus.uni-paderborn.de
vossf@mail.uni-paderborn.de
martin.droege@upb.de
peter.stadler@upb.de
ramon.voges@upb.de</p33_emails>
  <p33_presenting_author>Schwengelbeck, Isabel</p33_presenting_author>
  <p33_title>TEI-Editionswerkstatt Urkunden@UPB. </p33_title>
  <p33_abstract>&lt;p&gt;Die aus (Lehramts-)StudentInnen und DozentInnen des Historischen Instituts der Universität Paderborn zusammengesetzte Projektgruppe, TEI-Editionswerkstatt‘ will zugleich wichtige Kompetenzen der Digital Humanities als auch Fachwissen vermitteln. Das gemeinsame Ziel ist es, vier Urkunden über die Gründung der Jesuitenuniversität Paderborn um 1600 in einer digitalen Quellenedition der Forschung online zur Verfügung zu stellen.&lt;/p&gt;
&lt;p&gt;Die Arbeitsweise von HistorikerInnen wird durch die Verfügbarkeit von Quellen in digitaler wie digitalisierter Art verändert (Bernsen 2017: 295, Kelly 2013). Daraus ergibt sich das Desiderat nach einer „der digitalen Welt angepassten, technikgestützten Quellenkritik“ (Pfanzelter 2016: 93). Herausforderungen hinsichtlich des Umgangs mit Quellen lassen sich für alle Ebenen der Gesellschaft ableiten – von einer Schülerschaft, die zur Partizipation in der Gesellschaft befähigt werden soll, über (Lehramts-) Studierende, die dieses ermöglichen sollen, bis hin zu den FachwissenschaftlerInnen und Lehrenden an den Universitäten. Dementsprechend hoch ist die Relevanz, das im folgenden vorgestellte Projekt im Kontext der „Kritik der digitalen Vernunft“ zu diskutieren und einen besonderen Fokus darauf zu legen, welche und wie stark ausgeprägte digitale Kompetenzen HistorikerInnen benötigen, um dem „digitalen Zeitalter“ gerecht zu werden und gleichzeitig den kritischen Anforderungen der Geisteswissenschaften zu genügen. Dies gilt in besonderem Maße mit Blick auf zukünftige GeschichtslehrerInnen, die in ihrer Position als gesellschaftliche Multiplikatoren einer adäquaten Ausbildung bedürfen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p33_abstract>
  <p34_paperID>173</p34_paperID>
  <p34_contribution_type>Poster</p34_contribution_type>
  <p34_acceptance>Akzeptiert</p34_acceptance>
  <p34_authors>Burckhardt, Daniel
Menny, Anna</p34_authors>
  <p34_organisations>Institut für die Geschichte der deutschen Juden (IGdJ), Deutschland
Institut für die Geschichte der deutschen Juden (IGdJ), Deutschland</p34_organisations>
  <p34_emails>burckhardtd@geschichte.hu-berlin.de
anna.menny@igdj-hh.de</p34_emails>
  <p34_presenting_author>Burckhardt, Daniel
Menny, Anna</p34_presenting_author>
  <p34_title>Schlüsseldokumente zur deutsch-jüdischen Geschichte: Eine digitale Edition des Instituts für die Geschichte der deutschen Juden</p34_title>
  <p34_abstract>&lt;p&gt;Die vom Institut für die Geschichte der deutschen Juden (IGdJ) erstellte, seit Juli 2015 von der DFG geförderte und seit September 2016 frei zugängliche zweisprachige (deutsch/englisch) Online-Quellenedition „Hamburger Schlüsseldokumente zur deutsch-jüdischen Geschichte“ (http://juedische-geschichte-online.net/) wirft am Beispiel von ausgewählten Quellen thematische Schlaglichter auf zentrale Aspekte der deutsch-jüdischen Geschichte Hamburgs.&lt;/p&gt;
&lt;p&gt;Mit der Auswahl und Digitalisierung von Text-, Bild-, Ton- und Sachquellen, die exemplarisch Einblick in historische Zusammenhänge und Ereignisse von der frühen Neuzeit bis in die Gegenwart bieten – den sog. Schlüsseldokumenten – führt sie das aufgrund von Vertreibung und Migration verstreute jüdische Erbe der Stadt digital wieder zusammen und trägt zu seiner langfristigen Sicherung für zukünftige Generationen bei. Ziel ist dabei, das Digitale nicht nur als ein weiteres Medium zu begreifen, sondern als einen Werkzeugkasten, mit dem das Material auf unterschiedlichen Ebenen bearbeitet werden kann. Zum einen führt die Digitalisierung selbst zur besseren Zugänglichkeit und nachhaltigen Sicherung, zum anderen erlauben die technische Auszeichnung und Verknüpfung der bereitgestellten Materialien die Auswertung bislang nicht systematisch erfasster Informationen. Und schließlich bietet eine digitale Publikationsumgebung die Möglichkeit, neben Textquellen Bild-, Ton- und Videodokumente (sowie zukünftig 3D-Repräsentationen von Objekten) einzubinden und damit in den Geschichtswissenschaften bislang eher stiefmütterlich behandelte Quellengattungen verstärkt in den Blick zu nehmen.&lt;/p&gt;
&lt;p&gt;Von diesen Überlegungen ausgehend, bilden die digitalisierten und technisch aufbereiteten Quellen konsequenterweise den Dreh- und Angelpunkt der Edition, die zugleich so strukturiert ist, dass sie hypertextuell angelegt und modular aufgebaut ist. Dass die Auseinandersetzung über konkrete Deutungen und Einordnungen am Beispiel konkreter Dokumente erfolgt und diese zugleich neuartig aufbereitet präsentiert werden, erlaubt ihre Fruchtbarmachung für neue Fragestellungen und kann Impulse für die deutsch-jüdische Geschichte geben. Alle Quellen werden als Transkript und digitales Faksimile bereitgestellt. Da die Digitalisierung und Online-Stellung von Quellen jedoch auch immer ein Herauslösen aus dem Überlieferungszusammenhang bedeutet und damit mit einer Entkontextualisierung und Entmaterialiserung verbunden ist,  wird bei dieser Edition Wert darauf gelegt, neben der Bereitstellung der digitalisierten Quelle, diese durch begleitende Interpretations- und Hintergrundtexte verstärkt in ihre historischen Kontexte einzubetten und zusätzliche Informationen zur Überlieferung, Rezeptionsgeschichte und zu wissenschaftlichen Kontroversen bereitzustellen.&lt;/p&gt;
&lt;p&gt;Indem für die Digitalisierung, Textauszeichnung und Metadatenerschließung auf existierende Standardformate digitaler Editionen und der Langfristarchivierung wie MODS (Katalogdaten), METS (Digitalisate), TEI (Textauszeichnung der Transkriptionen und Übersetzungen), DOI (persistente Adressierung) sowie GND-Beacon-Dateien zurückgegriffen wird und bestehende Werkzeuge (Oxygen XML Editor) und technische Infrastrukturen (MyCoRe, Zotero) nachgenutzt werden, zugleich aber die Nutzerfreundlichkeit und Bedienbarkeit im Vordergrund steht, wurde eine innovative digitale Quellenedition zur jüdischen Geschichte Hamburgs geschaffen, die das Digitale als eine Möglichkeit ansieht, analoge Quelle neuartig zu präsentieren und mit weiteren (Informations-)schichten anzureichern und damit neue Impulse für die Forschung zu geben.&lt;/p&gt;
&lt;p&gt;Neben der Auswahl und Aufbereitung des ausgewählten heterogenen Quellenmaterials zeichnet sich das Angebot durch umfassende Recherchemöglichkeiten (Karte, Zeitstrahl, Themen) sowie eine attraktive Präsentationsform aus. In den Transkriptionen und Übersetzungen werden Personen, Organisationen und Orte systematisch mit Normdaten ausgezeichnet. Auf diese Weise werden digitale Edierungstechniken für Quellen zur jüdischen Geschichte erprobt. Dies ermöglicht die bidirektionale Verknüpfung der Edition mit externen Angeboten. So können ergänzende Informationen aus Linked Data Services wie denen der DNB und von Getty automatisiert ergänzt werden. Umgekehrt ermöglicht die Generierung von eigenen GND-Beacon-Listen externen Anbietern eine einfache Verknüpfung ihrer Angebote mit den entsprechenden Inhalten im Quellenportal.&lt;/p&gt;
&lt;p&gt;Unser Poster zeigt die zentralen Eigenschaften der Online-Edition und hilft, den konzeptionellen Rahmen zu verstehen. Es illustriert die Verknüpfung zwischen TEI-Kodierung der Dokumente sowie ihrer Präsentation und Navigation. Es soll damit den konzeptionellen Grundgedanken des Projektes veranschaulichen, das Digitale mit seinen Möglichkeiten ernst zu nehmen, jedoch nicht in Konkurrenz, sondern in Ergänzung zum Analogen.&lt;/p&gt;
</p34_abstract>
  <p35_paperID>200</p35_paperID>
  <p35_contribution_type>Poster</p35_contribution_type>
  <p35_acceptance>Akzeptiert</p35_acceptance>
  <p35_authors>Heckelen, Malte</p35_authors>
  <p35_organisations>Universität Stuttgart, Deutschland</p35_organisations>
  <p35_emails>malte.heckelen@ilw.uni-stuttgart.de</p35_emails>
  <p35_presenting_author>Heckelen, Malte</p35_presenting_author>
  <p35_title>Verhaltensmuster in Massendiskursen: Ein Opinion Dynamics - Modell</p35_title>
  <p35_abstract>&lt;p align="left"&gt;Die sozialpsychologische Persuasionsforschung untersucht die Determinanten der Überzeugung auf der Mikroebene von Kleingruppen. Opinion Dynamics – Modelle, die zumeist auf den für die Modellierung komplexer Systeme eingesetzten und auch in den Digital Humanities wichtiger werdenden agentenbasierten Simulationen basieren(vgl. Jannidis 2017, Kapitel 22), befassen sich hingegen mit der Verbindung der Mikroebene zur Makroebene: mit bewusst einfach konzeptualisierten Agenten als Black Box, deren simple Botschaftenübertragungen interagieren und zu emergenten Meinungsdynamiken führen. Dieses ebenfalls auf agentenbasierter Modellierung basierende Simulationsprojekt stattet Agenten hingegen mit einer kognitiven Architektur aus, die auf Dual Process - Modellen der Meinungsbildung und strategischen Handlungswahl basiert (siehe nächster Absatz). Dies soll nicht nur die Untersuchung der Mikro- und Makroebenen, sondern auch der Mesoebene sozialen Diskursverhaltens ermöglichen. Die kausale Verbindung dieser Ebene mit Netzwerkstruktur, technischen Bedingungen des Botschaftenaustauschs (zum Beispiel Filtering in Social Media) und resultierenden Meinungsdynamiken soll Aufschluss über Diskursnormen geben, die etwa zur plötzlichen Verbreitung radikaler Meinungen führen.&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;Dual Process - Framework für das Opinion Dynamics – Modell&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;Das entwickelte Modell baut auf Dual Process - Modellen der Persuasionsforschung und der handlungstheoretisch orientierten Soziologie auf. Das soziologische Modell der Frame-Selektion (Kroneberg 2010, Esser 1996) konzeptualisiert bewusstes Entscheiden und unbewusstes Verhalten in einem Rational Choice - Metaframework: Der Nutzen der Informationsverarbeitung und Handlungsabwägung auf rational-kalkulierende Weise gegenüber der automatischen Verarbeitung anhand sozial geteilter Frames und Skripte steigt mit der vorhandenen Verarbeitungsenergie und der Uneinordenbarkeit der Situation in Klassen. Das Elaboration-Likelihood-Modell (Petty und Cacioppo 1986) und das Systematic-Heuristic-Model (Chaiken und Eagly 1989) aus der Sozialpsychologie sehen die Verarbeitung persuasiver Botschaften ebenfalls auf zwei Wegen: Verarbeitung der argumentativen Struktur (systematisch) und Orientierung an Sender- und Botschaftenattributen (heuristisch).&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;Das vorliegende Modell integriert diese Sichtweisen zu einem handlungstheoretischen Blick auf Massendiskurse: Individuen maximieren die Meinungsähnlichkeit in ihrer Umgebung und minimieren den daraus resultierenden Energieverlust. Agenten wählen entweder kalkulierend oder normbezogen-automatisch Aktionen der Botschaftenrezeption und -produktion, um die Meinungsähnlichkeit in ihrer Umgebung zu maximieren. Lokale Diskussionsnormen bilden sich situationsabhängig als Cluster automatisch-spontan gewählter Handlungen, deren Assoziation mit bestimmten Situationen durch wiederholte (und erfolgreiche) Auswahl im rational-kalkulierenden Modus zustande kommt.&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;Computermodell&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;Das Computermodell basiert auf dem Cultural Dissemination - Modell (Axelrod 1997): In diesem verfügen Agenten über Kulturen/Meinungen, repräsentiert als Zahlenketten, die ausgetauscht werden. Ob ein Agent seine Meinung an die eines anderen anpasst (Austausch einer Zahl), hängt von der Levenshtein-Distanz der beiden Zahlenketten und einem darauf bezogenen Grenzwert ab. Diese Agenteninteraktion wird im vorliegenden Modell erweitert: Agenten rezipieren Botschaften entweder im systematischen Modus oder im heuristischen Modus. Der systematische Modus ist dem klassischen Cultural Dynamics - Modell im Ablauf gleich. Im heuristischen Modus hängt die Änderung der Agentenmeinung von Senderattributen wie Popularität, der Ähnlichkeit von Agenteneigenschaften und der Ähnlichkeit der gesendeten Meinung zu Nachbarschaftsmeinungen ab. Agenten können Meinungen unverändert senden oder sie an die Botschaften des Empfängers anpassen, wobei beide Varianten mit zusätzlichen Referenzen auf Botschaftenqualitäten (Popularität, Meinungsähnlichkeit zu Rezipientenpeers u.a.) versehen werden können. Alle Entscheidungen werden über die Maximierung von Nutzenfunktionen im rationalen Modus oder die automatische Wahl im automatisch-spontanen Modus getroffen. Der automatische Modus der Handlungswahl wird mittels eines mit einem adaptiven k-means-Clusteralgorithmus gekoppelten Q-Learning-Algorithmus implementiert (Karimpanal und Wilhelm 2017, Wen et al. (2006)). Kann eine neue Situation (Vektor aus Umgebungsparametern) nicht eindeutig einem durch Erfahrung gebildeten Cluster zugeordnet werden, wird sie im rationalen Modus verarbeitet.&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;Projektfortschritt&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;Das Modell ist im Java-Framework Repast Symphony mit voller Funktionalität des rationalen Entscheidungsmodus implementiert. Die Manipulation der Starteigenschaften des komplexen Netzwerks sowie der automatisch-spontane Entscheidungsmodus folgen noch 2017. Das geplante Poster wird die Ziele des Projekts, die Funktionsweise des Computermodells sowie erste Analysergebnisse vorstellen.&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;Literatur&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p&gt;Axelrod, Robert (1997): “The Dissemination of Culture.” &lt;em&gt;Journal of Conflict Resolution&lt;/em&gt; 41 (2): 203–26.&lt;/p&gt;

&lt;p&gt;Chaiken, Shelly, / Eagly, Alice H. Eagly (1989): “Heuristic and Systematic Information Processing Within and Beyond the Persuasion Context.” In: Uleman, J.S. / Bargh, J.A.: &lt;em&gt;Unintended Thought. &lt;/em&gt;New York: Gilford,&lt;em&gt;&lt;/em&gt;212–52.&lt;/p&gt;

&lt;p&gt;Esser, Hartmut. (1996): “Die Definition der Situation.” &lt;em&gt;Kölner Zeitschrift &lt;/em&gt;&lt;em&gt;f&lt;/em&gt;&lt;em&gt;ür Soziologie &lt;/em&gt;&lt;em&gt;u&lt;/em&gt;&lt;em&gt;nd Sozialpsychologie&lt;/em&gt; 48 (2): 1–34.&lt;/p&gt;

&lt;p&gt;Karimpanal, Thommen George, / Wilhelm, Erik (2017): “Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering.” &lt;em&gt;Neurocomputing&lt;/em&gt; 263: 39-47.&lt;/p&gt;

&lt;p&gt;Kohle, Hubertus (2017): “Digitale Rekonstruktion und Simulation”. In: Jannidis, Fotis / Kohle, Hubertus (eds.): &lt;em&gt;Digital Humanities. Eine Einführung&lt;/em&gt;. Stuttgart: J.B. Metzler, 315-327.&lt;/p&gt;
&lt;p&gt;Kroneberg, Clemens. 2010. “Das Modell Der Frame-Selektion: Grundlagen Und Soziologische Anwendung Einer Integrativen Handlungstheorie.” PhD, Universität Mannheim.&lt;/p&gt;
&lt;p&gt;Petty, Richard E., und John T. Cacioppo. 1986. “The Elaboration Likelihood Model of Persuasion.” In: &lt;em&gt;Advances in Experimental Social Psychology&lt;/em&gt; 19: 123–205.&lt;/p&gt;
&lt;p&gt;Wen, Feng / Chen, Zonghai / Zhuo, Rui / Zhou, Guangming (2006): “Reinforcement Learning Method of Continuous State Adaptively Discretized Based on K-Means Clustering.” &lt;em&gt;Control and Decision&lt;/em&gt; 21 (2): 143-146.&lt;/p&gt;
</p35_abstract>
  <sessionID>60</sessionID>
  <p36_paperID>192</p36_paperID>
  <p36_contribution_type>Poster</p36_contribution_type>
  <p36_acceptance>Akzeptiert</p36_acceptance>
  <p36_authors>Ullrich, Sabine
Bruder, Daniel
Hadersbeck, Maximilian</p36_authors>
  <p36_organisations>Ludwig-Maximilians-Universität München, Deutschland
Universität Cambridge, Vereinigtes Königreich
Ludwig-Maximilians-Universität München, Deutschland</p36_organisations>
  <p36_emails>sabine.ullrich@campus.lmu.de
dmb77@cam.ac.uk
maximilian@cis.uni-muenchen.de</p36_emails>
  <p36_presenting_author>Ullrich, Sabine</p36_presenting_author>
  <p36_title> Aufdecken von “versteckten” Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning</p36_title>
  <p36_abstract>&lt;p&gt;In diesem Poster soll ein Tool vorgestellt werden, welches dem Philologen mit Methoden des Machine Learning das Aufspüren von Einflüssen aus Zitaten in textgenetischen Prozessen unterstützen soll, indem es erlaubt, “ähnliche”, d.h. potentielle Zitate vorzufiltern und diese einer genaueren Untersuchung zu unterziehen. Dadurch könnten – in einer konkreten Anwendung – beispielweise Wittgensteins Gedankensprünge, Ideen und Einfälle teil-automatisiert vorgefiltert nachvollzogen werden und nähere Analysen gemacht werden. Mit der Möglichkeit, eine Bemerkung einer anderen Textsammlung gegenüberzustellen, könnten -- so die Idee –, nicht angegebene Zitate, Verweise, Einflüsse auf den Denk- und Schreibprozess aufgedeckt werden.&lt;/p&gt;
</p36_abstract>
  <p37_paperID>260</p37_paperID>
  <p37_contribution_type>Poster</p37_contribution_type>
  <p37_acceptance>Akzeptiert</p37_acceptance>
  <p37_authors>Kamocki, Pawel
Ketzan, Erik
Wildgans, Julia
Witt, Andreas</p37_authors>
  <p37_organisations>WWU Münster, Germany; IDS Mannheim; ELDA, France
Birkbeck, University of London
IDS Mannheim; Universität Mannheim
IDS Mannheim; Universität zu Köln</p37_organisations>
  <p37_emails>pawel.kamocki@gmail.com
eketzan@gmail.com
j.wildgans@googlemail.com
witt@ids-mannheim.de</p37_emails>
  <p37_presenting_author>Kamocki, Pawel</p37_presenting_author>
  <p37_title>CLARIN Legal Information Plattformen und Legal Helpdesk</p37_title>
  <p37_abstract>&lt;p dir="ltr"&gt;Wissenschaftler im Bereich der Digital Humanities sind ständig auf einen Zugang zu vertrauenswürdigen und zuverlässigen rechtlichen Informationen angewiesen. Die entscheidenden rechtlichen Herausforderungen stellen sich vor allem im Immaterialgüterrecht (insbesondere in Bezug auf das Urheberrecht, das sui generis-Recht für Datenbanken und das verwandte Schutzrecht für den Verfasser von wissenschaftlichen Ausgaben) und im Datenschutzrecht. Daher ist es sinnvoll, beides bereits in der Anfangsphase jedes Projekts zu berücksichtigen, um rechtliche Probleme in späteren Projektphasen und das Scheitern von Forschungsprojekten zu vermeiden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Allerdings erscheint eine ständige Information über die rechtlichen Rahmenbedingungen vor dem Hintergrund der ständigen Änderungen der Gesetze, die die neuen Technologien betreffen, sehr schwierig. Auch in 2018 wird es sowohl im deutschen als auch im europäischen Datenschutzrecht wesentliche Änderungen geben, die Auswirkungen auf die Erhebung, den Zugang und die Verwendung von Forschungsdaten haben werden. Darüber hinaus wird derzeit über den Entwurf einer neuen Richtlinie im Urheberrecht diskutiert, die möglicherweise schon bald verabschiedet wird. All diese Änderungen im Blick zu behalten erfordert jedenfalls regelmäßigen Zugang zu aktuellen rechtlichen Informationen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Daher haben Pawel Kamocki und Erik Ketzan im Jahr 2012 die CLARIN-D Legal Information Plattform für DH Forscher in Deutschland aufgesetzt: Sie ist sowohl in deutscher als auch in englischer Sprache verfügbar. 2016 folgte die CLARIN Legal Information Plattform für Wissenschaftler aus den übrigen CLARIN Consortium Ländern, die bisher lediglich in englischer Sprache abrufbar ist. Beide Webseiten stellen in verschiedenen Artikeln und Formaten (derzeit insgesamt ca. 25.000 Wörter) rechtliche Informationen für den Bereich der Digital Humanities bereit und streben dabei danach, die umfangreichste und aktuellste Wissensressource für Wissenschaftler zu sein.&lt;/p&gt;
&lt;p dir="ltr"&gt;Sie enthalten Erklärungen zu den grundlegenden rechtlichen Prinzipien und Konzepten im Bereich des Urheberrechts (Gegenstand, Rechteinhaberschaft, Umfang und Reichweite des Schutzes und Schrankenregelungen insbesondere für wissenschaftliche Zwecke) und des sui-generis-Rechts für Datenbanken, zur Lizenzierung (einschließlich der Nutzung öffentlicher Lizenzen für Daten und Software) und zum Datenschutz. Darüber hinaus werden Wissenschaftler bei Bedarf auch zu praktischen Lizenzauswahlinstrumenten weitergeleitet, wie z.B. dem “Public License Selector” (http://ufal.github.io/public-license-selector/), der 2014 im Rahmen einer Kooperation zweier CLARIN-Zentren von Kamocki, Stranak und Sedlak entwickelt wurde. Zusätzlich bieten die Plattformen Zugriff auf die CLARIN Legal Issues Committee (CLIC) White Paper Series, die eine Open Access Publikation von Kommentaren und Forschungsergebnissen bezüglich rechtlicher Fragestellungen im Bereich der Sprachwissenschaft unter der redaktionellen Leitung des CLIC ermöglichen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Das Legal Helpdesk ist der “direkte Draht” zu einer persönlichen Hilfestellung: Dieses ermöglicht eine Kontaktaufnahme mit einem Teammitglied des CLARIN-Teams, das Wissenschaftler zu hilfreichen Ressourcen und Informationen bezüglich ihrer Forschungsfrage leiten kann.&lt;br /&gt;Die Plattformen sind frei im Internet verfügbar und werden in regelmäßigen Abständen aktualisiert. Beide werden häufig im Rahmen von CLARIN-D und CLARIN-EU-Projekten genutzt.&lt;/p&gt;
&lt;p dir="ltr"&gt;Unser Poster wird diese hilfreichen CLARIN Ressourcen anhand von Graphiken und Text vorstellen und aktuelle Updates darstellen, die der DH Community möglicherweise noch unbekannt sind.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p37_abstract>
  <p38_paperID>238</p38_paperID>
  <p38_contribution_type>Poster</p38_contribution_type>
  <p38_acceptance>Akzeptiert</p38_acceptance>
  <p38_authors>Rolshoven, Jürgen
Etimi, Valmir
Neugebauer, David
Seipel, Peter
Wiehe, Thomas</p38_authors>
  <p38_organisations>Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p38_organisations>
  <p38_emails>rols@spinfo.uni-koeln.de
vetemi@smail.uni-koeln.de
david.neugebauer@posteo.de
pseipel1@uni-koeln.de
twiehe@uni-koeln.de</p38_emails>
  <p38_presenting_author>Rolshoven, Jürgen</p38_presenting_author>
  <p38_title>Strings&amp;Structures</p38_title>
  <p38_abstract>&lt;p&gt;Der vorliegende Beitrag befasst sich mit dem Projekt "Strings&amp;Structures. Codes of Sense and Function in Genomics and Linguistics" (http://www.spinfo.phil-fak.uni-koeln.de/stringsandstruct.html). Dieses Projekt wird von der Sprachlichen Informationsverarbeitung und der Bioinformatik im Rahmen der Exzellenzinitiative der Universität zu Köln durchgeführt. Beide Bereiche befassen sich intensiv mit der Prozessierung von Texten. Bei der Sprachlichen Informationsverarbeitung handelt es sich um natürlichsprachliche Texte und Textkorpora, bei der Bioinformatik um genomische Texte. Das Projekt zielt auf die Aufdeckung von Mustern in Texten und die Analyse der Beziehung der Muster untereinander. Vor dem Hintergrund dieser Fragestellungen werden gemeinsam nutzbare Algorithmen entwickelt. Jedoch sollten dabei wesentliche Unterschiede der zugrundeliegenden Textarten nicht übersehen werden. Natürlichsprachliche Texte sind das Resultat grammatischer Produktionssysteme, genomische Texte sind Produktionssysteme. Das linguistische Vorhaben zielt auf die Rekonstruktion der erzeugenden Produktionssysteme aus zugrundeliegenden Textkorpora. Weitere Unterschiede zwischen natürlichsprachlichen Texten und genomischen Texten liegen in der Größe der zugrunde liegenden Alphabete und der zweigliedrigen Kombinatorikebenen natürlicher Sprachen. Wenngleich die Interaktion und Dynamik der Einheiten in genomischen Texten hochkomplex ist, so kann die Funktion einer einzelnen Einheit gut bestimmt werden. In natürlichen Sprachen dagegen ist die Bedeutung einzelner Einheiten oftmals nur schwierig zu bestimmen. Sie ist hochgradig kontext- und situationsabhängig. Dies hängt auch damit zusammen, dass sprachliche Einheiten weitgehend polysem sind.&lt;/p&gt;
&lt;p&gt;Die automatische Aufdeckung der Bedeutung und Funktion sprachlicher Zeichen vollzieht sich in einem vierstufigen Prozess:&lt;/p&gt;
&lt;p&gt;1. Ermittlung minimaler bedeutungs- oder funktionstragender Einheiten.&lt;/p&gt;
&lt;p&gt;2. Kombinatorik dieser Einheiten durch Aufdeckung morphologischer Prozesse.&lt;/p&gt;
&lt;p&gt;3. Syntaktische Kombinatorik der morphologisch erkannten Einheiten.&lt;/p&gt;
&lt;p&gt;4. Auswertung syntaktischer Strukturen für die Bestimmung der Bedeutung sprachlicher Einheiten.&lt;/p&gt;
&lt;p&gt;Dieses vierstufigen Verfahren wird in schrittweiser Verfeinerung in weitere Komponenten zerlegt, die algorithmisch als Module in einem Prozesskettensystem frei verschaltet werden. Ein solches graphisch orientiertes System ermöglicht auch Laien, Prozessketten für die Lösung eigener Fragestellungen zu schaffen (https://github.com/spinfo/stringsnstructures).&lt;/p&gt;
&lt;p&gt;Ad 1. Ermittlung minimaler bedeutungs- oder funktionstragender Einheiten.&lt;/p&gt;
&lt;p&gt;Bei der Ermittlung minimaler Bedeutung oder funktionstragende Einheiten wird von dem strukturalistischen Grundgedanken der Zeichenkonstitution durch Opposition ausgegangen. Dieser Gedanke wird mit Hilfe von Suffixbäumen umgesetzt. In Suffixbäumen verweisen Verzweigungen auf potenziell in Opposition stehende Zeichenketten hin. Allerdings führt eine direkte Auswertung von Verzweigungen Suffixbäumen zu einer viel zu mächtigen Menge potentieller Morpheme. Daher müssen Filtermechanismen für deren Reduktion konstruiert werden. Ein Filtermechanismus beruht darin, nur identische Zeichenketten aus vorwärts und rückwärts aufgebauten Suffixbäumen zu verwenden.&lt;/p&gt;
&lt;p&gt;Ad 2. Kombinatorik dieser Einheiten durch Aufdeckung morphologischer Prozesse.&lt;/p&gt;
&lt;p&gt;Ein weiterer Filtermechanismus liegt in der Begrenzung der Kombinatorik von kleinsten funktions- oder bedeutungstragenden Einheiten. Formal kann morphologische Kombinatorik als Typ-2-Sprache im Sinne der Chomsky-Hierarchie formaler Sprachen betrachtet werden. Mit zusätzlichen Kriterien zur Unterscheidung bedeutungs- oder funktionstragender Einheiten kann die Übermenge, die der Suffixbaumgenerator liefert, drastisch reduziert werden. Die verbleibenden Einheiten sind in den nachfolgenden Schritten syntaktisch und semantisch zu analysieren.&lt;/p&gt;
&lt;p&gt;Ad 3. Syntaktische Kombinatorik der morphologisch erkannten Einheiten&lt;/p&gt;
&lt;p&gt;Eines der Probleme maschineller syntaktischer Sprachverarbeitung liegt in der Kontextsensitivität natürlicher Sprachen. Dies hat unter anderem zur Folge, dass Einheiten, die bedeutungsmäßig zusammengehören, in Sätzen oftmals weit voneinander getrennt sind. Für die Erkennung semantischer Zusammengehörigkeit und semantischer Abhängigkeit werden in dem vorliegenden Projekt Kookurrenzmatrizes ausgewertet. Die Kookurrenzmatrizes speichern semantische Vektoren, die semantische Abhängigkeit ausdrücken. Starke semantischer Abhängigkeit -etwa eines Verbs zu seinem Objekt -können werden in einer Baumstruktur direkt durch benachbarte Knoten ausgedrückt, selbst dann, wenn es Vorkommen des Objekts gibt, die gar nicht unmittelbar neben dem Verb im Textkorpus stehen. Letztlich könnten auf diese Weise kontextabhängige Phänomene aufgedeckt werden.&lt;/p&gt;
&lt;p&gt;Ad 4. 4. Auswertung syntaktischer Strukturen für die Bestimmung der Bedeutung sprachlicher Einheiten.&lt;/p&gt;
&lt;p&gt;Syntaktische Strukturen in natürlichen Sprachen haben die Funktion, die Prozessierung sprachlichen Inputs zu erleichtern und zu beschleunigen. Syntaktische Strukturbäume ermöglichen es, korrekte Beziehungen zwischen sprachlichen Elementen herzustellen. Für die Bestimmung von Bedeutungspotenzial sind syntaktische Strukturen daher von grundlegender Bedeutung. Wird das Bedeutungspotenzial wiederum durch vektorielle Kookurrenzmatrizes erfasst, dann tragen syntaktischer Strukturbäume dazu bei, die Zahl der Komponenten der Matrizes stark zu reduzieren und folglich die vektorielle Semantik zu schärfen.&lt;/p&gt;
&lt;p&gt;Eine Besonderheit des hier gewählten Vorgehens liegt in der Interaktion von subsymbolischen, vektoriellen und symbolischen, baumstrukturorientierten Verfahren. Die Stärke symbolischer Verfahren liegt in ihrer Kompaktheit und der Möglichkeit der Falsifikation. Subsymbolischer Verfahren sind nicht oder nur schwierig falsifizierbar. Sie machen semantische Unschärfe und semantische Ähnlichkeit fassbar&lt;/p&gt;
</p38_abstract>
  <p39_paperID>165</p39_paperID>
  <p39_contribution_type>Poster</p39_contribution_type>
  <p39_acceptance>Akzeptiert</p39_acceptance>
  <p39_authors>Ruiz Fabo, Pablo
Martínez Cantón, Clara
Calvo Tello, José</p39_authors>
  <p39_organisations>UNED, Spanien
UNED, Spanien
Universität Würzburg</p39_organisations>
  <p39_emails>pablo.ruiz@linhd.uned.es
cimartinez@flog.uned.es
jose.calvo@uni-wuerzburg.de</p39_emails>
  <p39_presenting_author>Ruiz Fabo, Pablo
Calvo Tello, José</p39_presenting_author>
  <p39_title>DISCO: Diachronic Spanish Sonnet Corpus</p39_title>
  <p39_abstract>&lt;p&gt;This poster presents a corpus of sonnets in Spanish of the 19th Century in XML-TEI. The corpus collects a total of 685 authors and 2677 sonnets, from several Spanish-speaking countries. It includes well-known authors, such as Gustavo Adolfo Bécquer, Delmira Agustini or “Clarín”, but also less canonized authors. Each text and presentationAuthor is enriched with identifiers and metadata. The corpus is available as a GitHub repository, in response to good practices for the use and reuse of the data and it conservation. Sonnets since the 15th century to the present are under validation and will be published shortly within the corpus, which is intended as a diachronic resource. The corpus is online at https://github.com/pruizf/disco &lt;/p&gt;
</p39_abstract>
  <p40_paperID>248</p40_paperID>
  <p40_contribution_type>Poster</p40_contribution_type>
  <p40_acceptance>Akzeptiert</p40_acceptance>
  <p40_authors>Andorfer, Peter
Karner, Stefan</p40_authors>
  <p40_organisations>ACDH, Österreich
Österreichische Nationalbibliothek</p40_organisations>
  <p40_emails>peter.andorfer@oeaw.ac.at
stefan.karner@onb.ac.at</p40_emails>
  <p40_presenting_author>Andorfer, Peter</p40_presenting_author>
  <p40_title>TEIHencer - Enhance your TEI-Documents</p40_title>
  <p40_abstract>&lt;p dir="ltr"&gt;Die Forschungstätigkeiten Georeferencing und Entity Linking sind wichtiger Bestandteil vieler DH-Projekte. Webservices/APIs und Tools versuchen diese Tätigkeiten zu vereinfachen und zu beschleunigen. Eines der bekannteren Tools, wenigstens im deutschsprachigen Raum ist dabei vermutlich der ‘DARIAH-DE Datasheet Editor’[1]. Dieser zeichnet sich durch seine einfache Benutzung aus, sei es was den Datenimport (ausfüllen einer Tabelle oder Hochladen einer CSV-Tabelle) betrifft oder die anschließende Disambiguierung/Verifizierung der vom ‘Getty Thesaurus of Geographic Names’[1] zurückgelieferten Treffer über ein Graphical User Interface. &lt;/p&gt;
&lt;p dir="ltr"&gt;Das Projekt TEIHencer greift diese Vorzüge des ‘DARIAH-DE Datasheet Editors’ auf und versucht diese einerseits mit der ‘TEI-Welt’ zu verknüpfen sowie mit GeoNames und der GND zwei alternative Normdaten Ressourcen einzubinden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Konkret handelt es sich bei TEIHencer[3] um ein Plug-In zu dem Python/Django basierten prosopographisch-geographischen Informationssystem APIS[4]. Mit Hilfe des TEIHencers ist es möglich, XML/TEI kodierte Texte in denen Lokalitäten, Orte ausgezeichnet sind, über eine Webformular in APIS zu importieren. Während des Imports werden die Orts-Entitäten entsprechend eines vom Benutzer wählbaren X-Path Ausdruckes geparst, gegen GeoNames und GND abgeglichen und im Falle von Übereinstimmung angereichert und in einer relationalen Datenbank gespeichert. Die gespeicherten Entitäten können anschließend über das APIS-Web-Interface im Falle mehrerer Treffer disambiguiert werden. Dies erfolgt über eine Kartendarstellung, in welcher die verschiedenen Treffer zu einer Entität aufscheinen. Darüber hinaus können über das APIS-Web-Interface noch weitere Informationen zu den Entitäten ergänzt (z.B. alternative Schreibweisen, Datierungen) sowie die einzelnen Entitäten miteinander in typisierte Beziehungen gesetzt werden (z.B. Ort A ist Nachfolger von Ort B.; Ort A ist Teil von Ort B).&lt;/p&gt;
&lt;p dir="ltr"&gt;Die mit Hilfe von TEIHencer angereicherten Daten können dann wieder als XML/TEI Dokument (kodiert als &lt;tei:listPlace&gt; Element) exportiert bzw. über HTTP GET request abgerufen und so etwa in andere Applikationen eingebunden werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Im Zuge der Posterpräsentation soll der TEIHencer der einschlägigen DH-Comunity vorgestellt werden und zwar an dem konkreten Fallbeispiel der “Andreas Okopenko: Tagebücher aus dem Nachlass (Hybridedition)”[5]. Dabei handelt es sich um ein digitales Editionsprojekt, das eine Auswahl der Tagebücher Andreas Okopenkos im Zeitraum von 1949 bis 1955 inhaltlich erschließen und einem breiteren Publikum zugänglich machen möchte. Einer der Schwerpunkte des Projekts liegt hierbei auf der inhaltlichen Erschließung des örtlichen Wirkungs- und Schaffensraums des Nachkriegsavantgardisten, indem nicht nur erwähnte Orte (&lt;tei:placeName&gt;), sondern nach Maßgabe auch Werke und Organisationen (&lt;tei:presentationTitle&gt; und &lt;tei:orgName&gt;) mit geographischen Normdaten verknüpft werden, um so ein umfassenderes Bild von Okopenkos kulturellem Kontext vermitteln zu können.&lt;/p&gt;
&lt;p dir="ltr"&gt;Neben der eigentlich Applikation und des konkreten Use-Cases wird am Poster auch das Konferenzthema “Kritik der Digitalen Vernunft” bzw. das Subthema “Kritik digitaler Angebote, Projekte und Werkzeuge” in Form der Frage nach der Nachhaltigkeit des vorgestellten Tools reflektiert. Eine solche glauben wir nämlich insofern gewährleisten zu können, als das Tool a) in ein konkretes Projekt (Okopenko) eingebettet ist, b) einen weit verbreiteten Standard (TEI) unterstützt, c) auf bestehende Eigenentwicklungen (APIS) aufbaut und d) Teile des Codes als selbstständige Module (TEI-Modul als python-package) konzipiert sind, die auch jenseits der konkreten Applikation Anwendung finden können. Darüber hinaus, e) ist der gesamte Code auf GitHub publiziert [3].  &lt;/p&gt;
&lt;p dir="ltr"&gt;[1] http://www.getty.edu/research/tools/vocabularies/tgn/index.html &lt;/p&gt;
&lt;p dir="ltr"&gt;[2] https://geobrowser.de.dariah.eu/edit/index.html&lt;/p&gt;
&lt;p dir="ltr"&gt;[3] https://github.com/acdh-oeaw/teihencer &lt;/p&gt;
&lt;p dir="ltr"&gt;[4] https://github.com/acdh-oeaw/apis-core &lt;/p&gt;
&lt;p dir="ltr"&gt;[5] https://www.onb.ac.at/bibliothek/sammlungen/literatur/forschung/projekte/andreas-okopenko-tagebuecher-aus-dem-nachlass-hybridedition/ &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p40_abstract>
  <sessionID>60</sessionID>
  <p41_paperID>293</p41_paperID>
  <p41_contribution_type>Poster</p41_contribution_type>
  <p41_acceptance>Akzeptiert</p41_acceptance>
  <p41_authors>Schlögl, Matthias
Bernád, Ágoston
Kaiser, Maximilian
Lejtovicz, Katalin
Rumpolt, Peter</p41_authors>
  <p41_organisations>Österreichische Akademie der Wissenschaften, Österreich
Österreichische Akademie der Wissenschaften, Österreich
Österreichische Akademie der Wissenschaften, Österreich
Österreichische Akademie der Wissenschaften, Österreich
Österreichische Akademie der Wissenschaften, Österreich</p41_organisations>
  <p41_emails>matthias.schloegl@oeaw.ac.at
agoston.bernad@oeaw.ac.at
maximilian.kaiser@oeaw.ac.at
katalin.lejtocicz@oeaw.ac.at
peter.rumpolt@oeaw.ac.at</p41_emails>
  <p41_presenting_author>Schlögl, Matthias
Bernád, Ágoston
Kaiser, Maximilian
Lejtovicz, Katalin</p41_presenting_author>
  <p41_title>Biographik in den Digital Humanities – Kritische Bestandsaufnahme und quantitative Analysemöglichkeiten am Beispiel des Österreichischen Biographischen Lexikons 1815–1950</p41_title>
  <p41_abstract>&lt;p dir="ltr"&gt;Quantitative Auswertungen gewinnen zunehmend auch in den Geisteswissenschaften an Bedeutung (z. B. Harber 2011). Nicht selten wird mittlerweile auch der Begriff „Big Data“ in Zusammenhang mit den Humanities ganz allgemein verwendet (z. B. gral 2016).  Für solche Analysen wird des Öfteren auf einheitliche Korpora zurückgegriffen (z. B. Wikipedia: Russo et al. 2015), die nicht nur einen standardisierten Zugriff auf tausende Datensätze, sondern auch homogene Metadaten und harmonisierte Vokabulare bieten. Während diese Korpora formal geradezu prädestiniert für Analysen mit den neuen digitalen Tools scheinen, sind die grundlegenden Daten durch die Entstehungsgeschichte der Datensammlungen nicht selten unausgewogen und können mitunter zu falschen bzw. für Historiker unbrauchbaren Ergebnissen führen. Insbesondere trifft dies auf über einen längeren Zeitraum entstandene Nationalbiographien zu, die durch neue Möglichkeiten und Methoden der Digital Humanities in den letzten Jahren zu einer wertvollen Ressource auch für quantitative Analysen aufgestiegen waren.&lt;/p&gt;
&lt;p&gt;Trotz dieses offensichtlichen Dilemmas beschäftigen sich kritische Studien meist mit der Genauigkeit und Sinnhaftigkeit der digitalen Tools selbst und kaum mit der historischen Verlässlichkeit der Ressourcen (z. B. Fokkens et al. 2014). Der Entstehungskontext und die Werkgenese der biographischen Nachschlagewerke geraten ebenso in den Hintergrund wie quellenkritische Aspekte, obwohl ihre Berücksichtigung für Analysen, die – im Rahmen der wissenschaftlichen Diskurse – mit dem Anspruch antreten mit historisch relevanten Aussagen zum Erkenntnisgewinn beizutragen, unerlässlich wäre. Die Posterpräsentation will diese Lücke schließen und liefert sowohl eine Metadaten orientierte, als auch eine computerlinguistisch fundierte Korpusanalyse der digitalen Fassung des Österreichischen Biographischen Lexikons 1815–1950 (ÖBL).&lt;/p&gt;
</p41_abstract>
  <p42_paperID>269</p42_paperID>
  <p42_contribution_type>Poster</p42_contribution_type>
  <p42_acceptance>Akzeptiert</p42_acceptance>
  <p42_authors>Hoffmann, Christoph</p42_authors>
  <p42_organisations>Österreichische Akademie der Wissenschaften, Österreich</p42_organisations>
  <p42_emails>christoph.hoffmann@oeaw.ac.at</p42_emails>
  <p42_presenting_author>Hoffmann, Christoph</p42_presenting_author>
  <p42_title>Science as a Service?</p42_title>
  <p42_abstract>&lt;p&gt;Einer der am weitesten verbreiteten Ansprüche von Digital Humanities Projekten und Forschungsvorhaben ist jener, nachhaltig nutzbare Daten und Services zu produzieren bzw. zu hinterlassen. Neben archivierbaren Datenformaten und quelloffener Software ist die Einrichtung von REST – APIs eine vielgenutzte Möglichkeit, erstellte Services in anderen Projekten nutzbar zu machen. [1] Der Vorteil solcher Schnittstellen liegt darin, dass die ihnen zugrunde liegenden Technologie (im wesentlichen HTTP) in so gut wie allen Plattformen und Programmiersprachen bereits implementiert ist. So sind sowohl Services als auch Daten welche über eine sauber definierte REST – Schnittstelle zugänglich sind, relativ einfach in einer Vielzahl anderer Projekte zu integrieren. In vielen Bereichen der Software – Entwicklung hat dieser Vorteil dazu geführt dass Service orientierte Softwarearchitektur und ein „API first approach“ immer populärer geworden sind.&lt;/p&gt;
&lt;p&gt;Auch das Austrian Center for Digital Humanities an der Österreichischen Akademie der Wissenschaften hat es sich für die kommenden zwei Jahre zu einer Aufgabe gemacht, die in den vergangenen drei Jahren in einer Vielzahl von Projekten entstandenen Daten und Services in standardisierten REST – Schnittstellen verfügbar zu machen bzw. bereits entstandene Schnittstellen entsprechend aufzubereiten und zu dokumentieren. Hierzu sollen zunächst wiederkehrende, generisch abbildbare Aufgaben in den Workflows identifiziert werden, sodann die Ihnen korrespondierenden, bereits bestehenden, Services für eine Verwendung außerhalb des Projektkontextes abgeändert werden.&lt;/p&gt;
&lt;p&gt;Ziel dieses Vorhabens ist es, einen Katalog an REST-Services zu erstellen, welcher in einer Sandbox zum einen die direkte Verwendung erlaubt, zum anderen die Funktionen der verschiedenen Endpunkte unmittelbar kritisch dokumentiert. Dies wird mittels einer AngularJS Single Page Application sowie eines MEAN-stack basierten Backends implementiert. Des Weiteren sollen bereits vorhandene Standards[2] und semantische Erweiterungen [3] auf Ihre Tauglichkeit und Sinnhaftigkeit in Digital Humanities Kontexten hin geprüft, und gegebenenfalls in den Schnittstellen implementiert werden werden. Exemplarisch soll auch modular wiederverwendbare Front – End Komponenten angeboten werden (bspw. Autocompletes u.a.)&lt;/p&gt;
&lt;p&gt;Parallel dazu gilt es, anhand der verwendeten Projekte kritisch zu reflektieren, welche Schritte, Funktionalitäten und Daten sich aus einem vorhandenen Projekt sinnvoll zur Nachnutzung herauslösen lassen. Wie kleinteilig lassen sich die Schritte eines geisteswissenschaftlichen Projektes modularisieren und an externe Services auslagern, wenn ein zur Kritik der Resultate fähiger Gesamtblick gewahrt bleiben soll? Wie exakt muss, umgekehrt gefragt, eine Schnittstelle dokumentiert sein, um einer kritischen Prüfung im Rahmen des verwendenden Projektes standhalten zu können? Diese und andere Betrachtungen sollen schlussendlich in einem White Paper zu REST – APIs für wiederkehrende Szenarien in Digital Humanities Projekten münden.&lt;/p&gt;
&lt;p&gt;Das Poster soll zum einen den Katalog und die Sandbox als Tools präsentieren, zum anderen erste Ergebnisse der Standardisierung von APIs zur Diskussion stellen. Eine Debatte zu den oben genannten Fragen soll dem Vorhaben eine noch breitere Grundlage bei der Erstellung des White Papers geben.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="left"&gt;[1] http://digitalhumanities.berkeley.edu/blog/15/05/01/project-sustainability-dh-collaboration-and-community&lt;/p&gt;
&lt;p&gt;[2] bspw https://github.com/OAI/OpenAPI-Specification oder http://jsonapi.org/&lt;/p&gt;
&lt;p&gt;[3] http://www.hydra-cg.com/spec/latest/core/ und http://microformats.org/&lt;/p&gt;
</p42_abstract>
  <p43_paperID>222</p43_paperID>
  <p43_contribution_type>Poster</p43_contribution_type>
  <p43_acceptance>Akzeptiert</p43_acceptance>
  <p43_authors>Seltmann, Melanie
Breuer, Ludwig Maximilian
Heinisch, Barbara</p43_authors>
  <p43_organisations>Universität Wien
Universität Wien
Universität Wien</p43_organisations>
  <p43_emails>melanie.seltmann@univie.ac.at
ludwig.maximilian.breuer@univie.ac.at
barbara.heinisch@univie.ac.at</p43_emails>
  <p43_presenting_author>Seltmann, Melanie</p43_presenting_author>
  <p43_title> Kollaborativ arbeiten und annotieren – Die Forschungsinfrastruktur des Spezialforschungsbereichs Deutsch in Österreich </p43_title>
  <p43_abstract>&lt;p&gt;Der Spezialforschungsbereich (SFB) „Deutsch in Österreich. Variation – Kontakt – Perzeption“ (FWF F 60) beschäftigt sich mit der Vielfalt sowie dem Wandel der deutschen Sprache in Österreich. Dabei behandelt er den Gebrauch und die subjektive Wahrnehmung von deutscher Sprache in Österreich und zeigt Einflüsse durch Kontaktsprachen auf. Der SFB gliedert sich in verschiedene Teilprojekte an vier verschiedenen Institutionen (Universität Wien, Universität Salzburg, Universität Graz und Österreichische Akademie der Wissenschaften) mit verschiedenen Schwerpunkten. &lt;/p&gt;
&lt;p&gt;Ein Ziel des SFB ist es, die Forschungsansätze und -ergebnisse einem möglichst breiten Publikum einfach und frei zugänglich zu machen. Hierfür ist eine gut durchdachte, funktionale und vor allem flexible Forschungsplattform sowie das Zurverfügungstellen (und die Nutzung) einiger Standards und Best Practices unumgänglich. Zum einen um die Arbeit über die verschiedenen Teilprojekte und Institutionen kollaborativ und einheitlich zu gestalten, zum anderen um den Open Science-Ansatz des Projektes verwirklichen zu können und Daten sowie Forschungsmethoden und -ergebnisse bereit stellen zu können. &lt;/p&gt;
&lt;p&gt;Hierzu ist eine breite - auch technische - Expertise nötig, da die Forschungsplattform den gesamten Forschungsprozess unterstützt und begleitet. Die Forschungsplattform dient dabei als Virtual Research Environment (VRE) (Sarwar et al. 2013: 551, Smith et al. 2011:54). Sie bietet Tools wie eine Call-Center-Maske für die Gewährspersonenakquise und -befragung, Transkriptions-, Annotations-, Kommunikations-, Arbeitsmanagement- und Dateienmanagementools – damit das ganze Spektrum des wissenschaftlichen Arbeitsprozesses von der InformantInnenakquise über die Datenauswertung bis hin zur Publikation unterstützt werden kann. Schließlich werden alle Prozesse und Daten nicht nur nachhaltig gespeichert, sondern auch weiterentwickelt und aufbereitet, damit sie WissenschafterInnen sowie der interessierten Öffentlichkeit zur Verfügung gestellt werden können. Eine Herausforderung stellen die Vielfalt der theoretischen Ansätze, praktischen Methoden, Datenformate, (u.a. Textkorpora, Audio- und Videodaten) und die (Meta-)Datenaufbereitung sowie die damit verbundene Sicherung und Nutzung dar. &lt;/p&gt;
&lt;p&gt;Die Grundlage der einzelnen Module der Forschungsplattform basiert auf Open Source-Tools, die gegebenenfalls an die Bedürfnisse des SFB angepasst werden. Die Tools werden dabei in Docker-Containern (Merkel 2014) gespeichert, was den Beitrag zur Nachhaltigkeit noch erweitert. Dies bietet den Vorteil, dass auch abseits des Projekts und sogar der Wissenschaft an den einzelnen Komponenten weiterentwickelt werden kann, insofern sich eine entsprechende Community bildet. Zudem bieten die Docker-Container die Möglichkeit, Tools sehr einfach für andere Projekte nutzen zu können. Die Docker-Container werden im SFB in Rancher (https://rancher.com/) verwaltet. Dies vereinfacht die gesamte Pipeline von der Entwicklung über das Testen bis hin zur produktiven Nutzung der Tools, indem alles an ein und derselben Stelle ausführbar ist.&lt;/p&gt;
&lt;p&gt;Auf dem Poster soll neben der Darstellung dieser Plattform genauer auf den Bereich der Annotation eingegangen werden. Da diese auf den verschiedensten linguistischen Ebenen geschieht und von einer Vielzahl von WissenschafterInnen gemacht wird, muss es zum einen klare Vorgaben geben, wie annotieren werden soll, zum anderen müssen die Annotationen nach einem einheitlichen Schema gebildet werden. Dennoch soll die Flexibilität der Annotationen, die das wissenschaftliche Arbeiten und den Erkenntnisprozess bedingen, gewährleistet sein. Dabei werden die Tags zwar mit Hilfe einer m:n-Verknüpfung zwischen Tags und Antworten erzeugt, sind aber hierarchisch projizierbar. Somit ist es möglich, dass die verschiedenen Teilprojekte unterschiedlich tief annotieren, aber dennoch auf die Annotationen der anderen Teilprojekte zurückgreifen können. Sie fügen sich zudem bestmöglich in die vorhandene Forschungslandschaft ein, z.B. durch Verwendung gängiger Taggingsysteme, um auch SFB-übergreifend leicht nutz-, adaptier- und vergleichbar zu sein.&lt;/p&gt;
</p43_abstract>
  <p44_paperID>190</p44_paperID>
  <p44_contribution_type>Poster</p44_contribution_type>
  <p44_acceptance>Akzeptiert</p44_acceptance>
  <p44_authors>Klemstein, Franziska</p44_authors>
  <p44_organisations>Technische Universität Berlin, Deutschland</p44_organisations>
  <p44_emails>f.klemstein@gmail.com</p44_emails>
  <p44_presenting_author>Klemstein, Franziska</p44_presenting_author>
  <p44_title>Denkmalpflege in der DDR. Analoge Netzwerke digital – Chancen und Möglichkeiten</p44_title>
  <p44_abstract>&lt;p&gt;Die klassische Kunstgeschichte verzichtet noch heute weitestgehend auf die Möglichkeiten, die unsere digitale Welt uns bietet. Zwar werden digitale Werkzeuge bereits vielfältig genutzt, jedoch bisher häufig ohne ausreichende Reflexion und Rückkopplung in die Lehre.[1]&lt;/p&gt;
&lt;p&gt;Innerhalb meines Dissertationsprojektes zum Thema „Denkmalpflege zwischen System und Gesellschaft – Netzwerke der Denkmalpflege im Sozialismus“ habe ich es mir zum Ziel gesetzt sowohl eine technikgeschichtliche Methode zur Darstellung von Handlungen und Strukturen zu nutzen als auch analoge Netzwerke digital abzubilden.&lt;/p&gt;
&lt;p&gt;Die technikhistorische Methode basiert auf dem von Wolfgang König entwickelten Akteur-Struktur-Modell (ASM), das eine Kombination von Handlungs- und Strukturtheorie darstellt. Innerhalb der Kunstgeschichte und Denkmalpflege fand diese Methode bisher jedoch kaum Beachtung. Die Anwendung dieses Modells innerhalb einer architekturhistorischen Arbeit, soll den Blick auf einen Themenbereich weiten, der bislang häufig nur Teilaspekte oder regionale Entwicklungen beschränkt wurde. Das Akteur-Struktur-Modell stellt dabei den Versuch dar, Handlungen und Strukturen strikt symmetrisch zu behandeln, da Strukturen aus Handlungen hervorgehen und Handlungen aus Strukturen.[2] Dabei wird zwischen verschiedenen Handlungsebenen (Makro-, Meso-, Mikroebene) unterschiedenen. Strukturen stehen hingegen „für Tradition und für Dauer, für soziokulturelle Verfasstheiten, in denen sich die Akteure bewegen und bewegen müssen.“[3] Strukturen bilden somit den Handlungsrahmen oder Spielraum der handelnden Personen (Mikroebene), Organisationen (Mesoebene) oder auch der Regierungen (Makroebene), wobei deren Handlungen bestehende Strukturen sowohl stabilisieren als auch destabilisieren können.&lt;/p&gt;
&lt;p&gt;Zugleich sollen mit der Anwendung des Modells auch seine Grenzen und Probleme aufgezeigt werden, die sich ergeben, wenn ein Modell aus einem anderen Wissenschaftsbereich für die Kunstgeschichte nutzbar gemacht wird. Obwohl Wolfgang König auf den Begriff des Netzwerkes verzichtet, möchte ich diesen innerhalb meines Dissertationsprojektes verwenden und folge hierbei Christoph Hubig, welcher vorschlägt, die Dynamik zwischen Akteuren und Strukturen mit Hilfe der Netzwerkmetapher zu modellieren.&lt;sup&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/sup&gt; Dies erscheint sinnvoll, da die Protagonisten der Denkmalpflege der DDR formale Beziehungen&lt;sup&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/sup&gt; miteinander unterhalten haben, welche die Dynamik innerhalb der scheinbar festen Strukturen, welche das sozialistische System geprägt beziehungsweise festgelegt hat, überhaupt erst möglich werden ließ. In diesem Sinne möchte ich auch die graphbasierte Datenbank neo4j nutzbar machen und den Netzwerkbegriff nicht nur als Metapher verwenden.&lt;/p&gt;
&lt;p&gt;Jeder Teil unseres Lebens wird von zahlreichen Verbindungen geprägt, so auch die institutionellen wie auch persönlichen Netzwerke innerhalb der Denkmalpflege der DDR. Es reicht mir jedoch nicht aus, diese Netzwerke lediglich aufzuzeigen. Vielmehr sollen die vernetzten Informationen (Personen, Dinge, Orte usw.) abgespeichert und durch unterschiedliche Abfragen in ihrer Vielschichtigkeit analysierbar sein. Die Informationen innerhalb eines Netzwerkes sollen dem entsprechend weder ignoriert noch in irgendeiner Weise zusammengefasst werden, sondern in ihrer Detailliertheit erfasst werden. Mehr noch als „gephi“[6], das vor allem als Visualisierungstool von Netzwerken geeignet ist, bietet die Graph-Datenbank „neo4j“[7] die Möglichkeit starke Vernetzungen darzustellen.&lt;/p&gt;
&lt;p&gt;Anhand meines Posters möchte ich einerseits die Möglichkeiten aufzeigen, die sich durch die Nutzung digitaler Werkzeuge (gephi und neo4j), besonders im Hinblick auf die Darstellung und Analyse von Netzwerken, ergeben und andererseits einen Vergleich zwischen analogen und digitalen Methoden anstellen.&lt;/p&gt;

&lt;p&gt;[1] Eine Ausnahme bildet in diesem Bereich der „Arbeitskreis digitale Kunstgeschichte“, dessen Mitglieder sich engagiert für einen reflektierten Einsatz digitaler Methoden einsetzen und dies bereits selbst umsetzen.&lt;/p&gt;
&lt;p&gt;[2] König, Wolfgang: Strukturen und Akteure – Ein Vorschlag zur Konzeptualisierung technisch-historischer Entwicklung, S. 505-516, in: EWE 24 (2013) 4, S. 514.&lt;/p&gt;
&lt;p&gt;[3] Ebd., S. 512.&lt;/p&gt;
&lt;p&gt;[4] Vgl.: König, Wolfgang: Technik und Geschichte. Interdisziplinarität, Theorien und Modelle, S. 605-616, in: EWE 24 (2013) 4, S. 605. Ebenso wie: Hubig, Christoph: Strukturdynamik und/oder Netzdynamik – Die Rolle der Akteure, S. 545-547, in: EWE 24 (2013) 4, S. 546-547.&lt;/p&gt;
&lt;p&gt;[5] Formale Beziehung meint soziale Beziehungen. König lehnt den Netzwerkbegriff ab, da er teilweise „realistisch“ und teilweise modellistisch verwendet wird und es bei der modellistischen Verwendung nichts gibt, was sich nicht in Netze integrieren ließe. Allerdings ist die Netzmetapher bei Akteuren, die formale Beziehungen – also „echte“ Beziehungen im Sinne von sozialen Beziehungen – unterhalten, Königs Ansicht nach durchaus gerechtfertigt, weshalb ich den Netzwerkbegriff innerhalb meines Dissertationsprojekts durchaus für sinnvoll erachte.&lt;/p&gt;
&lt;p&gt;[6] https://gephi.org/&lt;/p&gt;
&lt;p&gt;[7] https://neo4j.com/&lt;/p&gt;
</p44_abstract>
  <p45_paperID>112</p45_paperID>
  <p45_contribution_type>Poster</p45_contribution_type>
  <p45_acceptance>Akzeptiert</p45_acceptance>
  <p45_authors>Aehnlich, Barbara
Seidel, Henry</p45_authors>
  <p45_organisations>FSU Jena, Deutschland
HU Berlin, Deutschland</p45_organisations>
  <p45_emails>barbara.aehnlich@uni-jena.de
hnrseidel@gmail.com</p45_emails>
  <p45_presenting_author>Aehnlich, Barbara
Seidel, Henry</p45_presenting_author>
  <p45_title>DH-Toolvergleich im Hinblick auf Texte historischer Sprachstufen</p45_title>
  <p45_abstract>&lt;p&gt;Mittlerweile versprechen zahlreiche Tools eine mehr oder minder problemlose Lemmatisierung und Annotierung mit Part-of-Speech-Tags von Texten; viele sollen auch für historische (oder andere nicht-standardisierte) Sprachdaten nutzbar sein.[1] Dabei birgt die Verarbeitung historischer Sprachdaten des Deutschen zahlreiche Probleme aufgrund des hohen Grads an Variation, insbesondere auf den Ebenen Phonologie und Graphematik, aber auch in den Bereichen der Morphologie, Syntax und Lexik. Bei einer automatischen Verarbeitung solcher Daten stellen vor allem die Variationen in Phonologie, Graphematik und Morphologie ein besonderes Hindernis dar.&lt;/p&gt;
&lt;p&gt;Das Poster stellt zwei der Tools vor, deren Anwendung auf Texte nicht-standardisierter Sprachstufen exemplarisch anhand zweier Textsorten aufgrund bestimmter Kriterien verglichen wurde. Zum einen handelt es sich um gedruckte deutschsprachige Rechtstexte der Frühen Neuzeit, also aus der Rezeptionszeit des römischen Rechts in Deutschland, deren Sprache in einem Projekt erforscht werden soll, zum anderen um Fürstinnen-Briefe aus einem an der Friedrich-Schiller-Universität Jena erstellten digitalen Korpus.[2] In beiden Fällen weisen die Quellen frühneuhochdeutschen Sprachstand auf. Anhand ausgewählter Beispiele aus den vorliegenden Texten sollen zwei gängige elektronische Werkzeuge miteinander verglichen werden – EXMARaLDA und LAKomp.&lt;/p&gt;
&lt;p&gt;EXMARaLDA wurde für das computergestützte Arbeiten mit überwiegend mündlichen Korpora entwickelt, wird aber regelmäßig auch für schriftliche Sprachdaten verwendet, so auch bei den &lt;em&gt;Frühneuhochdeutschen Fürstinnenkorrespondenzen im mitteldeutschen Raum&lt;/em&gt;. Das Tool besteht im Wesentlichen aus einem Transkriptions- und Annotationseditor, einem Werkzeug zum Verwalten von Korpora und einem Such- und Analysetool.[3]&lt;/p&gt;
&lt;p&gt;Das Werkzeug LAKomp[4] wurde im Projekt SaDA (Semiautomatische Differenzanalyse von komplexen Textvarianten)[5] entwickelt und dient der Aufbereitung eines historischen Korpus. Nach der Transkription können die Texte hier lemmatisiert und annotiert werden. Aufgrund der Besonderheiten bei frühneuhochdeutschen Handschriften und Drucken wird der Lemmatisierungs- und Annotationsvorgang komplett manuell durchgeführt. Dabei ist dem Benutzer mit LAKomp ein Werkzeug an die Hand gegeben, das ihn sehr schnell und präzise große Textmengen bearbeiten lässt und damit den Mehraufwand händischer Annotation nahezu ausgleicht.&lt;/p&gt;
&lt;p&gt;Damit liegen zwei Werkzeuge vor, bei denen manuell annotiert werden muss, die aber dennoch bestimmte Unterschiede aufweisen, die für Nutzerinnen und Nutzer, die mit nicht-standardisierten Sprachdaten arbeiten, je nach Arbeitsziel vor- oder nachteilig sein können. So sind etwa bei LAKomp die halbautomatische Annotation auf der Grundlage des DWB und die Ausgabefunktion besonders gelungen, leider kann hier aber bisher nur lemmabasiert annotiert werden; bei EXMARaLDA ermöglichen die flexiblen Annotationskriterien eine besondere Breite von möglichen Annotationen, eine automatische oder halbautomatische Annotation des frühneuhochdeutschen Textmaterials ist jedoch bislang auch mit Hilfsmitteln wie dem Tree­tagger[6] nicht ohne weiteres möglich und die Daten können nur als EXMARaLDA-Korpus und nicht in Form einer digitalen Edition ausgegeben werden.&lt;/p&gt;
&lt;p&gt;Die beiden genannten Tools werden auf dem Poster hinsichtlich ihrer Anwendbarkeit auf Texte historischer Sprachstufen anhand folgender Kriterien verglichen: Funktionalitäten, Nutzerfreundlichkeit (Technischer Support, Qualität des Handbuchs, Verständlichkeit der Benutzeroberfläche, Verfügbarkeit eines Editors…) und Nachnutzbarkeit. Das Poster wird diesen Tool-Vergleich anhand ausgewählter Beispiele aus einem Rechtsbuch sowie einem Fürstinnenbrief aus dem 16. Jahrhundert präsentieren und stellt somit Überblick und Evaluation der Werkzeuge gleichermaßen dar.&lt;/p&gt;
&lt;p&gt;[1] In Auswahl: CATMA, CorA, EXMARaLDA, GATE, LAKomp, WebAnno&lt;/p&gt;
&lt;p&gt;[2] https://archive.thulb.uni-jena.de/hisbest/content/below/index.xml?XSL.DisplayComponentBrowse=true; http://www.laudatio-repository.org/repository/corpus/LAUDATIO%3AFuerstinnenkorrespondenz/TEI-header_version4_Schema7_2017-03-06T08%3A38%3A26%3A247Z&lt;/p&gt;
&lt;p&gt;[3] Für genauere Informationen vgl. http://exmaralda.org/de/ueber-exmaralda/&lt;/p&gt;
&lt;p&gt;[4] LAKomp steht für &lt;strong&gt;L&lt;/strong&gt;emmatisierung, &lt;strong&gt;A&lt;/strong&gt;nnotation, &lt;strong&gt;Komp&lt;/strong&gt;aration.&lt;/p&gt;
&lt;p&gt;[5] http://www.informatik.uni-halle.de/ti/forschung/ehumanities/sada/&lt;/p&gt;
&lt;p&gt;[6] http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/&lt;/p&gt;
</p45_abstract>
  <sessionID>60</sessionID>
  <p46_paperID>241</p46_paperID>
  <p46_contribution_type>Poster</p46_contribution_type>
  <p46_acceptance>Akzeptiert</p46_acceptance>
  <p46_authors>Mondaca, Francisco
Rolshoven, Jürgen
Schildkamp, Philip
Vogt, Andreas</p46_authors>
  <p46_organisations>Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p46_organisations>
  <p46_emails>f.mondaca@uni-koeln.de
rols@spinfo.uni-koeln.de
philip.schildkamp@uni-koeln.de
vogt.andreas@uni-koeln.de</p46_emails>
  <p46_presenting_author>Schildkamp, Philip</p46_presenting_author>
  <p46_title>Syndred - A Syntax-Driven Editor for Lexical Resources</p46_title>
  <p46_abstract>&lt;p&gt;Die Strukturierung textueller Daten erfolgt manuell, computergestützt, halb- oder vollautomatisch. Die letzteren Verfahren sind von praktischem Interesse, da die lediglich händische oder halbautomatische Strukturierung von Daten oftmals aufwendiger als deren Erzeugung ist. Gleichzeitig ist mit der automatischen Strukturierung ein Gewinn an Flexibilität und Dynamik verbunden, der für die Prozessierung großer Datenmengen zunehmend an Bedeutung gewinnt (Bernstein et al. 2016).&lt;/p&gt;
&lt;p&gt;In Syndred[1] wird die inhärente Struktur textueller Daten im Sinne domänenspezifischer, formaler Sprachen durch Produktionssysteme -formale Grammatiken formuliert in Extended Backus-Naur Form (EBNF)- modelliert. Die formalen Grammatiken steuern einen Parser, der hierarchische Strukturen für textuelle Daten in Gestalt von Strukturbäumen erzeugt und dabei als Seiteneffekt die formale Korrektheit der Texteingabe überprüft. Die automatisch erzeugten Strukturbäume können in vielfältiger Weise weiter prozessiert oder persistiert werden, z.B. als TEI-Dokumente oder mit Hilfe von Treewalkern (Parr 2013: 17 ff) gezielt für andere Anwendungen transformiert oder angereichert werden.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Für das beschriebene System gibt es zwei Modi operandi, einerseits die Analyse gegebener, vorhandener textueller Daten, andererseits die Analyse und Kontrolle der Textproduktion durch Editoren. Letztere Funktion drückt sich in der Projektbezeichnung Syntax kontrollierte Editoren –Syndred, Syntax-Driven Editor- aus. Wegweisend für die Entwicklung syntaxkontrollierter Editoren sind die Arbeiten von Gutknecht (1985) und Wirth, Gutknecht (1992: 78 ff). In Compilern wird jedoch meist die Analyse des zu übersetzenden Programms direkt in Programmcode vorgenommen (Wirth 1986: 43).&lt;/p&gt;
&lt;p&gt;Im Gegensatz dazu definiert Syndred textuelle Strukturen durch domänenspezifische Grammatiken und bildet sie auf Syntaxgraphen ab, die Parser steuern. Damit ist der syntaxkontrollierte Editor ein flexibles Werkzeug für unterschiedliche, dynamische und kollaborative Projekte, die u.a. in der Sprachlichen Informationsverarbeitung verfolgt werden. Dazu zählen Lexikoneditoren -Pledari Grond[2]- und kollaborative Korrektur -Digitale Rätoromanische Chrestomathie (DRC)[3], Annotierte Rätoromanische Chrestomathie (ARC)[4]- oder die Digitalisierung von Bibliothekskatalogkarten. Kollaboratives Arbeiten wird durch effiziente formale Kontrolle des Editors unterstützt.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Ein solches Arbeiten erfordert eine netzwerkbasierte, dezentrale Architektur, idealerweise eine Web-Applikation mit clientseitigem Editor und serverseitigem Parser. Dies hat nicht nur den Vorteil der Plattformunabhängigkeit, weiter wird auch die Rechenleistung und die Persistierung anfallender Daten an zentraler Stelle von leistungsfähiger Hardware übernommen.&lt;/p&gt;
&lt;p&gt;Bereits im Vorlauf des vorliegenden Beitrags wurde ein System implementiert, welches eine lose Kopplung zwischen den zentralisierten Komponenten und multiplen, dezentralen Bearbeitungsinstanzen zulässt. Auf Seite der Clients wird das auf React[5] basierende Editor-Framework DraftJS[6] eingesetzt, welches mittels im JSON-Format abgewickelter Kommunikation an eine serverseitige Java-Applikation auf Basis des Spring[7]-Frameworks gekoppelt ist. Um eine lose Kopplung handelt es sich hier, da es ohne weiteres möglich ist die clientseitige Schnittstelle als native Software zu implementieren oder ein alternatives Web-Frontend anzubinden.&lt;/p&gt;
&lt;p&gt;Neben der Verwaltung der verschiedenen Kollaborationsinstanzen und der Datenpersistenz ist die Hauptaufgabe der serverseitigen Infrastruktur die Bereitstellung multipler Parser-Threads. Dabei soll das als Thread-Pooling bezeichnete Vorgehen zum Einsatz kommen, um jeder Instanz nach Bedarf einen eigenen Parser-Thread zur Verfügung zu stellen. Zum Einen sichert dies die zeitnahe Verfügbarkeit aller für die kollaborative Bearbeitung notwendiger Ressourcen und zum anderen sorgt die informationstechnische Verkapselung der Datenverarbeitung im Parser durch das Threading für strukturelle Vereinfachung, da Speicherzuweisung wie -verwaltung und das CPU-Scheduling komplett an das Betriebssystem abgetreten werden und es somit nicht zu Kollisionen zwischen verschiedenen Instanzen kommen kann -was auch im Hinblick auf Datensicherheit von Vorteil ist.&lt;/p&gt;
&lt;p&gt;Ziel ist die Implementierung eines Editors im Split-View Design, welches einerseits das Erstellen und Bearbeiten der EBNF-Grammatiken und andererseits syntaxkontrolliertes Editieren von textuellen Daten nebeneinander zulässt. Damit soll die Möglichkeit geboten werden Grammatiken aus bestehenden Textdaten abzuleiten oder Arbeiten an textuellem Material von bestehenden Grammatiken anleiten und Prüfen zu lassen, wobei sowohl die Grammatik wie auch die Textdaten in direktem, visuellen Bezug zueinanderstehen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;br clear="all" /&gt;
&lt;p&gt;[1]                                https://github.com/spinfo/syndred &lt;br clear="all" /&gt;  &lt;/p&gt;

&lt;p&gt;[2]                                http://pledarigrond.ch/rumantschgrischun/ &lt;br clear="all" /&gt;  &lt;/p&gt;

&lt;p&gt;[3]                                http://www.spinfo.phil-fak.uni-koeln.de/24652.html &lt;br clear="all" /&gt;  &lt;/p&gt;

&lt;p&gt;[4]                                http://www.spinfo.phil-fak.uni-koeln.de/24653.html &lt;br clear="all" /&gt;  &lt;/p&gt;

&lt;p&gt;[5]                                https://facebook.github.io/react/ &lt;br clear="all" /&gt;  &lt;/p&gt;

&lt;p&gt;[6]                                https://draftjs.org/ &lt;br clear="all" /&gt;  &lt;/p&gt;

&lt;p&gt;[7]                                https://projects.spring.io/spring-framework/ &lt;br clear="all" /&gt;  &lt;/p&gt;

</p46_abstract>
  <p47_paperID>206</p47_paperID>
  <p47_contribution_type>Poster</p47_contribution_type>
  <p47_acceptance>Akzeptiert</p47_acceptance>
  <p47_authors>Reinöhl, Uta
Kölligan, Daniel
Kiss, Börge
Mondaca, Francisco
Neuefeind, Claes
Sahle, Patrick</p47_authors>
  <p47_organisations>Universität zu Köln
Universität zu Köln
Universität zu Köln
Universität zu Köln
Universität zu Köln
Universität zu Köln</p47_organisations>
  <p47_emails>uta.reinoehl@uni-koeln.de
d.koelligan@uni-koeln.de
kiss@mailbox.org
f.mondaca@uni-koeln.de
c.neuefeind@uni-koeln.de
sahle@uni-koeln.de</p47_emails>
  <p47_presenting_author>Kiss, Börge
Mondaca, Francisco
Neuefeind, Claes</p47_presenting_author>
  <p47_title> VedaWeb – eine webbasierte Plattform für die Erforschung altindischer Texte</p47_title>
  <p47_abstract>&lt;p&gt;Das vorgeschlagene Poster thematisiert den technischen Entwurf und den funktionalen Aufbau von VedaWeb, einer webbasierten Plattform für die sprachwissenschaftliche Erforschung altindischer Texte (siehe http://vedaweb.uni-koeln.de). &#x2028;Das 2017 begonnene Vorhaben wird als Kooperationsprojekt an der Universität zu Köln durchgeführt, in enger Zusammenarbeit zwischen Fachwissenschaftlern des Instituts für Linguistik (Allgemeine sowie Historisch-Vergleichende Sprachwissenschaft, Sprachliche Informationsverarbeitung) sowie dem Cologne Center for eHumanities (CCeH). Das Projekt mit dreijähriger Laufzeit wird von der DFG in der LIS-Förderlinie „eResearch-Technologien“ gefördert.&lt;/p&gt;
&lt;p&gt;Über die VedaWeb-Plattform werden altindische, in Sanskrit verfasste Texte digital einsehbar, morphologisch und metrisch annotiert sowie nach lexikographischen und korpuslinguistischen Kriterien durchsuchbar zur Verfügung gestellt. Das Projekt versteht sich als ein Baustein, um den in Köln bestehenden Schwerpunkt auf südasiatische Sprachen weiter zu stärken. Als Pilottext dient zunächst der Rigveda, einer der ältesten und wichtigsten Texte der indogermanischen Sprachfamilie. Der Rigveda ist in der ältesten Sprachform des Altindischen, dem Vedischen, verfasst. Seine Entstehung kann auf das späte zweite Jahrtausend v. Chr. datiert werden. Mit einem Umfang größer als die Ilias und Odyssee zusammen stellt er eine überaus reiche Datengrundlage dar. Perspektivisch sollen auch weitere Texte wie etwa der Atharvaveda, Yajurveda und vedische Prosatexte in die VedaWeb-Plattform integriert werden.Es wird angestrebt, das VedaWeb längerfristig zur zentralen Anlaufstelle für die internationale Fachgemeinschaft, die mit altindischen Primärtexten arbeitet, auszubauen.&lt;/p&gt;
&lt;p&gt;Ausgangspunkt und Grundlage des Projekts ist eine vollständige morphologische (d.h. wortstrukturelle) Annotation des Rigveda, die im Vorfeld an der Universität Zürich durchgeführt und dem Projekt zur Verfügung gestellt wurde. Hinzu kommen zum einen metrische Informationen (Kevin Ryan, Harvard University, siehe http://www.meluhha.com/rv/) sowie perspektivisch auch syntaktische Informationen aus verschiedenen abgeschlossenen und andauernden Forschungsprojekten als weitere Annotationsebenen. Anhand dieser Annotationen werden im Projekt verschiedene Recherche- und Analysewerkzeuge entwickelt und sukzessive in die VedaWeb-Plattform integriert. Hierzu gehören eine kombinierte Suchfunktion nach linguistischen Parametern (u.a. Lemmata, Wortformen, morphologischen und metrischen Informationen), die Verknüpfung mit dem Standardwörterbuch zum Rigveda von Hermann Grassmann (1873), die Anzeige von Übersetzungen (u.a. Grassmann 1876, Geldner 2003, Griffith 1896) und von Kommentaren (Oldenberg 1909/1912), sowie die Möglichkeit des Exports von annotierten Textabschnitten nach vom Nutzer gewählten Kriterien. Von zentraler Bedeutung ist die Verknüpfung des Rigveda mit der am CCeH angesiedelten Portalseite für Sanskritwörterbücher (Cologne Sanskrit Lexicon, siehe http://www.sanskrit-lexicon.uni-koeln.de), einer zentralen Anlaufstelle für die internationale Sanskritforschung. Auf Basis einer Modellierung der Daten in TEI werden die Wortformen über die jeweiligen Lemmata mit dem digital erfassten Wörterbuch von Grassmann verknüpft,&#x2028;so dass sowohl vom Text auf das Wörterbuch verwiesen wird als auch umgekehrt vom Wörterbuch aus Textstellen aufgesucht werden können. Auf diese Weise kann über das Lemma gleichzeitig auch eine Verknüpfung zu den weiteren Sanskrit-Wörterbüchern hergestellt werden, etwa um vergleichende, wörterbuchübergreifende Recherchen zu ermöglichen.&lt;/p&gt;
&lt;p&gt;Der Schwerpunkt des Posters liegt auf der Präsentation des Systementwurfs der VedaWeb-Plattform sowie der dort eingesetzten Technologien. Dies umfasst zum einen eine Beschreibung der funktionalen Elemente der Nutzerschnittstelle, die dem Anwender als Forschungsumgebung dient, indem sie verschiedene Werkzeuge bereitstellt (z.B. Suche, Verlinkung, Export in TEI-Format). Zum anderen werden die Architektur des zugrundeliegenden Systems&#x2028;und die für deren Umsetzung verwendeten Technologien thematisiert. Grundlage bildet eine REST-API über Spring (siehe https://spring.io/). Für die Suchlogik wird Lucene eingesetzt, unter Verwendung eines Solr-Servers (siehe http://lucene.apache.org/solr/). Das Frontend wird komponentenbasiert als Single Page App (SPA) unter Verwendung von Ajax-Techniken umgesetzt, u.a. durch die Verwendung gängiger JavaScript-Frameworks wie etwa AngularJS, siehe https://angularjs.org/, oder auch React, siehe https://facebook.github.io/react/). Mit der Fokussierung auf den Systementwurf möchten wir mit dem Poster vor allem einen kompakten Überblick über die Nutzungsmöglichkeiten sowie über die technische Funktionsweise der VedaWeb-Plattform geben.&lt;/p&gt;
</p47_abstract>
  <p48_paperID>194</p48_paperID>
  <p48_contribution_type>Poster</p48_contribution_type>
  <p48_acceptance>Akzeptiert</p48_acceptance>
  <p48_authors>Helling, Patrick
Mathiak, Brigitte</p48_authors>
  <p48_organisations>Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p48_organisations>
  <p48_emails>patrick.helling@uni-koeln.de
mathiak@gmail.com</p48_emails>
  <p48_presenting_author>Helling, Patrick
Mathiak, Brigitte</p48_presenting_author>
  <p48_title>Sechs Wege der FRBRisierung von Textverknüpfungen</p48_title>
  <p48_abstract>&lt;p&gt;Linked Open Data wird auch in den Geisteswissenschaften immer wichtiger. Dabei geht es sehr oft um die Verknüpfung von Text mit weiterem Text, z.B. mit Kommentaren, Referenzen auf andere Werke oder anderen Entitäten, wie Personen, Orte, etc. Mit Ontologien wie den Functional Requirements for Bibliographic Records object-oriented (FRBRoo) können zwar bibliographische Gegenstände im Rahmen einer digitalen Datenbank auf eine Art und Weise modelliert und bereitgestellt werden, dass diese allein durch ihre Organisation und objektübergreifende Strukturierung und Verknüpfung bibliographisch-informationellen Mehrwert im Rahmen der Recherche erzeugen können. Allerdings ist nicht immer klar, wie Verknüpfungen aus den Texten heraus mit anderen Texten oder aber zu komplett anderen Entitäten zu modellieren sind.&lt;/p&gt;
&lt;p&gt;Die Verknüpfung von verschiedensten Dokumenten und Entitäten ist jedoch eine der Hauptideen bei der Benutzung von Ontologien und Linked Open Data. Auf dem Poster präsentieren wir zu diesem Zweck eine grundlegende Ontologie-Struktur und stellen verschiedene modulare Modellierungsstrategien genauer vor. Zur Veranschaulichung werden wir eine Umsetzung unserer ontologischen Konzepte an einem Beispiel aus den Geisteswissenschaften demonstrieren.&lt;/p&gt;
</p48_abstract>
  <p49_paperID>280</p49_paperID>
  <p49_contribution_type>Poster</p49_contribution_type>
  <p49_acceptance>Akzeptiert</p49_acceptance>
  <p49_authors>Schlesinger, Claus-Michael</p49_authors>
  <p49_organisations>Universität Stuttgart, Deutschland</p49_organisations>
  <p49_emails>claus-michael.schlesinger@ilw.uni-stuttgart.de</p49_emails>
  <p49_presenting_author>Schlesinger, Claus-Michael</p49_presenting_author>
  <p49_title>Die Max-Bense-Collection. Digitale Re-Publikation von Erstausgaben mit erweiterten Plattformfunktionen</p49_title>
  <p49_abstract>&lt;p&gt;Die Max-Bense-Collection versammelt ca. 700 Texte von Max Bense in Form von digitalisierten Erstausgaben, die zum Teil nicht oder nur schwer zugänglich sind. Das Projekt verfolgt drei Ziele: Erstens wird durch die Re-Publikation der Zugang zu den Texten für die Bense-Forschung und für die Forschungen zur Informationsästhetik erleichtert. Die digitale Re-Publikation der Erstausgaben ist insbesondere im Hinblick auf die auch für Benses Theoriebildung im Zusammenhang mit den Entwicklungen der Konkreten Poesie wichtige grafische und typografische Gestaltung der Texte forschungsrelevant. Zweitens ermöglicht die entwickelte Plattformlösung auf Basis von Omeka (http://www.omeka.org) den beteiligten Forscher*innen und Projektpartner*innen eine erweiterte Nutzung der digitalisierten Texte für projekt- und ausstellungsbegleitende Präsentationen und Kommentierungen ausgewählter Texte auf eigenen, projektbasierten Unterseiten der Plattform. Drittens dient die Plattform damit der Vernetzung von Nutzer*innen aus Forschung und beteiligten Kulturerbeinstitutionen, indem die Nutzung und projektbezogene Repräsentation der Materialien für die Institutionen nachvollziehbar und übersichtlich bleibt.&lt;/p&gt;
&lt;p&gt;Das Projekt ist eine Zusammenarbeit des Stuttgart Research Center for Text Studies mit dem Zentrum für Kunst und Medien Karlsruhe, das Deutsche Literaturarchiv Marbach stellt zusätzliches Material für die Sammlung bereit.&lt;/p&gt;
</p49_abstract>
  <p50_paperID>291</p50_paperID>
  <p50_contribution_type>Poster</p50_contribution_type>
  <p50_acceptance>Akzeptiert</p50_acceptance>
  <p50_authors>Vertan, Cristina
von Hahn, Walther</p50_authors>
  <p50_organisations>Universität Hamburg, Deutschland
Universität Hamburg, Deutschland</p50_organisations>
  <p50_emails>cristina.vertan@uni-hamburg.de
vhahn@informatik.uni-hamburg.de</p50_emails>
  <p50_presenting_author>Vertan, Cristina</p50_presenting_author>
  <p50_title>Eine Fallstudie zur Annotation von Vagheit in Werken Dimitrie Cantemirs</p50_title>
  <p50_abstract>&lt;p&gt;Besonders bei der Bearbeitung von historischen und multilingualen Texten erscheinen einige bisherige Techniken der DH diskussionswürdig. Vor allem ist es die Grundannahme, dass Datenbank-Inhalte, die durch Annotationen in Texten markiert wurden, logisch immer den Wert "wahr" haben müssen.  Dazu passt, dass hauptsächlich Nomen und Verben annotiert werden, nicht aber z.B. ihre negierte Vorkommen, so dass "p" und   "¬ p" gleichbedeutend werden.&lt;/p&gt;
&lt;p&gt;Im Projekt HerCoRe wird zum ersten Mal versucht, bei der Annotation von unscharfen Äußerungen verschiedene Ebenen der Vagheit zu annotieren und sie für die hermeneutische Interpretation durch den Geisteswissenschaftler transparent zu halten, da die Methoden der DH keine hermeneutische Potenz haben.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;
</p50_abstract>
  <sessionID>60</sessionID>
  <p51_paperID>106</p51_paperID>
  <p51_contribution_type>Poster</p51_contribution_type>
  <p51_acceptance>Akzeptiert</p51_acceptance>
  <p51_authors>Steiner, Elisabeth</p51_authors>
  <p51_organisations>Universität Graz, Österreich</p51_organisations>
  <p51_emails>elisabeth.steiner@uni-graz.at</p51_emails>
  <p51_presenting_author>Steiner, Elisabeth</p51_presenting_author>
  <p51_title>Virtuelle Ausstellungen und Rundgänge: digitalisiertes Kulturerbe vermitteln und präsentieren</p51_title>
  <p51_abstract>&lt;p&gt;Virtuelle Rundgänge, virtuelle Ausstellungen oder virtuelle Touren: Die Begriffe bezeichnen oft ähnliche Vorgehensweisen. Zusätzlich zu strukturierten Suchzugängen werden sie auf den unterschiedlichsten Portalen für digitalisiertes Kulturerbe in der Regel als Ergänzung angeboten. So vielfältig wie die möglichen Themen der Rundgänge sind die technischen Umsetzungen und Visualisierungen. Auch im Webportal „Kultur- und Wissenschaftserbe Steiermark“, einem Ergebnis des Projektes „Repositorium Steirisches Wissenschaftserbe“, ergänzen Virtuelle Rundgänge den strukturierten Suchzugang. Die gewählte Bibliothek StoryMapJS erfüllt den Zweck der Visualisierung der Rundgänge für dieses Projekt gut. Ein Desiderat ist jedoch die bessere Strukturierung und Standardisierung der Daten und Metadaten. Insgesamt wird die wissenschaftliche Auseinandersetzung mit Virtuellen/Digitalen Ausstellungen/Rundgängen oft vernachlässigt, weil sie in erster Linie als Verschönerung bzw. nachrangiges Angebot von Suchportalen verstanden werden. Ganz im Gegenteil sind diese Einstiegspunkte aber für das Publikum oft leichter verständlich und können einen besseren Überblick über die zu vermittelnde Thematik geben als hochstrukturierte Interfaces und rein textuelle Beschreibungen.&lt;/p&gt;
</p51_abstract>
  <p52_paperID>183</p52_paperID>
  <p52_contribution_type>Poster</p52_contribution_type>
  <p52_acceptance>Akzeptiert</p52_acceptance>
  <p52_authors>Wissik, Tanja
Krek, Simon
Jakubicek, Milos
Tiberius, Carole
Navigli, Roberto
McCrae, John
Tasovac, Toma
Varadi, Tamas
Koeva, Svetla
Costa, Rute
Kernerman, Ilan
Monachini, Monica
Trap-Jensen, Lars
Sandford Pedersen, Bolette
Hildenbrandt, Vera
Kallas, Jelena
Porta-Zamorano, Jordi</p52_authors>
  <p52_organisations>Österreichische Akademie der Wissenschaften, Österreich
Institut Josef Sefan, Slowenien
Lexical Computing CZ s.r.o.,Tschechien
Instituut voor Nederlandse Lexicologie, Niederlande
Università degli Studi di Roma La Spienza, Italien
National Universtiy of Ireland, Galway Irland
Centar za digitalne humanističke nauke, Serbien
Magyar Tudományos Akadémia,Ungarn
Institute for Bulgarian Language, Bulgarien
Universidade Nova de Lisboa, Portugal
K Dictionaries Ltd., Israel
Consiglio Nazionale delle Ricerche, Italien
Det Danske Sprog- og Litteraturselskab, Dänmark
Kobenhavns Universitet, Dänemark
Universität Trier, Deutschland
Eesti Keele Instituut, Estland
Real Academia Española; Spanien</p52_organisations>
  <p52_emails>Tanja.Wissik@oeaw.ac.at
simon.krek@guest.arnes.si
milos.jakubicek@sketchengine.co.uk
carole.tiberius@inl.nl
navigli@di.uniroma1.it
john@mccr.ae
ttasovac@humanistika.org
varadi@nytud.hu
svetla@dcl.bas.bg
rute.costa@fcsh.unl.pt
ilan@kdictionaries.com
monica.monachini@ilc.cnr.it
ltj@dsl.dk
vnb282@ku.dk
hildenbr@uni-trier.de
jelena.kallas@eki.ee
porta@rae.es</p52_emails>
  <p52_presenting_author>Wissik, Tanja</p52_presenting_author>
  <p52_title>ELEXIS – Eine europäische Forschungsinfrastruktur für lexikographische Daten</p52_title>
  <p52_abstract>&lt;p&gt;Die Bedeutung von Wörterbüchern, seien es einsprachige, zweisprachige oder mehrsprachige Wörterbücher, ist in der heutigen Informationsgesellschaft nicht zu unterschätzen. Sie geben nicht nur Auskunft über Wortbedeutungen und dazugehörige Übersetzungen, sondern sind selbst Bestandteil der Kulturgüter eines Landes und sie stellen bedeutende Ressourcen für Linked Open Data und Semantic-Web-Technologien. Obwohl in fast jedem Land Wörterbücher erstellt wurden und werden, seien es traditionelle Wörterbücher in gedruckter Form oder lexikographische Ressourcen in digitaler Form, waren die Bestrebungen nach Kooperation auf europäischer Ebene eher limitiert. Dies führte dazu, dass bis jetzt die Synergien zwischen traditioneller Lexikographie und maschineller Sprachverarbeitung nicht optimal genutzt werden konnten. Dies soll durch das Infrastrukturprojekt ELEXIS geändert werden.&lt;/p&gt;
&lt;p&gt;Im Rahmen des ELEXIS Projekts soll eine Infrastruktur für lexikographische Daten entwickelt werden, die auf mehreren Ebenen ansetzt und den Bereich der traditionellen Lexikographie mit dem Bereich der maschinellen Sprachverarbeitung miteinander verknüpft: Zum einen soll Kooperation und Austausch zwischen unterschiedlichen Forschungsgruppen, aber auch zwischen Forschungsgruppen und Verlagshäusern gefördert werden. Zum anderen soll an gemeinsamen Standards gearbeitet werden, um den Austausch und die Wiederverwendbarkeit von lexikographischen Daten in den unterschiedlichsten Szenarien zu fördern. Die Infrastruktur soll den Zugang zu Methoden, Tools und Daten ermöglichen und den bis jetzt nicht so weit verbreiteten Open-Access-Gedanken fördern.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Um diese Ziele umzusetzen, hat sich ein Konsortium von 17 Partnern gebildet. Untern den Partnern befinden sich Institutionen mit Expertise in Lexikographie, in maschineller Sprachverarbeitung, in Semantic-Web-Technologien und in den digitalen Geisteswissenschaften sowie nationale Sprachinstitutionen, Verlagshäuser und Partner mit Expertise im Bereich der Standardisierung und Normung. Nachfolgend die Liste der Partner (mit den Bezeichnungen in der jeweiligen Landessprache wie im Proposal angegeben): Institut Josef Sefan (Slowenien), Lexical Computing CZ s.r.o. (Tschechien), Instituut voor Nederlandse Lexicologie (Niederlande), Università degli Studi di Roma La Spienza (Italien), National Universtiy of Ireland, Galway (Irland), Österreichische Akademie der Wissenschaften (Österreich), Centar za digitalne humanističke nauke (Serbien), Magyar Tudományos Akadémia (Ungarn), Institute for Bulgarian Language (Bulgarien), Universidade Nova de Lisboa (Portugal), K Dictionaries Ltd (Israel), Consiglio Nazionale delle Ricerche (Italy), Det Danske Sprog- og Litteraturselskab (Dänmark), Kobenhavns Universitet (Dänemark),Universität Trier (Germany), Eesti Keele Instituut (Estland), Real Academia Española (Spanien).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Die ELEXIS Infrastruktur wird zum einen Tools und Services zur Erstellung, Verarbeitung und Retrodigitalisierung von lexikographischen Daten und zum anderen den Zugang zu bereits existierenden lexikographischen Daten anbieten. Damit die zukünftigen Nutzer das Potential der Infrastruktur vollends ausnutzen können, sind die Entwicklung von online Trainingsmaterialien sowie die Abhaltung von Trainingsworkshops geplant. Weiters sollen GastforscherInnen-Programme den Austausch zwischen den Forschungsgruppen aktiv fördern und für Forschungsvorhaben den Zugang zu Daten ermöglichen, die aus unterschiedlichen Gründen nicht Open Access zur Verfügung gestellt werden können.&lt;/p&gt;
&lt;p&gt;ELEXIS wird eng mit den bereits existierenden Forschungsinfrastrukturen CLARIN und DARIAH zusammenarbeiten und auf den bereits vorhanden Infrastrukturen aufbauen und zugleich diese beiden Infrastrukturen näher zusammenzubringen.&lt;/p&gt;
&lt;p&gt;In diesem Poster werden die Grundzüge des neuen europäischen Infrastrukturprojektes beschrieben sowie die Methoden und Maßnahmen, mit denen die oben genannten Ziele erreicht werden sollen, präsentiert. Weiters wird speziell auf den Nutzen und die Vorteile der ELEXIS Infrastruktur für die DH Community eingegangen.&lt;/p&gt;
</p52_abstract>
  <p53_paperID>187</p53_paperID>
  <p53_contribution_type>Poster</p53_contribution_type>
  <p53_acceptance>Akzeptiert</p53_acceptance>
  <p53_authors>Müller, Andreas</p53_authors>
  <p53_organisations>Universität Wien, Österreich</p53_organisations>
  <p53_emails>andreas.w.mueller@outlook.com</p53_emails>
  <p53_presenting_author>Müller, Andreas</p53_presenting_author>
  <p53_title>Digitized Inhumanities: Qualitative Inhaltsanalyse von Hexenprozessakten mit MAXQDA</p53_title>
  <p53_abstract>&lt;p&gt;&lt;strong&gt;Forschungskontext:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In den Sozialwissenschaften ist die qualitative Inhaltsanalyse unter Anwendung moderner Analysesoftware wie MAXQDA etabliert. In den Geschichtswissenschaften gewinnen diese Zugänge unter dem Schlagwort der „Digital Humanities“ erst langsam an Verbreitung. Meine Masterarbeit „Die Magie der Inhaltsanalyse: Entwurf einer Inhaltsanalyse für den Vergleich von Hexenprozessakten aus Rostock 1584 und Hainburg 1617/18“ versucht daher diese Zugänge der Sozialwissenschaften in einem methodisch eher „traditionellen“ Forschungsbereich, der historischen Hexenprozessforschung, anzuwenden.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ausgangspunkt:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Das Ausgangsmaterial bilden 37 „Geständnisse“ (Urgichten) aus Hexenprozessen in Hainburg 1617/18 und Rostock 1584. Die inhaltlich nach Befragungspunkten strukturierten Dokumente stehen am Ende des juristischen Prozesses vor der Hinrichtung der Angeklagten. Diese bilden eine Synthese aus den Ansichten des Gerichts, den Angaben von Zeugen und den unter Folter entstandenen Aussagen der Angeklagten.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Forschungsfrage(n):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;§  Wie unterscheiden sich die aus den Urgichten hervorgehenden Hexereiimaginationen in Hainburg 1617/18 und Rostock 1584?&lt;/li&gt;
&lt;li&gt;§  Wie spiegeln sich regionale Unterschiede (sozio-ökonomisch, konfessionell, politisch) in den Dokumenten wieder? Welche Elemente der Hexereivorstellung erweisen sich als starr, welche als flexibel?&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Methodik:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Methodische Grundlage bildet das Methodenangebot der qualitativen Inhaltsanalyse“ (Mayring 2015; Kuckartz 2014), die auf die spezifisch geschichtswissenschaftlichen Herausforderungen angepasst wurde. Als Analysetool wurde MAXQDA12 verwendet. Ein auf dem „elaborierten Hexereibegriff“ (Dillinger 2007) basierendes Kategoriensystem wurde deduktiv auf die Texte angewendet. Für die Analyse wurde ein Mixed-Methods Ansatz verfolgt, der die quantitative Auswertung des Kategoriensystems als „Kontrastmittel“ für die qualitative Analyse heranzieht.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ergebnisse&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Vor allem in der Imagination des Schadenszaubers traten deutliche Unterschiede hervor, welche die sozio-ökonomischen Kontexte widerspiegeln. Im Weinbaugebiet Hainburg findet sich vor allem der Vorwurf des Wetterzaubers gegen Weinreben, Obst und Getreide. In der Seehandelsstadt Rostock fehlen diese Delikte weitgehend und es steht vor allem die Verbreitung von Krankheiten durch Bettlerinnen im Zentrum. Die Vorstellungen vom Teufelspakt, Hexentanz und Flug sind deutlich homogener als der Schadenszauber, wenn sie auch verschiedene Schwerpunktsetzungen und Ausgestaltungen aufweisen. Darüber hinaus hat die quantitative Analyse überraschende Unterschiede in der Inhaltsstruktur zu Tage gefördert, die über den Rahmen der Arbeit offene Fragen aufwirft. Es entsteht dabei der Befund einer wesentlich stärker „integrierten“ bzw. „kohärenten“ Hexereiimagination in Hainburg, die sich in zahlreichen Kategorienüberschneidungen manifestiert, während in Rostock teils völlig isolierte Elemente und Narrative zu Tage treten.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diskussion:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Die Masterarbeit setzte sich als methodisches Ziel, den Vergleich zweier historischer Textkorpora unter Einsatz der qualitativen Inhaltsanalyse vorzunehmen. Angestrebtes Ziel einer nachfolgenden Dissertation ist es eine „vergleichende Inhaltsanalyse“ für den Einsatz insbesondere in den Geschichtswissenschaften zu entwerfen. Dafür scheint es angebracht diesen ersten Schritt zur kritischen Diskussion zu stellen und die Potenziale der qualitativen Inhaltsanalyse, aber auch die Möglichkeiten anderer Analyseverfahren, für den systematischen Textvergleich zu diskutieren.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Anliegen der Posterpräsentation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Das Poster selbst wird die Ergebnisse des erfolgten Vergleichs in Form von Grafiken und Diagrammen (Codematrix Browser, Code Relation Browser, Dokumentenvergleichsdiagramm aus MAXQDA) in das Zentrum rücken. Hierfür werden vor allem die über das quantifizierende „Kontrastmittel“ deutlich gewordenen strukturellen Unterschiede der Vergleichsgruppen aufgezeigt und erläutert. Hiermit erfüllt das Poster zwei Zwecke: 1. Ermöglicht es die Diskussion über und Kritik an dem gewählten Analyseverfahren. 2. Können die Ergebnisse als Anregung dienen die Anwendbarkeit ähnliche Verfahren für das jeweils eigene Forschungsfeld zu reflektieren.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Literatur:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dillinger, Johannes (2007). &lt;em&gt;Hexen und Magie. Eine historische Einführung.&lt;/em&gt; Frankfurt/M.: Spee.&lt;/p&gt;
&lt;p&gt;Kuckartz, Udo (2014). &lt;em&gt;Qualitative Inhaltsanalyse. Methoden, Praxis, Computerunterstüzung.&lt;/em&gt; Weinheim: Beltz Juventa.&lt;/p&gt;
&lt;p&gt;Mayring, Philipp (2015). &lt;em&gt;Qualitative Inhaltsanalyse. Grundlagen und Techniken.&lt;/em&gt; Weinheim: Beltz Juventa.&lt;/p&gt;
</p53_abstract>
  <p54_paperID>167</p54_paperID>
  <p54_contribution_type>Vortrag</p54_contribution_type>
  <p54_acceptance>Poster</p54_acceptance>
  <p54_authors>Sippl, Colin
Fuchs, Florian
Burghardt, Manuel</p54_authors>
  <p54_organisations>Lehrstuhl für Medieninformatik, Universität Regensburg
Lehrstuhl für Medieninformatik, Universität Regensburg
Lehrstuhl für Medieninformatik, Universität Regensburg</p54_organisations>
  <p54_emails>Colin.Sippl@stud.uni-regensburg.de
Florian.Fuchs@stud.uni-regensburg.de
manuel.burghardt@ur.de</p54_emails>
  <p54_presenting_author>Sippl, Colin
Fuchs, Florian
Burghardt, Manuel</p54_presenting_author>
  <p54_title>Digital Dylan – Computergestützte Analyse der Liedtexte von Bob Dylan (1962 – 2016)</p54_title>
  <p54_abstract>&lt;p&gt;Dieser Beitrag beschreibt die computergestützte Analyse der Liedtexte von Bob Dylan im Zeitraum 1962 – 2016. Dabei wird überprüft, ob bestehende, qualitative Einteilungen Dylans Werk in unterschiedliche Schaffensperioden auch quantitativ durch Wortfrequenzen und Signifikanztests belegt werden können. Es erfolgt somit eine grundlegende Erprobung von distant reading-Ansätzen im Bereich der Analyse von Liedtexten.&lt;/p&gt;
&lt;p&gt;Eine Web-App mit allen Ergebnissen des Digital Dylan-Projekts findet sich auf https://www.colin-sippl.de/dylan &gt; Analyse.&lt;/p&gt;
</p54_abstract>
  <p55_paperID>166</p55_paperID>
  <p55_contribution_type>Vortrag</p55_contribution_type>
  <p55_acceptance>Poster</p55_acceptance>
  <p55_authors>Romanello, Matteo
Gradl, Tobias</p55_authors>
  <p55_organisations>Deutsches Archäologisches Institut, Deutschland
Universität Bamberg, Deutschland</p55_organisations>
  <p55_emails>matteo.romanello@dainst.de
tobias.gradl@uni-bamberg.de</p55_emails>
  <p55_presenting_author>Romanello, Matteo
Gradl, Tobias</p55_presenting_author>
  <p55_title>Ist die DARIAH-DE Forschungsinfrastruktur fit für Daten der realen Welt? Bericht über einen Anwendungsfall mit archäologischen Daten und seine ersten Ergebnisse </p55_title>
  <p55_abstract>&lt;p dir="ltr"&gt;1. Hintergrund &lt;/p&gt;
&lt;p dir="ltr"&gt;Eine wesentliche Kritik an Forschungsinfrastrukturen behauptet:&lt;/p&gt;
&lt;p dir="ltr"&gt;This is the central paradox for big infrastructure design: the very wish to cater to everyone pushes the designers toward generalization, and thus necessarily away from delivering data models specific enough to be useful to anyone. [vanZundert 2012]&lt;/p&gt;
&lt;p dir="ltr"&gt;Können generische Infrastrukturen und Datenmodelle für individuelle Forschungsfragen von Bedeutung sein? Und wenn ja, wie verhalten sich spezifische Forschungsdaten gegenüber den generischen Infrastrukturen? In diesem Beitrag diskutieren wir diese Frage im Hinblick auf die von DARIAH-DE entwickelte Forschungsdateninfrastruktur und insbesondere  das Data Modelling Environment (DME). Als Schema und Crosswalk Registry entstanden [Gradl et al. 2015], entwickelte sich das DME zu einem umfangreichen Werkzeug für die Modellierung, Verfeinerung, Bereinigung und Anreicherung von Daten. Die Beispieldaten, die für diesen Anwendungsfall herangezogen wurden, stammen aus einer Datenbank der aus dem Deutschen Archäologischen Institut geführten Grabung in Pergamon und beschreiben etwa 100 keramische Grabungsfunde. &lt;/p&gt;
&lt;p dir="ltr"&gt;Für den Anwendungsfall wurde ein archäologischer Kontext gewählt, da relevante Forschungsdaten aufgrund ihrer Heterogenität eine besondere Herausforderung für Forschungsinfrastrukturen darstellen [Gradl &amp; Henrich 2016a]. Das wesentliche Ziel dieses Beitrags besteht darin, die Verwendbarkeit des DME auch im spezifischen Kontext von Pergamon-Daten anzudeuten. Eine Integration weiterer archäologischer Daten wie 2D-Bildern, 3D-Modellen und verschiedener Arten kontrollierter Vokabulare und geographischer Daten könnten für den gewählten Anwendungsfall Erkenntnisgewinne erreicht werden, die ggf. neue Fragen für die qualitative Forschung aufwerfen. &lt;/p&gt;
&lt;p dir="ltr"&gt;Durch eine kombinierte Visualisierung orts- und zeitbezogener Abhängigkeiten könnte man sich schnell einen ersten Überblick über die zeitliche und geographische Verteilung der Datensätze verschaffen. Wo liegt z. B. die höchste Dichte von auf die hellenistische Zeit datierten, keramischen Funden vor? Ein solcher visueller Überblick über die Grabungsdaten könnte den WissenschaftlerInnen auch erlauben, Diskrepanzen und Sonderfälle in der archäologischen Dokumentation einer Grabung zu erkennen. &lt;/p&gt;
&lt;p dir="ltr"&gt;2. Beschreibung des Anwendungsfalls&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Verarbeitung der Daten wird unterstützt durch das DME und insbesondere dessen Fähigkeit, auf externe Ressourcen zuzugreifen. Zwei Schnittstellen zu Diensten des DAI wurden implementiert, damit zeit- und ortsbezogene Textangaben wie „Grobdatierung: hellenistisch-kaiserzeitlich“ oder „Provenienz: Pergamon“ mit den entsprechenden und in Zahlen ausgedrückten Werten kartiert werden können. Schließlich werden die angereicherten Daten mittels des DARIAH-DE Geo-Browsers visualisiert, um die zeitliche und geographische Verteilung der in den Datensätzen beschriebenen Objekte visuell abzubilden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 1: Schematische Darstellung des Arbeitsablaufs.&lt;/p&gt;
&lt;p dir="ltr"&gt;Der Arbeitsablauf besteht aus fünf wesentlichen Schritten, die sich der Modell- oder Instanzebene zuweisen lassen.&lt;/p&gt;
&lt;ul&gt;&lt;li dir="ltr"&gt;
&lt;p dir="ltr"&gt;Modellebene: Datenmodelle und semantische Verbindungen zwischen diesen werden spezifiziert. &lt;/p&gt;
&lt;/li&gt;
&lt;li dir="ltr"&gt;
&lt;p dir="ltr"&gt;Instanzebene: Während Aufgaben der Modellierung einmalig auszuführen sind, werden die Aufgaben der Instanzebene für jede Datei bzw. jede Aktualisierung der Daten durchlaufen.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p dir="ltr"&gt;2.1 Vorverarbeitung&lt;/p&gt;
&lt;p dir="ltr"&gt;Die archäologische Grabung in Pergamon wurde mittels der iDAI.Field Software dokumentiert. iDAI.Field ist ein modulares Dokumentationssystem für Feldforschungsprojekte, das am DAI entwickelt wurde und in ca. 50 verschiedenen Projekten eingesetzt wurde. Die durch iDAI.Field gesammelten Daten werden in einer FileMaker-Datenbank gespeichert. Für eine Verarbeitung in der DARIAH-DE Infrastruktur wurde zunächst ein XML-Export aus der Datenbank ausgeführt. &lt;/p&gt;
&lt;p dir="ltr"&gt;2.2 Datenmodellierung&lt;/p&gt;
&lt;p dir="ltr"&gt;Um Pergamon-Daten in ein vom Geo-Browser unterstütztes Eingabeformat, wie die Keyhole Markup Language (KML) umwandeln zu können, müssen die relevanten Datenmodelle im DME vorliegen bzw. definiert werden. Dies kann durch das Hochladen von XSD-Schemata initiiert werden. Einmal hinterlegte Modelle können nach deren Definition in weiteren Anwendungsfällen nachgenutzt werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Abbildung 2 veranschaulicht neben dem Elementmodell auch die Funktionalität zur Verarbeitung von Beispieldaten, mit deren Hilfe überprüft werden kann, ob Daten korrekt prozessiert werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 2: Verarbeitung von Beispieldaten und Visualisierung der Datenstruktur.&lt;/p&gt;
&lt;p dir="ltr"&gt;2.3 Datenanreicherung&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Funktionalität des DME stellt zwei wesentliche Methoden zur Datenmodellierung bereitgestellt [Gradl &amp; Henrich 2016b]:&lt;/p&gt;
&lt;ol&gt;&lt;li dir="ltr"&gt;
&lt;p dir="ltr"&gt;Inhaltliche Spezifikation von Daten durch die Definition von domänenspezifischen Sprachen [Parr 2012]&lt;/p&gt;
&lt;/li&gt;
&lt;li dir="ltr"&gt;
&lt;p dir="ltr"&gt;Anwendung von Transformationsregeln. Neben bereits implementierter Funktionalität - z.B. zur automatischen Sprachverarbeitung - kann das DME flexibel durch Plugins erweitert werden, um neue Funktionen zur Verarbeitung von Daten einzubinden.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p dir="ltr"&gt;Im vorliegenden Anwendungsfall sind Daten in eine strukturierte und außerhalb der Pergamon-Datenbank interpretierbare Form zu bringen. Hierzu werden Daten u. A. durch die Nutzung des iDAI.Gazetteer (Auflösung von Ortsbezeichnungen) und der iDAI.Chronontology (Auflösung zeitlicher Angaben) angereichert.&lt;/p&gt;
&lt;p dir="ltr"&gt;iDAI.ChronOntology&lt;/p&gt;
&lt;p dir="ltr"&gt;Die ChronOntology API ermöglicht u. a. eine Freitextsuche. Beispielsweise ist es möglich nach Zeitangaben zu suchen, die den String „Kaiserzeitlich“ beinhalten und die entsprechende Datierungen aufweisen können. So ist der Begriff „kaiserzeitlich“ mit „-27“ und „476“ als Beginn- und Enddatum verbunden. &lt;/p&gt;
&lt;p dir="ltr"&gt;Im Rahmen des DME wird das Modell der Pergamon-Daten dahingehend erweitert, dass unter dem in XML vorhandenen Element &lt;Grobdatierung&gt; zunächst Grammatik und Transformationsregel angelegt werden. Hierunter werden die zu produzierenden zusätzlichen Elemente modelliert: im konkreten Fall die strukturierte Antwort des iDAI.ChronOntology Dienstes. Abbildung 3 zeigt neben diesem erweiterten Elementmodell bereits das Ergebnis der Anwendung dieser Funktionalität auf die Beispieldaten.&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 3: Erweiterung des Datenmodells durch Zusatz  der strukturierten Antwort des iDAI.ChronOntology Dienstes.&lt;/p&gt;
&lt;p dir="ltr"&gt;Für eine in Bezug auf die Pergamon-Daten optimierte Anfrage an den iDAI.ChronOntology Dienst wird die Semantik des Elements &lt;Grobdatierung&gt; expliziert. Die Grammatik in Abbildung 4 veranlasst die Zerlegung zusammengesetzter Datierungsangaben, um die vorliegende von-bis Semantik darzustellen (z. B. bei hellenistisch-kaiserzeitlich) und die einzelnen Anfrageterme zu extrahieren [Parr 2012].&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 4: Bearbeitung eines Elements des Datenmodells durch eine vom Benutzer editierbare Grammatik. &lt;/p&gt;
&lt;p dir="ltr"&gt;Durch die Adressierung der so gebildeten Terme in &lt;dating&gt; kann die anschließende Transformationsregel (vgl. Abbildung 5) auf eine verfeinerte Variante des zuvor unstrukturierten Inhalts zurückgreifen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Ausführung der Chronontology API ist durch Anwendung von Funktionalität des umgesetzten DAI-Plugins möglich. Im vorliegenden Fall gestaltet sich das Kommando wie folgt:&lt;/p&gt;
&lt;p dir="ltr"&gt;Chronontology = dai.chronontology.query(@dating);&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 5: Spezifikation einer Transformationsregel zur Abfrage des iDAI ChronOntology API.&lt;/p&gt;
&lt;p dir="ltr"&gt;Um aus der potenziellen Menge zurückgegebener Einträge ein Intervall zu berechnen, werden Kommandos aus dem math Funktionsraum verwendet:&lt;/p&gt;
&lt;p dir="ltr"&gt;BeginMin = math.min(@Response.Item.Resource.Timespan.Begin.At);&lt;/p&gt;
&lt;p dir="ltr"&gt;EndMax = math.max(@Response.Item.Resource.Timespan.End.At);&lt;/p&gt;
&lt;p dir="ltr"&gt;Hierdurch wird in den Daten exakt ein Zeitintervall hinterlegt, welches der gewünschten Semantik [frühester Begin, spätestes Ende] der Zeitangabe entspricht.&lt;/p&gt;
&lt;p dir="ltr"&gt;iDAI.Gazetteer&lt;/p&gt;
&lt;p dir="ltr"&gt;Vergleichbar mit der Chronontology API können auch Funktionen der Gazetteer API auf Daten angewandt werden. Im vorliegenden Beispiel wird der für eine Anfrage zurückgegebene, erste Treffer als wahrscheinlichste Koordinate verwendet und in den Daten berücksichtigt:&lt;/p&gt;
&lt;p dir="ltr"&gt;Location = dai.gazetteer.topcoord(@ResolveLocation);&lt;/p&gt;
&lt;p dir="ltr"&gt;2.4 Mapping der Datenstrukturen&lt;/p&gt;
&lt;p dir="ltr"&gt;Für die Transformation originärer Pergamon-Daten in das KML Format ist schließlich die Modellierung von Zusammenhängen der Datenmodelle erforderlich. Abbildung 6 zeigt die drei Mappings, die für den Anwendungsfall modelliert wurden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Abb. 6: Visualisierung der Mappings zwischen Quellmodell (Pergamon XML) und Zielmodell (KML) im DME.&lt;/p&gt;
&lt;p dir="ltr"&gt;Über einfache Wertkorrespondenzen, wie bei BeginMin (Pergamon) zu Begin (KML) hinaus können auch an dieser Stelle Transformationsregeln spezifiziert werden. Für die Übertragung der Koordinaten in das KML Schema wird so z. B. folgende Regel definiert:&lt;/p&gt;
&lt;p dir="ltr"&gt;[@Latitude != ""]&lt;br /&gt;  Coordinates = concat(@Latitude, ",", @Longitude)&lt;br /&gt;[endif]&lt;/p&gt;
&lt;p dir="ltr"&gt;Koordinaten werden demnach nur angelegt, wenn @Latitude (für Daten im Quellschema) gesetzt ist. Zur Erzeugung eines Strings “Latitude, Longitude” wird die Konkatenationsanweisung verwendet.&lt;/p&gt;
&lt;p dir="ltr"&gt;2.5 Visualisierung der Mapping-Ergebnisse&lt;/p&gt;
&lt;p dir="ltr"&gt;Transformierte Daten können in verschiedenen Formaten heruntergeladen werden. Als KML Datei exportiert, können die 100 archäologischen Beispieldatensätze im GeoBrowser bereitgestellt und angezeigt werden (vgl. Abbildung 7). Abb. 7: Visualisierung der Mapping-Ergebnisse mittels des Geo-Browsers.&lt;/p&gt;
&lt;p dir="ltr"&gt;Nur 16 der 100 Datensätze haben Ortsangaben und können deshalb positioniert werden (Pergamon: 15, Knidos: 1), während fast alle eine Zeitangabe aufweisen. Die Möglichkeit, eine historische Karte (hier der Barrington Atlas map of the Roman Empire) auszuwählen, bietet einen zusätzlichen Nutzen, da sie eine bessere Kontextualisierung der Daten ermöglicht. Da der GeoBrowser derzeit keine XML-Namespaces unterstützt, müssen diese im Moment manuell aus den KML Daten entfernt werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;3. Schlußdiskussion&lt;/p&gt;
&lt;p dir="ltr"&gt;Dieser Anwendungsfall basierte auf einer zu geringen Menge von Daten, als dass akute Mehrwerte erreichen werden könnten. Die Visualisierung von Daten aus mehreren Grabungsorten könnte dagegen die Einführung neuer Formen, Farben oder Keramiktypen in ort- und zeitbezogener Abhängigkeit darstellen und so zu der Generierung neuer Hypothesen führen. Das DME ist flexibel genug, um mit den heterogenen Daten der Archäologie umgehen zu können. &lt;/p&gt;
&lt;p dir="ltr"&gt;Indem das DME eine Modellierung von Verarbeitung von Daten spezifischer Anwendungsfälle ermöglicht, hat es das Potential das „OpenRefine für die digitalen Geisteswissenschaften“ zu werden: ein generisches Tool zur Modellierung, Verfeinerung, Bereinigung und Anreicherung von Forschungsdaten, das eine breite Vielfalt von Arbeitsabläufen unterstützen kann.&lt;/p&gt;
&lt;p dir="ltr"&gt;Zugleich stellt sich aber auch die Frage, wer die typischen BenutzerInnen des DME sein können? Oder: ist es realistisch zu erwarten, dass GeisteswissenschaftlerInnen dieses Werkzeug ohne die Unterstützung von DH Spezialisten bedienen können? Tatsächlich scheint das DME eine gemeinsame Basis der Kollaboration und Kommunikation sein zu wollen, in der das Wissen von GeisteswissenschaftlerInnen mit der technischen Expertise von DH-Experten zusammengeführt werden. Hierdurch können Aufgaben, wie die des vorliegenden Anwendungsfalls erfüllt werden ohne sämtliche technische Problemstellungen von Grund auf neu lösen zu müssen. Durch die wachsende Zahl von bestehenden Quell-/Zielmodelle, Transformationsregeln und API-Wrappers kann Wissen und Funktionalität nachgenutzt werden. &lt;/p&gt;
</p55_abstract>
  <sessionID>60</sessionID>
  <p56_paperID>297</p56_paperID>
  <p56_contribution_type>Vortrag</p56_contribution_type>
  <p56_acceptance>Poster</p56_acceptance>
  <p56_authors>Hiebert, Matthew
Lässig, Simone
Witt, Andreas</p56_authors>
  <p56_organisations>German Historical Institute Washington
German Historical Institute Washington
Universität zu Köln</p56_organisations>
  <p56_emails>hiebert@ghi-dc.org
laessigs@ghi-dc.org
andreas.witt@uni-koeln.de</p56_emails>
  <p56_presenting_author>Hiebert, Matthew
Witt, Andreas</p56_presenting_author>
  <p56_title>Deutsche Geschichte-Digital: Ergebnisse der TEI-Konvertierung und Integration in Pilotprojekten</p56_title>
  <p56_abstract>&lt;p&gt;Das Deutsche Historische Institut in Washington (DHI) befindet sich in der Entwicklungsphase von "German History-&lt;em&gt;Digital&lt;/em&gt;" (GH-D), einer transatlantischen digitalen Initiative, um die wissenschaftlichen Bedürfnisse von HistorikerInnen im Kontext neuer historiografischer und technologischer Herausforderungen zu bewältigen. GH-D ist eine neue Infrastruktur zur Erleichterung der transnationalen historischen Wissensschöpfung für eine große Wissenschaftsgemeinschaft und eine wachsende Zahl von "Citizen Scholars," die bereits auf digitale Ressourcen des DHI angewiesen sind. In der vorgeschlagenen Präsentation diskutieren wir Zwischenergebnisse von zwei zentralen DHI-Pilotprojekten innerhalb von GH-D. Das erste ist "Deutsche Geschichte in Dokumenten und Bildern", unterstützt von der Deutschen Forschungsgemeinschaft (DFG), und das zweite ist Deutsche Geschichte &lt;em&gt;Intersections&lt;/em&gt;, unterstützt durch das Europäische Wiederaufbauprogramm (ERP). Wir konzentrieren uns speziell auf die Herausforderungen und Vorteile der Konvertierung von Dokumenten in die Text Encoding Initiative (TEI) und die Integration mit dem Scalar Content Management System.&lt;/p&gt;
&lt;p&gt;Trotz großer Fortschritte haben die &lt;em&gt;Digital Humanities&lt;/em&gt; die Forschung in der deutschen Geschichte noch nicht nachhaltig beeinflusst. Die vergangenen zehn Jahre haben die Verbreitung von für das Feld relevanten Online-Ressourcen hervorgebracht, doch bleiben diese Materialien weitgehend in verschiedenen Systemen getrennt, schwer zu finden oder von WissenschaftlerInnen in Korpora zusammenzuführen. Historiker produzieren zunehmend wissenschaftlichen Inhalt in digitaler Form, aber es gibt nach wie vor keine etablierten Kriterien für die Peer-Review von digitalen Publikationen und Projekten. Dies beschränkt letztlich die Zeit und Energie, die die Forschungsgemeinschaft bereit ist, in die digitale Wissensproduktion zu investieren und begrenzt damit das Wachstumspotenzial der Digitalen Geisteswissenschaften.&lt;/p&gt;
&lt;p&gt;Die Erhaltung und der zukünftige Zugang zu digitalen Materialien sind auch im nordamerikanischen Kontext von zentraler Bedeutung, da dort kontinentale und nationale digitale Forschungsinfrastrukturen für digitale Geisteswissenschaften fehlen. Es gibt derzeit keine Entsprechung zu europäischen Forschungsinfrastrukturen wie CLARIN oder DARIAH. In Bezug auf die wissenschaftliche Methodik entsteht die Erwartung, dass HistorikerInnen von einer Fülle von digitalen Werkzeugen profitieren, doch besteht weiterhin eine unzureichende Integration zwischen Werkzeugen für die historische Forschung und zwischen Werkzeugsets und Online-Ressourcen. Die Bedeutung der &lt;em&gt;Citizen Science&lt;/em&gt; und der kooperativen Wissensschöpfung für die Zukunft der historischen Forschung wird ebenfalls anerkannt, doch für diese Entwicklungen ist es unerlässlich, dass es jenseits von e-Listen und anderen veralteten Kommunikationstechnologien wissenschaftliche Umgebungen gibt, um flächenbezogene Forschungsgemeinschaften, wissenschaftliche Zusammenarbeit und öffentliches Engagement zu schaffen. &lt;/p&gt;
&lt;p&gt;Die Planung für GH-D umfasste die Befragung von mehr als vierhundert WissenschaftlerInnen, die bereits mit digitalen Ressourcen arbeiten, welche vom DHI produziert wurden. Die umfassendste dieser Ressourcen ist die im Jahr 2003 gestartete digitale Quellensammlung "Deutsche Geschichte in Dokumenten und Bildern" (GHDI), die an deutsch- und englischsprachigen Universitäten weitläufig genutzt wird. Derzeit wird GHDI in Verbindung mit GH-D einem technischen und konzeptionellen Umbau unterzogen. Es enthält Tausende Seiten englischsprachiger Übersetzungen deutscher historischer Texte sowie Bilder und Karten, die von ca. 5.000 Besuchern pro Tag genutzt werden. Unsere Planung für GH-D beinhaltet auch weiterhin Konsultationen und Workshops mit ExpertInnen aus den Geschichtswissenschaften und den digitalen Geisteswissenschaften sowie die Gründung von Partnerschaften mit Institutionen und großen Initiativen, die unser Interesse für die Zukunft der Geschichte im digitalen Zeitalter teilen.&lt;/p&gt;
&lt;p&gt;Die Deutsche Geschichte-Digital-Plattform befasst sich mit den Bedürfnissen der digitalen Forschung durch fünf Ziele und integrierte Arbeitspakete, die auf diese Ziele abgestimmt sind: Entdeckung, Analyse, Produktion, Bewahrung und Gemeinschaft.&lt;/p&gt;
&lt;p&gt;Eine große Herausforderung für die Forschung online ist, dass eine Vielzahl von digitalen Ressourcen, insbesondere solche, die unabhängig oder von kleineren Institutionen produziert werden, keine standardisierten Metadatenauszeichnungen haben und nicht über einen zentralen wissenschaftlichen Index zugänglich sind. GH-D beinhaltet die Entwicklung eines Peer-Review-Index von wissenschaftlichen digitalen Objekten mit Dublin Core (DC) und CLARINs Component MetaData Infrastructure (CMDI) Standards über einen angepassten Blacklight (http://projectblacklight.org/) Technologie-Stack.&lt;/p&gt;
&lt;p&gt;Für WissenschaftlerInnen, die historische digitale Projekte in Nordamerika entwickeln, gibt es keine interinstitutionelle Infrastruktur für die Speicherung ihrer Daten oder die Bereitstellung von Open Access. CLARIN-D, Teil der europäischen Forschungsinfrastruktur CLARIN, berät und unterstützt das GH-D-Projekt zur Erstellung eines Portals für CLARIN in Nordamerika am DHI Washington. Im Mittelpunkt dieses Prozesses steht die Implementierung eines Repository-Systems, das eine nachhaltige Speicherung des Inhalts und die Einbindung in eine digitale Umgebung ermöglicht, um den Zugriff, die Suche und die interoperablen Datenformate zu erleichtern. Der Inhalt des Repositoriums selbst folgt internationalen, weithin akzeptierten und unterstützten Standards. Die hohe Qualität der technischen Lösung und die Konformität mit den Standards werden durch eine unabhängige Organisation gesichert, die das Data Seal of Approval (http://www.datasealofapproval.org) ausgibt. Unsere Partnerschaft mit CLARIN fördert den freien Zugang, die offene, kooperative Wissenschafts- und Wissenserzeugung im nordamerikanischen Kontext und ist ein wichtiger Bestandteil der gesamten Strategie der digitalen Geisteswissenschaften des DHI. Als Institut der Max Weber Stiftung sind wir auch in Partnerschaft mit DARIAH-DE. DARIAH-DE wird Web-Hosting und langfristige Bewahrung von DHI-Digitalprojekten in ihrer Gesamtheit, einschließlich der ersten Version der Webseite Deutsche Geschichte in Dokumenten und Bildern, zur Verfügung stellen.&lt;/p&gt;
&lt;p&gt;Als kooperative Wissensplattform wird GH-D Redakteure, Forscher und &lt;em&gt;Citizen Scientists&lt;/em&gt; bei der Entwicklung weiterer innovativer Online-Projekte zusammenbringen. Drei solcher Pilotprojekte befinden sich derzeit in der Entwicklung und beinhalten TEI und unsere Internationalisierung der Scalar 2.0 Plattform. GH-D verwendet Scalar 2.0 für das Baseline Content Management System, insbesondere aufgrund seiner Schnittstellenfunktionen, Unterstützung für Resource Description Framework (RDF), Konnektivität zu externen Repositorien, Dublin Core (DC) Unterstützung, Hypothes.is Integration und sein Mehrfachpfad-Navigationssystem.&lt;/p&gt;
&lt;p&gt;HistorikerInnen nutzen zunehmend digitale Geisteswissenschaften, um Daten zu analysieren und ihre Forschungsergebnisse darzustellen. Ein weiterer Vorteil der Speicherung von TEI-Digitalobjekten in einem CLARIN-Repository ist, dass eine ganze Palette von korpus-linguistischen analytischen Werkzeugen von WissenschaftlerInnen auf Textinhalte angewendet werden kann. In diesem Zusammenhang werden wir unsere Entwicklung von Scalar Adapters für den Anschluss an deutsche Repositorien und virtuelle Forschungsumgebungen (VREs) diskutieren.&lt;/p&gt;
&lt;p&gt;Die GH-D-Plattform integriert die Blog-Aggregation, ein erweitertes Diskussionssystem, Community-orientierte Tools und Social Media, um miteinander kooperierende Wissensgemeinschaften zu erleichtern und Forschung zu öffnen. Dies ist ein wegweisender Aspekt unseres Projektes, das die Annahme von sozialen und gemeinschaftlichen digitalen Instrumenten durch HistorikerInnen in ihren Forschungsaktivitäten untersuchen wird. Wir wollen hierbei auch die einzigartige Rolle nutzen, die das DHI als Drehscheibe des transatlantischen wissenschaftlichen Dialogs und als eines großen Knotenpunkts in einem internationalen Netzwerk von HistorikerInnen spielt, um die Zusammenhänge zwischen verschiedenen Wissenschaftsgemeinschaften zu erleichtern.&lt;/p&gt;
&lt;p&gt;Deutsche Geschichte - Digitale Projekte bietet ein Modell für neue, quellenbasierte methodische Ansätze in den Geschichtswissenschaften. Die Initiative zielt darauf ab, durch digitale Instrumente, Standards und Methoden zur argumentbasierten Forschung beizutragen. Es unterstützt narratologische Komplexität in der Geschichtsschreibung und vermeidet redaktionelle Ansätzen, die eine singuläre Erzählung oder „&lt;em&gt;master narrative&lt;/em&gt;“ ergeben. Es fördert transnationale Ansätze in der historischen Forschung durch das Verfügbarmachen einer transnationalen technischen Plattform, die auf TEI und anderen aufkommenden Standards in den digitalen Geisteswissenschaften wie DC und RDF gründet. Wir freuen uns darauf, mit "Digital Humanities im deutschsprachigen Raum" unsere Arbeit mit der DH-Community in Deutschland zu teilen und zu diskutieren.&lt;/p&gt;
</p56_abstract>
  <p57_paperID>264</p57_paperID>
  <p57_contribution_type>Vortrag</p57_contribution_type>
  <p57_acceptance>Poster</p57_acceptance>
  <p57_authors>Haaf, Susanne</p57_authors>
  <p57_organisations>Berlin-Brandenburgische Akademie der Wisesnschaften, Deutschland</p57_organisations>
  <p57_emails>haaf@bbaw.de</p57_emails>
  <p57_presenting_author>Haaf, Susanne</p57_presenting_author>
  <p57_title> Perspektiven auf ein Korpus. Kombinationen quantitativ-qualitativer Analysemethoden zur Ermittlung von Textgliederungsprinzipien </p57_title>
  <p57_abstract>&lt;p&gt;1 Einführung&lt;/p&gt;
&lt;p&gt;Im Bereich digital basierter Untersuchungen wird mittlerweile zunehmend eine Verzahnung quantitativen und qualitativen Arbeitens gefordert, etwa in Form einer Überprüfung automatisch gewonnener Ergebnisse durch Autopsie der zugrundeliegenden Quellen. In der konkreten Arbeit der Korpusanalyse wird aus dieser scheinbaren Dichotomie jedoch schnell eine Methodenvielfalt, denn gerade durch Kombinationen verschiedener Analysemethoden werden unterschiedliche Phänomene greifbar und entfaltet sich das volle Potential quantitativ-qualitativen Arbeitens.&lt;/p&gt;
&lt;p&gt;Im Folgenden soll dieser Problembereich beispielhaft umrissen werden, wobei die folgenden Aspekte hervortreten:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Maschinelle Analysemethoden ermöglichen den verbesserten Zugang zu Korpusdaten. Sie müssen jedoch ergänzt werden durch eine gute Kenntnis der digitalen Daten sowie der zugrundeliegenden (physischen) Quellen.&lt;/li&gt;
&lt;li&gt;Die statistische Datenanalyse kann nicht notwendig nur signifikante Merkmale von Text(sort)en, sondern auch Unstimmigkeiten der Korpuszusammenstellung zutage fördern.&lt;/li&gt;
&lt;li&gt;Verschiedene Verfahren der Visualisierung gewähren unterschiedliche Blickwinkel auf ein Korpus und tragen so zur Differenziertheit der Beurteilung der Datenlage bei.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Zugrunde liegen laufende Untersuchungen zu Textgliederungsprinzipien, die charakteristisch für Textsorten der Erbauungsliteratur des 17. Jahrhunderts sind. Im Mittelpunkt steht dabei die Frage, inwieweit Textgliederungsprinzipien durch die TEI-Annotation von Texten maschinell greifbar werden bzw. wo die Grenzen eines solchen Ansatzes liegen.&lt;/p&gt;
&lt;p&gt;2 Hintergrund und methodischer Ansatz&lt;/p&gt;
&lt;p&gt;Prinzipien zur Textgliederung können sich nach Stein auf ganz verschiedenen sprachlichen Ebenen befinden, so etwa auf der grammatisch-syntaktischen, der inhaltlich-thematischen, aber auch auf der typographischen Ebene (Stein 2003: 422). Linguistische Korpora ermöglichen gemeinhin die Analyse grammatisch-syntaktischer, teils auch inhaltlicher Strukturen. Die typographische und konzeptionelle Ebene wird jedoch erst durch die Anreicherung digitaler Quellen mit Layout-basiertem Markup (in Form von  XML(TEI)-Annotationen) greifbar. Durch solches Markup wird es möglich, eher visuelle Textgliederungsprinzipien statistisch auswertbar zu machen.&lt;/p&gt;
&lt;p&gt;In der hier präsentierten Studie wurde dieses Vorgehen anhand des Korpus des Deutschen Textarchivs (DTA 2017) erprobt: Einzelne TEI-Strukturen aus drei Teilkorpora mit Texten des 17. Jhs. wurden hinsichtlich der Häufigkeit ihres Auftretens und ihrer Verteilung im jeweiligen Korpus verglichen, so etwa das Vorliegen von Versen (tei:l), verschiedene Arten von Hervorhebungen (tei:hi), Marginalien, Titel (tei:head), die Komplexität der Struktur (einander über-/untergeordnete tei:div), Listen etc.&lt;/p&gt;
&lt;p&gt; Zugrundegelegt wurden die folgenden 3 Teilkorpora aus Werken des 17. Jhs.:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Prosaische Erbauungsliteratur: 25 Bände (10 Autoren, 10.501 Seiten)&lt;/li&gt;
&lt;li&gt;Funeralschriften: 334 Schriften (14.316 Seiten)&lt;/li&gt;
&lt;li&gt;Referenzkorpus: 187 Bände (60.798 Seiten)&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Zu jedem untersuchten Merkmal wurden relative Häufigkeiten im Verhältnis zur Token-Anzahl des jeweiligen Textes gebildet. Die Merkmalsextraktion erfolgte mit XSLT, die automatische Datenanalyse mittels der Python-Bibliothek NumPy. Als Visualisierungen wurden Balkendiagramme, Streudiagramme  und Kastendiagramme genutzt und miteinander verglichen. Die jeweiligen Visualisierungen wurden mit Python-Pandas und Matplotlib erstellt.&lt;/p&gt;
&lt;p&gt;Im Folgenden werden einige Ergebnisse der Untersuchung in Beziehung gesetzt zu den eingangs umrissenen Problembereichen.&lt;/p&gt;
&lt;p&gt;3 Ergebnisse&lt;/p&gt;
&lt;p&gt;3.1 Ermittlung signifikanter Merkmale anhand von Häufigkeiten&lt;/p&gt;
&lt;p&gt;Ein Merkmal, hinsichtlich dessen sich die quantitative Methode als unmittelbar aufschlussreich erwies, war das Vorhandensein von Marginalien. Der Vergleich der hier genutzten Teilkorpora des 17. Jhs. ergab, dass Marginalien in Funeralschriften signifikant häufiger auftreten als in den Vergleichskorpora (p-Wert: 7,82e-06; Abb. 1).&lt;/p&gt;
&lt;p&gt;Um die Repräsentativität dieses Merkmals für die Textsorte Funeralschrift adäquat zu evaluieren, wurden Median und umliegende Quartile ermittelt und im Kastendiagramm visualisiert: Es zeigte sich eine relativ regelmäßige Verteilung mit dem Mittelwert in der Nähe des Medians und ohne allzu extreme Ausreißer (Abb. 2). Das Referenzkorpus weist dagegen eine sehr inhomogene Verteilung auf (Abb. 3), das Korpus der prosaischen Erbauungsliteratur unterliegt ebenfalls mehr Schwankungen.&lt;/p&gt;
&lt;p&gt;Folglich stellt die Marginalie offenbar ein recht typisches Merkmal für Funeralschriften gegenüber den Vergleichskorpora dar.&lt;/p&gt;
&lt;p&gt;3.2 Korpusevaluation anhand struktureller Merkmale&lt;/p&gt;
&lt;p&gt;Ähnlich überzeugende Häufigkeiten im Balkendiagramm erbrachte zunächst das Merkmal der „Versform“ (tei:l) für die Funeralschriften. Im Balkendiagramm, das auf dem Mittelwert der Versfrequenzen basiert, erschien der Anteil an Versen in Funeralschriften verhältnismäßig viel höher als in den Vergleichskorpora (Abb. 4).&lt;/p&gt;
&lt;p&gt;Das Streudiagramm der Funeralschriften führte für dieses Merkmal allerdings eine Merkwürdigkeit zutage: Eine relativ große Gruppe von Texten enthielt verhältnismäßig viel mehr Text in Versen als die sonstigen Funeralschriften (Abb. 5). Hierbei handelte es sich um reine Epicediendrucke Simon Dachs, die zwar im DTA korrekt als „Funeralschriften“ kategorisiert waren, jedoch nur einen Teil der typischen gedruckten Leichenpredigt enthielten (Personalschriften 2017). Diese betreffenden Epicediendrucke weichen hinsichtlich der Verse so signifikant vom Rest des Korpus ab, dass der Mittelwert für Verse im Funeralschriftenkorpus dadurch angehoben wurde (vgl. die blaue Linie in Abb. 5). Nach Entfernen der betreffenden Schriften sank der Mittelwert wiederum signifikant (Abb. 6).&lt;/p&gt;
&lt;p&gt;Hier wiesen statistische Auffälligkeiten also weniger auf signifikante Merkmale einer Textsorte sondern vielmehr auf Unstimmigkeiten im zusammengestellten Korpus hin und ermöglichten die Evaluation der Korpuszusammenstellung im Allgemeinen.&lt;/p&gt;
&lt;p&gt;Was nun das Auftreten des untersuchten Merkmals für Funeralschriften betrifft, zeigen sich bereits im Streudiagramm große Schwankungen, was durch das zugehörige Kastendiagramm belegt wurde  (Abb. 7). Verse treten in Funeralschriften nach diesem Befund also nicht in typischen Häufigkeiten auf. Dass sie dennoch für diese Textsorte relevant sind, zeigt sich (auch kontrastiv zu den Vergleichskorpora) daran, dass Verse in den meisten Dokumenten des Korpus (301 von 334) enthalten waren.&lt;/p&gt;
&lt;p&gt;3.3 Signifikanz des Ortes eines Merkmals im Dokument&lt;/p&gt;
&lt;p&gt;Weiterhin wurde am Beispiel des Wechsels zur Antiquaschrift deutlich, dass zudem detaillierte Kenntnisse der digitalen Daten und ihrer Annotation für die quantitative Analyse notwendig sind. Auch dieses Merkmal trat auffällig häufig in Funeralschriften auf (Abb. 8). Entscheidend für dessen Beurteilung war allerdings Ort seines gehäuften Auftretens in den Dokumenten: Der Wechsel zur Antiquaschrift findet sich besonders häufig in den Marginalien der Funeralschriften, die Häufigkeiten dieser Merkmale (Marginalien, Wechsel zur Antiquaschrift) korrespondieren also miteinander.&lt;/p&gt;
&lt;p&gt;Ein Blick in die Quellen verrät die Funktion dieser Merkmale: Marginalien sind meist Träger des bibliographischen Nachweises zum Text (Bibelstellenangabe, Kirchenväterzitat). Im reinen Fließtext unterscheidet sich die Häufigkeit des Wechsels zur Antiquaschrift zwischen den betrachteten erbaulichen Textsorten hingegen kaum (Abb. 9). Offenbar erbringen Funeralschriften also insgesamt mehr bibliographische Nachweise als prosaische Erbauungsschriften, indem sie das Medium der Marginalie zusätzlich nutzen.&lt;/p&gt;
&lt;p&gt;3.4 Relevante Merkmale mit niedrigen Häufigkeiten&lt;/p&gt;
&lt;p&gt;Schließlich zeigt sich allerdings auch, dass der Blick in die physischen Quellen für das quantitative Arbeiten unerlässlich ist, besonders dort, wo Häufigkeiten eher gering(er) ausfallen. Ein Beispiel hierfür ist der Wechsel der Frakturtype, der nach dem Wechsel zur Antiquaschrift im Allgemeinen die zweithäufigste Form der Hervorhebung in den untersuchten Korpora ist. Hierbei liegen im Fließtext die erbaulichen Korpora hinter dem Referenzkorpus, wobei die prosaische Erbauungsliteratur den niedrigsten Mittelwert aufweist. Daraus folgt jedoch nicht, dass der Frakturwechsel für diese Literatur eine eher geringe Bedeutung trägt. In Rückbindung an die Quellen zeigt sich, dass in der prosaischen Erbauungsliteratur die genannten Hervorhebungen Funktionen ausüben, die dem erbaulichen Charakter der Texte zumindest entgegenkommen: Frakturwechsel dient hier der Hervorhebung von Bibelzitaten und Kernaussagen (auch mit wiederholendem Charakter) und folglich als Orientierungshilfe im Text (Beispiele Abb. 10).&lt;/p&gt;
&lt;p&gt;An dieser Stelle stößt die allein auf der TEI-Struktur basierende Untersuchung an ihre Grenzen. Hier wäre es evtl. aufschlussreich, rein prosaische Korpora zu vergleichen oder die Analyse von TEI-Strukturen durch weitere Merkmale der Textoberfläche (z.B. Wiederholungsstrukturen) zu ergänzen.&lt;/p&gt;
&lt;p&gt;Ein weiteres Beispiel in diesem Zusammenhang ist die Häufigkeit von Listen (tei:list) in den Korpora. Erwartungsgemäß treten diese in den erbaulichen Texten nur selten auf. In den erbaulichen Prosawerken treten Listen zwar nicht häufig, aber relativ regelmäßig auf (Abb. 11), und zwar in den Registern am Buchbeginn oder Buchende (in tei:front oder tei:back), deren listenähnliche Strukturen mit tei:list getaggt wurden. Als Register haben sie trotz geringer Häufigkeit aber eine große Bedeutung für die Benutzbarkeit und Zugänglichkeit eines Textes.&lt;/p&gt;
&lt;p&gt;4 Fazit&lt;/p&gt;
&lt;p&gt;Quantitative Analysen geben nicht allein Auskunft über Häufigkeiten von Merkmalen und führen den Betrachter zu zahlenmäßigen Auffälligkeiten. Je nach Perspektive eröffnen sie verschiedene Zugänge zu den Daten, ermöglichen die Evaluation des Korpus und der Fragestellung während der Untersuchung. Um Fehlinterpretationen zu entgehen, ist dabei erstens eine gute Kenntnis der (physischen) Quellen, zweitens die Kenntnis des analysierten Korpus und seiner „digitalen“ Eigenschaften sowie drittens der Blick auf die erhobenen Daten aus unterschiedlichen statistischen Perspektiven  vonnöten. Diese Kombination der verschiedenen quantitativen und qualitativen Analyseansätze und ihrer Teilergebnisse ermöglicht dann jedoch Einblicke in die Komplexität der tatsächlichen Gegebenheiten, die damit differenziert interpretierbar werden.&lt;/p&gt;
&lt;p&gt;Für die zugrundeliegende Fragestellung ergibt sich außerdem, dass die Auswertung der TEI-Strukturierung Prinzipien der Textgliederung in verschiedenen Textsorten offenlegen (z.B. Marginalien, Wechsel zur Antiquaschrift), aber auch anhand der Quellen gewonnene erste Eindrücke zahlenmäßig relativieren kann (z.B. Frakturwechsel). Für anschließende Untersuchungen ist die Betrachtung weiterer textueller Spezifika in Kombination mit der TEI-Strukturierung geplant.&lt;/p&gt;
&lt;p&gt;5 Bibliographie&lt;/p&gt;
&lt;p&gt;Deutsches Textarchiv (2017): Deutsches Textarchiv. Grundlage für ein Referenzkorpus der neuhochdeutschen Sprache. Herausgegeben von der Berlin-Brandenburgischen Akademie der Wissenschaften, Berlin 2017  http://www.deutschestextarchiv.de [letzter Zugriff: 24.09.2017]&lt;/p&gt;
&lt;p&gt;Forschungsstelle für Personalschriften (2017): „Aufbau einer Leichenpredigt“, Akademie der Wissenschaften und der Literatur Mainz http://www.personalschriften.de/leichenpredigten/aufbau.html [letzter Zugriff: 24.09.2017]&lt;/p&gt;
&lt;p&gt;Stein, Stephan (2003): „Textgliederung . Einheitenbildung im geschriebenen und gesprochenen Deutsch. Theorie und Empirie“, Berlin: De Gruyter.&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;Clara, Abraham a Sancta: „Judas Der Ertz-Schelm“, Bd. 1. Salzburg 1686, S. 314. In: Deutsches Textarchiv http://www.deutschestextarchiv.de/santa_judas01_1686/350 [letzter Zugriff: 24.09.2017]&lt;/p&gt;
&lt;p&gt;Spener, Philipp Jakob: „Pia Desideria“, Frankfurt (Main) 1676, S. 47. In: Deutsches Textarchiv http://www.deutschestextarchiv.de/spener_piadesideria_1676/73 [letzter Zugriff: 24.09.2017]&lt;/p&gt;
&lt;p&gt;Wülfer, Daniel: „Das vertheidigte Gottes-geschick/ und vernichtete Heyden-Glück“, Nürnberg 1656, S. 105. In: Deutsches Textarchiv http://www.deutschestextarchiv.de/wuelffer_gottesgeschick_1656/171 [letzter Zugriff: 24.09.2017]&lt;/p&gt;
</p57_abstract>
  <p58_paperID>108</p58_paperID>
  <p58_contribution_type>Vortrag</p58_contribution_type>
  <p58_acceptance>Poster</p58_acceptance>
  <p58_authors>Hannesschläger, Vanessa
Andorfer, Peter</p58_authors>
  <p58_organisations>OEAW Österreichische Akademie der Wissenschaften, Österreich
OEAW Österreichische Akademie der Wissenschaften, Österreich</p58_organisations>
  <p58_emails>vanessa.hannesschlaeger@oeaw.ac.at
peter.andorfer@oeaw.ac.at</p58_emails>
  <p58_presenting_author>Hannesschläger, Vanessa</p58_presenting_author>
  <p58_title>Menschen gendern? Einige Gedanken über Datenmodellierung zur Erhebung von Geschlechterverteilung anhand der TEI2016 Abstracts App</p58_title>
  <p58_abstract>&lt;p dir="ltr"&gt;In diesem Beitrag wird die TEI2016 Abstracts App vorgestellt, die im Anschluss an die TEI Konferenz 2016 vom Konferenzveranstalter (Austrian Centre for Digital Humanities der Österreichischen Akademie der Wissenschaften) aus den TEI-XML-Fassungen der Konferenzabstracts gebaut wurde. Der Workflow von der ursprünglichen Einreichung der Abstracts via ConfTool bis zu ihrer Präsentation in einem Web-Interface wird kurz skizziert. Die App wird als Ausgangspunkt für die eigentliche Problemstellung dienen, der sich der Vortrag widmet: Es wird um die Frage gehen, ob und in welcher Form sich aktuelle Diskurse und Perspektiven der Gender Studies bei der Modellierung von Personendaten zu Zeitgenoss*innen berücksichtigen und anwenden lassen, und welche Konsequenzen das für das Datenmodell, die Daten selbst und den Workflow eines entsprechenden Projekts haben kann.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;1. Ausgangspunkt: Die TEI2016 Abstacts App&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Abstracts zur TEI Konferenz 2016 wurden via ConfTool eingereicht und anschließend in Microsoft Word und InDesign ediert, um ein gedrucktes Book of Abstracts herzustellen.&lt;sup&gt;1&lt;/sup&gt; Die InDesign-Datei wurde dann als PDF exportiert und online verfügbar gemacht. Diese beiden Fassungen des Book of Abstracts wurden vor der Konferenz fertiggestellt. Nach der Konferenz wurde die InDesign Datei erneut exportiert, diesmal in XML Format. Diese Datei wurde dann teils manuell, teils automatisiert in Einzeldateien (jedes Abstract als eine Datei) zerlegt, bearbeitet und mit umfangreichen Metadaten versehen, um ordentliche TEI-P5-Dateien herzustellen. Diese wurden auf GitHub veröffentlicht. Die jeweiligen &lt;TEIheader&gt; wurden mit umfassenden Informationen ausgestattet und detailliert codiert; etwa wurden innerhalb der &lt;editor&gt;- und &lt;presentationAuthor&gt;-Tags konsequent &lt;person&gt;-, &lt;forename&gt;- und &lt;surname&gt;-Tags eingesetzt. Das stellte sich für die Weiterverarbeitung, Analyse und Visualisierung der Daten später als vorteilhaft heraus.&lt;/p&gt;
&lt;p dir="ltr"&gt;Dieses Corpus bildete die Basis für die TEI2016 Abstracts App, in der die Abstracts in der Erstversion über ein Inhaltsverzeichnis, das nach Abstract-Titel oder nach Verfassenden sortiert werden konnte, ansteuerbar waren. Schon in dieser rudimentären Form zeigte sich das Potential der Daten und der Applikation für Auswertungen, die über Struktur und Zusammensetzung der Konferenz und ihrer Beitragenden Auskunft geben würden. Die App wurde daher sowohl im Bereich der Funktionalität als auch auf der Ebene der enthaltenen Daten substantiell weiterentwickelt.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;2. Fragestellung und gender-theoretische Überlegungen&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Wir trafen die Entscheidung, der weiteren Entwicklung des Prototyps der App eine Forschungsfrage zugrundezulegen. Wir entschieden uns für die Frage nach der Geschlechterverteilung unter den Beitragenden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Aus Perspektive der Gendertheorie (in Butler’scher Tradition) bedarf die Entscheidung, diese Frage zu stellen, an sich eine Rechtfertigung, denn “a performative utterance (or practice) brings into being that of which it speaks.” (Butler 2010, 150f) Das bedeutet, dass die Frage nach der Verteilung der Geschlechter die Unterscheidung der Geschlechter erst hervorbringt, und so die historische Situation, die unser Körper bedeutet, fortsetzt: “As an intentionally organized materiality, the body is always an embodying of possibilities both conditioned and circumscribed by historical convention. In other words, the body is a historical situation, as Beauvoir has claimed, and is a manner of doing, dramatizing, and reproducing a historical situation.” (Butler 1988, 521) Berücksichtigt man außerdem die Konzepte des &lt;em&gt;Doing&lt;/em&gt; bzw. &lt;em&gt;Undoing Gender&lt;/em&gt; (West &amp; Zimmerman 1987; Hirschauer 1994 &amp; 2001), so wird deutlich, dass die oben formulierte Frage deutlich präzisiert werden muss, um die zu erhebenden Daten in einer Weise strukturieren zu können, die eine zeitgenössische Auffassung von Geschlechteridentitäten wiedergibt. Beide Ansätze gehen davon aus, dass Geschlechterrollen und -identitäten nur durch kontinuierliche Auslebung und Nicht-/Erfüllung von Erwartungen entstehen. Will man demnach die Geschlechterverteilung innerhalb einer Gruppe erheben, muss man zuerst definieren, was man unter Geschlecht eigentlich versteht.&lt;/p&gt;
&lt;p dir="ltr"&gt;Die Frage nach dem Geschlecht der Konferenzbeitragenden wurde daher wie folgt konkretisiert: Da sie aus einem Interesse an der Stellung bzw. Repräsentation der Geschlechter im gegenwärtigen Wissenschaftsbetrieb heraus gestellt wurde, richtet sie sich nicht auf die biologische Beschaffenheit der Körper der Konferenzbeitragenden, sondern auf ihre gesellschaftliche (Selbst-)Wahrnehmung (also auf &lt;em&gt;gender&lt;/em&gt; und nicht &lt;em&gt;sex&lt;/em&gt;). Folgt man Butlers Auffassung, nach der die performative Äußerung das, wovon sie spricht, hervorbringt, wären daher eigene Angaben der Betroffenen zu ihrem Geschlecht die ideale Datenquelle für diese Fragestellung gewesen. Solche Angaben standen allerdings nicht zur Verfügung.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;3. Datenmodellierung und -anreicherung&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Nachdem eine Personenliste aus dem Abstracts-Corpus extrahiert worden war, kamen wir zu dem Schluss, dass das Zuweisen von Geschlechtern an Personen unter Berücksichtigung der oben skizzierten geschlechtertheoretischen Überlegungen kein gangbarer Weg sein konnte. Um die soziodeterministische Neugier, aus der heraus die Ausgangsfrage gestellt worden war, dennoch befriedigen zu können, entwickelten wir ein gendersensibles Workaround für die Modellierung der Daten, das uns dennoch eine Erhebung der Geschlechterverteilung erlaubte: Anstatt den &lt;person&gt;s ein &lt;sex&gt; zuzuweisen, wie es die TEI-Richtlinien vorsehen, entschieden wir uns dafür, dem jeweiligen &lt;forename&gt; einen @type zuzuweisen, der eine Angabe zum (sozialen) Geschlecht enthalten sollte. Diese Vorgehensweise erlaubte es, die Daten nicht den eigentlichen Personen nachzubauen und damit annahmen über deren Selbstwahrnehmung zu treffen, sondern zu erheben, welchem Geschlecht sie von einer allgemeinen Gesellschaft oder Öffentlichkeit aufgrund ihrer Namen aller Wahrscheinlichkeit nach zugeordnet würden.&lt;/p&gt;
&lt;p&gt;Um das Vorgehen weiter zu objektivieren, entschieden wir uns gegen die Zuordnung der Geschlechter zu den Vornamen auf Basis unseres eigenen Weltwissens. Stattdessen machten wir uns auf die Suche nach Namensdatenbanken, die Angaben zum Geschlecht der enthaltenen Vornamen enthalten. Das Natural Language Toolkit (NLTK) etwa beinhaltet eine solche Liste (Kantrowitz 1997), jedoch werden dort keine Angaben gemacht, wie diese Liste zustande kam und auf welcher Grundlage die Geschlechter zugeordnet wurden. Wir entschieden uns daher für genderize.io, eine Datenbank, die nach eigenen Angaben auf Daten von Social-Media-Plattformen basiert, wo Personen die Informationen zu ihrem Geschlecht selbst bestimmen können. Diese Behauptung ist in Anbetracht der Tatsache, dass Plattformen wie Facebook mittlerweile eine große Anzahl an möglichen Geschlechteridentitäten jenseits eines klassischen bipolaren Geschlechtermodells zur Auswahl anbieten, in Zweifel zu ziehen, da genderize.io Namen ausschließlich als weiblich oder männlich einordnet. Dennoch beschlossen wir, diese Quelle der Liste im NLTK vorzuziehen: Wenn nämlich die Behauptung über die Zusammensetzung des Namenscorpus von genderize.io der Wahrheit entspricht, entspricht die gewählte Form der Datenerhebung exakt dem theoretischen Fundament unseres Ansatzes.&lt;/p&gt;
&lt;p dir="ltr"&gt;Nach dem Abgleich unserer Personenliste mit genderize.io via deren API konnten wir 102 von 124 Namen ein Geschlecht zuordnen. Da genderize.io nur zwei Geschlechter kennt, setzten wir “male”, “female” und “no-match” als @type-Werte ein. Die Namen, die nach diesem ersten Abgleich “no-match” waren, verglichen wir anschließend manuell mit der genderchecker.com-Datenbank, die ihre Geschlechtsangaben aus den "2001 and 2011 UK Census Data" bezieht, "together with multiple online sources and contributions from our 2m website visitors". Ein interessanter Aspekt des Datenmodells von genderchecker.com ist die Berücksichtigung der Möglichkeit eines dritten Geschlechts: "If we see just one instance of a name appearing as both male and female, we categorise it as unisex." Aus gendertheoretischer Perspektive ist dieses Modell ein Schritt in die richtige Richtung, wenngleich die Benennung “unisex” diskutabel scheint. In unserer Namensliste kam kein solcher Fall vor - nach dem Abgleich der verbleibenden 22 Namen mit genderchecker.com blieben drei Namen “no-match”.&lt;/p&gt;
&lt;p dir="ltr"&gt;Da Daten und Code, aus denen die TEI2016 Abstracts App gebaut ist, unter einer offenen Lizenz auf GitHub frei zugänglich sind, steht es den Beitragenden frei, ihr Geschlecht und damit unsere statistische Auswertung der Geschechterverteilung zu verändern - das Ergebnis bleibt also ein vorläufiges. Im geplanten Vortrag wird es im Rahmen einer live-Demonstration der App präsentiert werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;4. Literatur&lt;/strong&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;Peter Andorfer, Vanessa Hannesschläger (2017): TEI Abstracts 2016. A little application to publish the abstracts of the TEI conference 2016. http://tei2016app.acdh.oeaw.ac.at/&lt;/p&gt;
&lt;p dir="ltr"&gt;Judith Butler (1988): Performative Acts and Gender Constitution: An Essay in Phenomenology and Feminist Theory. In: Theatre Journal 40:4, 519–531.&lt;/p&gt;
&lt;p dir="ltr"&gt;Judith Butler (2010): Performative Agency. In: Journal of Cultural Economy 3:2, 147–161.&lt;/p&gt;
&lt;p dir="ltr"&gt;Genderchecker. http://genderchecker.com/&lt;/p&gt;
&lt;p dir="ltr"&gt;genderize.io: Determine the gender of a first name. https://genderize.io/&lt;/p&gt;
&lt;p dir="ltr"&gt;Vanessa Hannesschläger, Daniel Schopper (Hg.) (2017): TEI Conference and Members’ Meeting 2016. Book of Abstracts [XML/TEI files]. https://github.com/acdh-oeaw/TEI2016abstracts&lt;/p&gt;
&lt;p dir="ltr"&gt;Mark Kantrowitz (1997): Name Corpus: List of Male, Female, and Pet names. CMU Artificial Intelligence Repository. http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/&lt;/p&gt;
&lt;p dir="ltr"&gt;Natural Language Toolkit (NLTK). http://www.nltk.org/&lt;/p&gt;
&lt;p dir="ltr"&gt;TEI Consortium (2017): TEI P5: Guidelines for Electronic Text Encoding and Interchange. http://www.tei-c.org/release/doc/tei-p5-doc/en/Guidelines.pdf&lt;/p&gt;
&lt;p dir="ltr"&gt;Candance West, Don Zimmerman (1987). Doing Gender. In: Gender and Society 1:2, 125–151.&lt;/p&gt;
&lt;p dir="ltr"&gt;Stefan Hirschauer (1994): Die soziale Fortpflanzung der Zwei-Geschlechtlichkeit. In: Kölner Zeitschrift für Soziologie und Sozialpsychologie 46:4, 668–692.&lt;/p&gt;
&lt;p dir="ltr"&gt;Stefan Hirschauer (2001): Das Vergessen des Geschlechts: Zur Praxeologie einer Kategorie sozialer Ordnung. In: Kölner Zeitschrift für Soziologie und Sozialpsychologie (Sonderheft 41), 208–235.&lt;/p&gt;
&lt;p dir="ltr"&gt;---&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;sup&gt;1&lt;/sup&gt; Dieser Workflow ist, zugegebenermaßen, einer Konferenz im Bereich der Digital Humanities, und der TEI Konferenz im Besonderen, nicht ganz angemessen; eine Integration etwa des DH-Convalidators hätte den Prozess wesentlich gestrafft. Eingespielte Prozesse und Zeitdruck waren die Hauptgründe für diese Vorgehensweise.&lt;/p&gt;
</p58_abstract>
  <p59_paperID>296</p59_paperID>
  <p59_contribution_type>Vortrag</p59_contribution_type>
  <p59_acceptance>Poster</p59_acceptance>
  <p59_authors>Chiarcos, Christian
Donandt, Kathrin
Ionov, Maxim
Rind-Pawlowski, Monika
Sargsian, Hasmik
Wichers Schreur, Jesse</p59_authors>
  <p59_organisations>Goethe-Universität Frankfurt, Deutschland
Goethe-Universität Frankfurt, Deutschland
Goethe-Universität Frankfurt, Deutschland
Goethe-Universität Frankfurt, Deutschland
Goethe-Universität Frankfurt, Deutschland
Goethe-Universität Frankfurt, Deutschland</p59_organisations>
  <p59_emails>chiarcos@informatik.uni-frankfurt.de
donandt@informatik.uni-frankfurt.de
ionov@informatik.uni-frankfurt.de
pawlowski@lingua.uni-frankfurt.de
sargsyan@em.uni-frankfurt.de
wichersschreur@em.uni-frankfurt.de</p59_emails>
  <p59_presenting_author>Chiarcos, Christian</p59_presenting_author>
  <p59_title>Universal Morphology zwischen Sprachtechnologie und Sprachwissenschaft: Sprachressourcen für Kaukasussprachen</p59_title>
  <p59_abstract>&lt;p&gt;Universal Morphology zwischen Sprachtechnologie und Sprachwissenschaft: Sprachressourcen für KaukasussprachenChristian Chiarcos, Kathrin Donandt, Maxim Ionov, Monika Rind-Pawlowski, Hasmik Sargsian, Jesse Wichers SchreurGoethe-Universität Frankfurt1 Hintergrund&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Universal Morphology&lt;/em&gt; (&lt;em&gt;Unimorph, &lt;/em&gt;http://unimorph.github.io/) ist ein aktuelles Communityproject zur Erfassung und automatischen Generierung der Flexionsmorphologie unterschiedlichster Sprachen, primär im dem Rahmen der Sprach&lt;em&gt;technologie&lt;/em&gt;. Dieses sprach&lt;em&gt;wissenschaftlich &lt;/em&gt;nutzbar zu machen, ist der Gegenstand unseres Vortrags. Ein Kernproblem ist, dass Unimorph drei gänzlich unterschiedliche Wissenschaftszweige einbeziehen muss, die Sprachtechnologie, die vergleichende Sprachwissenschaft, sowie die individuellen Sprach- und Kulturwissenschaften. Das Unimorph-Schema muss hierfür einen für alle Seiten annehmbaren Kompromiss darstellen. Wir diskutieren behutsame Erweiterungen von Unimorph, um die Anwendbarkeit für die kaukasischen Sprachen im Besonderen und für Sprachdokumentationsdaten im Allgemeinen zu ermöglichen. Neben Schema-Erweiterungen betrifft dies auch Format und Datenmanagement von Unimorph-Repositorien.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Das &lt;strong&gt;Kaukasusgebiet&lt;/strong&gt; ist für den Reichtum und die Diversität seiner Sprachen bekannt, die oftmals eurozentristische Ansichten und Überzeugungen traditioneller Sprachwissenschaften infrage gestellt haben, und die sich daher in besonderer Weise als Ausgangspunkt für die kritische Prüfung von linguistischen Modellen mit universellem Anspruch eignen. Hier betrachten wir aktuelle Ansätze zur Schaffung von sprachübergreifenden (`universellen‘) morphologischen Annotationen im Rahmen des &lt;em&gt;Unimorph&lt;/em&gt;-Projektes (https://unimorph.github.io/).&lt;/p&gt;
&lt;p&gt;Die autochthonen &lt;strong&gt;kaukasischen Sprachfamilien&lt;/strong&gt; (kartvelisch, abxazisch-adygisch und nakhisch-dagestanisch) weisen interessante morphologische und morphosyntaktische Besonderheiten auf (vgl. auch, z.B., Klimov 1994):&lt;/p&gt;
&lt;ol start="1" type="1"&gt;&lt;li&gt;Reiche, oftmals agglutinierende Morphologie&lt;/li&gt;
&lt;li&gt;Verbozentrische Satzstruktur&lt;/li&gt;
&lt;li&gt;reiche Verbalmorphologie mit präfigierender Konjugation (v.a. kartvelisch)&lt;/li&gt;
&lt;li&gt;weit verbreitete (Split-)Ergativität&lt;/li&gt;
&lt;li&gt;Reiches Kasussystem (v.a. nakh-dagestanisch)&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Wir berichten Ergebnisse zu unserer Arbeit zu Batsbi (nakh-dagestanisch), Mingrelisch (kartvelisch), Khinalug (nakh-dagestanisch), des weiteren Armenisch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Armenisch&lt;/strong&gt; ist ein eigenständiger Zweig der indoeuropäischen Sprachen, der seit vorchristlicher Zeit Jahren im Kaukasus gesprochen wird, und im geographischen Sinne als Kaukasussprache angesprochen werden kann, aber die o.g. Merkmale nicht teilt. Darüber hinaus werden im Kaukasus Iranisch, Turksprachen, und afroasiatische Sprachen gesprochen, die jedoch erst später eingewandert sind. Alle Kaukasussprachen sind sprachtechnologisch schlecht unterstützt, viele sind bedroht, die meisten (mit Ausnahme von Georgisch, Armenisch und Albanisch/Udi) wurde  erst in jüngerer Vergangenheit verschriftlicht. Allen gemeinsam ist ein großer Lehnwortschatz (u.a. aus dem Iranischen, Türkischen, Russischen) und Lehnbeziehungen untereinander.&lt;/p&gt;
&lt;p&gt;2 Unimorph für Sprachdokumentation im Kaukasus?&lt;/p&gt;
&lt;p&gt;Das Unimorph &lt;strong&gt;Datenformat&lt;/strong&gt; ist eine Liste von tab-separierten Einträgen für jeweils eine Wortform, ihr Lemma und entsprechenden Unimorph. Letztere sind eine Menge von nicht qualifizierten Merkmalen, die durch ein Semikolon voneinander getrennt und in unsortierter Reihenfolge aufgeführt werden. In der Verbalmorphologie sind z.B. Person und Numerus voneinander getrennt. Der Eintrag für deutsch „(ich) treffe (dich)“ wäre dementsprechend&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;treffen  treffe  V;IND;PRS;1;SG&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Der Eintrag für mingrelisch &lt;em&gt;kešerxvaduk&lt;/em&gt; (`Ich werde dich treffen') besitzt folgende interlineare Glosse:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;kešerxvaduk                        &lt;/p&gt;
&lt;p&gt;ke-   še-   r-    xvad  -u    -k&lt;/p&gt;
&lt;p&gt;AFF   PV    O2SG  meet  TM    S1SG&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;In Unimorph wird diese Analyse wie folgt repräsentiert:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;xvad  kešerxvaduk  AFF;LGSPEC4;ARGDA2S;V;LGSPEC6;ARGNO1S&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Das Verb kongruiert hier mit beiden syntaktischen Argumenten: des Subjekts (1S, Nominativ) und des Objekts (2S, Dativ), für die &lt;strong&gt;zusammengesetzte Merkmale&lt;/strong&gt; gebildet werden, die ein Argument mit dessen Person, Numerus usw. zusammenstellt; z.B. werden hier ArgNo1S für „Nominativargument=1. Person Singular“ und ArgDa2S für „Dativargument=2. Person Singular“ aufgeführt. Ein methodisches Problem ist, dass das Unimorph-Schema dieselbe Information hierbei in unterschiedlicher Weise ausdrückt, wie im Vergleich deutlich wird: mingrelisch ArgNo1S entspricht deutsch 1;SG. Da der Zusammenhang zwischen Kasus und grammatischen Rollen für das Deutsche nicht explizit definiert ist, gibt es keine Möglichkeit, diese automatisch als äquivalent zu interpretieren. Leider erlaubt es Unimorph zudem nicht, herkömmliche Terminologie zu verwenden, in der beide Argumente bzgl. ihrer syntaktischen Rollen (`Subjekt' und `Objekt') beschrieben werden, sondern zieht stattdessen die Kasusmorphologie heran. Dies ist insofern problematisch, als der Kasus im Verb nicht morphologisch realisiert ist, und Argumente im Satz nicht (pro)nominal realisiert werden müssen. Konventionell werden statt dessen grammatische Rollen verwendet.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Ein weiteres Problem, das auch in diesem Beispiel deutlich wird, ist der Umgang mit &lt;strong&gt;Segmentierung&lt;/strong&gt;. Konventioneller interlinear glossierter Text folgt einer Morph(em)-Segmentierung, bei der Morpheme zunächst identifiziert und dann nach ihrer Funktion glossiert werden. Unimorph dagegen standardisiert diese Funktionen segmentierungsunabhängig. Um praktisch nutzbar zu sein, sollten morphologische Inventorien allerdings dieselbe Segmentierung wie die annotierten Korpora besitzen, d.h., wird ein Merkmal durch die Kombination mehrerer Morpheme ausgedrückt (wie hier Tempus, Aspekt und Modus durch TM+PV). In Unimorph können diese separat aber nur als sprachspezifische Merkmale beschrieben werden. Problematisch ist nun, dass sprachspezifische Merkmale nur durch (LgSpec+) numerische Indizes dargestellt werden. Dies schränkt ihre Interpretierbarkeit ein. Auch wenn die Definitionen dieser Kennzeichnungen in einer separaten Datei aufgeführt werden, stellt die Reduktion auf nummerierte LGSPEC-Marker eine unnötige Hürde für die Lesbarkeit der Daten dar. Eine transparentere Lösung wäre durch die Erweiterung der LGSPEC-Marker um lesbare und etablierte Abkürzungen, wie sie in der konventionellen Glossierung verwendet werden, gegeben.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Ähnlich zu (mehreren Argumenten von) Verben gibt es für Nomina Fälle &lt;strong&gt;mehrfacher Kodierung derselben Merkmalskategorie&lt;/strong&gt; (v.a. Kasus), die mit Unimorph nicht behandelt werden können. In der sog. &lt;em&gt;Suffixaufnahme&lt;/em&gt; spezifizieren adnominale Elemente &lt;em&gt;neben&lt;/em&gt; ihrem inhärenten Kasus auch Agreement-Merkmale ihres Kopfnomens, z.B. durch Wiederholung von dessen Kasusmorphologie. Dies wurde ursprünglich für Georgisch beschrieben, gilt aber als verbreitet im Kaukasus. Bedauerlicherweise kann diese Information in Unimorph nicht positional kodiert werden, sondern erfordert eine Erweiterung des Label-Inventars. Deshalb schlagen wir die Einführung numerischer Indizes in der nominalen Morphologie vor, wobei der inhärente Kasus unbezeichnet bleibt, der Kasus des direkten Kopfes durch Anhängen von -1 an das Feature-Label, der Kasus von dessen Kopf durch -2, usw.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Diese &lt;strong&gt;numerischen Indizes&lt;/strong&gt; sind auch auf die verbale Domäne übertragbar. Gegeben etablierte Hierarchien grammatischer Rollen bzw. der zugeordneten Kasus, kann die bisherige Verbundmarkierung multipler Argumente durch eine Indizierungsstrategie ersetzt werden, die sich auf diese bezieht, und bei der das höchstrangige Element (z.B. das Subjekt) unbezeichnet bleibt, während andere Argumente nach ihrer Stellung im Ranking gekennzeichnet werden. Eine alternative Repräsentation des mingrelischen &lt;em&gt;kešerxvaduk&lt;/em&gt; wäre also&lt;/p&gt;
&lt;p&gt;V;...&lt;/p&gt;
&lt;p&gt;1;SG;&lt;/p&gt;
&lt;p&gt;2-1;SG-1&lt;/p&gt;
&lt;p&gt;Dies korrigiert auch die &lt;strong&gt;Asymmetrie zwischen zusammengesetzten und individuellen Merkmalen. &lt;/strong&gt;Auch die Zuschreibung mehrerer Merkmale einer Kategorie kann nominal und verbal einheitlich gehandhabt werden, und die Vergleichbarkeit über Sprachen hinweg wird vereinfacht.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Ein anders gelagertes Problem ist das in Unimorph verwendete, und nicht erweiterbare &lt;strong&gt;Datenformat&lt;/strong&gt;: In einem Forschungsfeld, wo Interlinearglossierung der State of the Art ist, stellt die Bereitstellung von Morphem-Inventorien in unvollständigen und weniger gut interpretierbaren Repräsentationen ein Akzeptanzproblem dar. Daher sollte Unimorph nicht als eigenständiger Formalismus verstanden werden, sondern als Austauschformat zwischen reichen und hochwertigen Sprachressourcen auf der einen Seite und morphologischen Generatoren auf der anderen. Allerdings sind &lt;em&gt;Format und Speicherort&lt;/em&gt; festgeschrieben, so dass zugrundeliegende Ressourcen an anderen Orten gespeichert und gepflegt werden müssen, und Ergänzungen aus der Sprachdokumentationsarbeit womöglich nicht eingepflegt werden. Wir schlagen daher vor, das jetzige Spaltenformat lediglich als Zwischenrepräsentation zu definieren, die bei Bedarf aus verschiedenen Quellformaten (XML, CSV, JSON, RDF) heraus generiert wird. Der Schlüssel hierzu liegt darin, diese Formate auf Basis von W3C-Standards auf RDF-Datenstrukturen um RDF-Links zu ergänzen (RDFa, JSON-LD) oder gänzlich auf RDF zu mappen (GRDDL, CSV2RDF), mit Hilfe von SPARQL Update auf eine lemon-Repräsentation (https://www.w3.org/2016/05/ontolex/) hin zu normalisieren, sowie mit Hilfe einer SPARQL SELECT-Query daraus das derzeitige Tabellen-Format bei Bedarf zu generieren. Das Repository enthält dann für jede Sprache die (a) &lt;em&gt;vollständigen&lt;/em&gt; Daten, und (b) ein standardisiertes Mapping auf lemon. Die TSV-Generierung ist nicht ressourcenspezifisch.&lt;em&gt;&lt;/em&gt;Der Gebrauch von RDF-Technologien für Datenkonversion und der RDF-Abfragesprache SPARQL für Datentransformation und -abfrage kann somit die Entwicklung einer technischen Infrastruktur für Unimorph ermöglichen, die es erlaubt, über die Grenzen des TSV-Formats hinauszuwachsen, wovon SprachwissenschaftlerInnen, ForscherInnen und NLP-IngenieurInnen, die mit &lt;em&gt;low-resource&lt;/em&gt;-Sprachen arbeiten, profitieren könnten.&lt;/p&gt;
&lt;p&gt;3  Anwendung und Ausblick&lt;/p&gt;
&lt;p&gt;Das Projekt `Linked Open Dictionaries' (LiODi, 2015-2020) ist eine aus NachwuchswissenschaftlerInnen bestehende und vom Bundesministerium für Bildung und Forschung (BMBF) finanzierte eHumanities-Forschungsgruppe, die daran arbeitet, einen nutzerorientierten Zugang zu LOD-Technologien in den Sprachwissenschaften zu entwickeln und diesen in Einzelstudien zum Sprachkontakt im Kaukasus zu demonstrieren. Ein wichtiges Element dafür sind Lehnwortuntersuchungen, und in morphologisch reichen Sprachen ist es möglich, dass eine flektierte Form Gegestand der Entlehnung war. Diese automatisch gestützt generieren und identifizieren zu können, erfordert einen morphologischen Generator, der auch über unvollständigen Daten hinweg generalisieren, und beispielsweise Paradigmen komplettieren kann. Unimorph-Ressourcen und –Technologien sind so eine höchst willkommene Ergänzung unserer Arbeit. Die praktische Anwendung des Schemas auf Sprachen der Kaukasus-Region erwies sich jedoch als problembehaftet.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Aufgrund dessen plädieren wir für die Einführung numerischer Indizes für verschiedene Argumente polyvalenter Verben und rekursive Merkmale in der Nominalflexion. Für die bessere Integration von existierenden Ressource aus der Sprachdokumentation insgesamt schlagen wir zudem eine Erweiterung der unterstützten Formate vor, so dass das jetzige Unimorph-Format nicht mehr von den zugrundeliegenden, reicheren Quelldaten separiert wird, sondern bedarfsabhängig daraus generiert wird.&lt;/p&gt;
&lt;p&gt;Literatur&lt;/p&gt;
&lt;p&gt;[Sylak-Glassman 2016]&lt;/p&gt;
&lt;p&gt;John Sylak-Glassman. The composition and use of the universal morphological feature schema (UniMorph schema). Technical report, Technical report, Department of Computer Science, Johns Hopkins University, https://unimorph.github.io/doc/unimorph-schema.pdf, accessed 2017-09-23, 2016. working draft, v.2.&lt;/p&gt;
</p59_abstract>
  <p60_paperID>144</p60_paperID>
  <p60_contribution_type>Vortrag</p60_contribution_type>
  <p60_acceptance>Poster</p60_acceptance>
  <p60_authors>Dahnke, Michael</p60_authors>
  <p60_organisations>Universität Würzburg, Deutschland</p60_organisations>
  <p60_emails>fedja_anatevka@web.de</p60_emails>
  <p60_presenting_author>Dahnke, Michael</p60_presenting_author>
  <p60_title> Professionalisierung der Ausbildung von Geisteswissenschaftlern in der Digitalisierung von Texten</p60_title>
  <p60_abstract>&lt;p&gt;Die Professionalisierung, die Didaktisierung und die Kanonisierung der Didaktik sind relevante Themen für die DH im deutschsprachigen Raum. Darum engagiert sich der Referent in der AG &lt;em&gt;Referenzcurriculum Digital Humanities&lt;/em&gt; und plädiert mit seinem Vortrag dafür, die DH-Ausbildung auf dem zentralen Gebiet der Digitalisierung von Texten stärker zu professionalisieren und zu kanonisieren. Mit seiner eigenen Lehrveranstaltung bietet er dafür eine Referenz, die er hiermit zur Diskussion in der Community stellt.&lt;/p&gt;
&lt;p&gt;Dass die Ausbildung von Geisteswissenschaftlern auf dem zentralen Gebiet der Digitalisierung von Texten entsprechend verbessert werden muss, ist evident, wenn man sich folgendes vergegenwärtigt: zwei Kernkompetenzen digitaler Geisteswissenschaftler sind die Erstellung von und die Fähigkeit zur Beurteilung von Software. Was für die Software, das Werkzeug, gilt, trifft gleichermaßen auf die verschiedenen Werkstoffe zu: das sind die verschiedenen Repräsentationsformen digitaler Objekte. Die Erstellung und die Einschätzung der Nützlichkeit ist genau wie die der Werkzeuge Aufgabe des digital arbeitenden Geisteswissenschaftlers. Dabei müssen digital arbeitende Geisteswissenschaftler nicht nur die inhaltliche Komponente der Werkstoffe, sondern auch ihre technische Beschaffenheit beurteilen können.&lt;/p&gt;
&lt;p&gt;Im Vortrag zur Professionalisierung der Vermittlung von Digitalisierung von Textkompetenz wird erstens verdeutlicht, dass textbasiert arbeitende digitale Geisteswissenschaftler wissen müssen, wie man aus einem gedruckten Korpus einen digitalen Volltext erstellt. Im zweiten Teil wird der Digitalisierungskurs der UB Würzburg beispielhaft für die Vermittlung der dazu benötigten Kenntnisse und Fähigkeiten vorgestellt. Welche ähnlichen oder identischen Angebote es darüber hinaus im deutschsprachigen Raum gibt, wird im dritten Teil thematisiert. Zentrale Motive für die diesbezügliche Suche sind folgende Fragen: Reicht das vorhandene Angebot bereits aus und muss nur noch das Bewusstsein der heute die Curricula Betreuenden für den Erwerb dieser Kernkompetenzen gestärkt werden? Oder kann der Bedarf in diesem Bereich bislang noch nicht einmal befriedigt werden und sind die Angebote darum deutlich auszubauen?&lt;/p&gt;
&lt;p&gt;Am Anfang der Transformation vom gedruckten zum digitalen Korpus steht die Suche nach bereits vorhandenen Digitalisaten. Diese Suche setzt neben nicht DH-spezifischen Kenntnissen im Bereich Informationskompetenz das Wissen um Metadaten zu digitalen Bildformaten voraus. Der digitale Text-Geisteswissenschaftler muss in dieser Situation wissen, dass die Chancen eines erfolgreichen OCR mit einem JPEG mit 72 dpi deutlich geringer sind als mit einem unkomprimierten TIFF, True Color und 300 dpi. Sollte er schließlich feststellen, dass ihm Digitalisate in der gewünschten Form nicht zugänglich sind, bedarf er der Kenntnisse zu digitalen Bildformaten genauso, um im nächsten Schritt erfolgreich den Scan selbst durchzuführen resp. nach seinen Vorgaben durchführen zu lassen. Analog braucht er auch vertiefte Kenntnisse der Digitalisierung. Er braucht sie nicht, um die weiteren Schritte auf dem Weg vom gedruckten zu dem mit den benötigten Metadaten angereicherten digitalen Korpus alle selber gehen zu müssen, sondern gegebenenfalls anleiten und die Ergebnisse beurteilen zu können. Die textbasiert arbeitenden digitalen Geisteswissenschaftler sind oft noch zu abhängig von Spezialisten in diesem Bereich. Bezüglich der Programmierung leuchtet das sofort ein. Darum wird in den zahlreichen DH-Studiengängen im deutschsprachigen Raum auch viel Wert auf Programmierkenntnisse gelegt. Genauso wichtig wie solide Kenntnisse der Programmierung ist die universitäre Lehre von der Textgewinnung aus nicht-digitalen Vorlagen.&lt;/p&gt;
&lt;p&gt;Das Würzburger Modul &lt;em&gt;Digitalisierung von Texten in den Geisteswissenschaften&lt;/em&gt; ist aus der Zusammenarbeit der Universitätsbibliothek Würzburg mit dem Lehrstuhl für Computerphilologie an der Universität Würzburg in einer früheren, inhaltlich ähnlichen Veranstaltung &lt;em&gt;Basismodul Digitalisierung&lt;/em&gt; entstanden. Diese richtete sich zunächst dezidiert an die BA-Studenten dieses Lehrstuhles. Die im Februar 2018 erstmals stattfindende fünftägige Nachfolgeveranstaltung &lt;em&gt;Digitalisierung von Texten in den Geisteswissenschaften&lt;/em&gt; der Würzburger Universitätsbliothek besteht aus zwei Teilen und steht BA-/MA- und Lehramtsstudenten gleichermaßen offen. Im ersten Teil werden an den ersten beiden Tagen, unterbrochen von kurzen Übungssequenzen, folgende Inhalte vermittelt:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;
&lt;p&gt;Wozu müssen textbasiert arbeitende, digitale Geisteswissenschaftler wissen, wie Text digitalisiert wird?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Digitalisate erhalten:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p&gt;Die Suche nach Digitalisaten als erster Schritt und die anschließende Entscheidung, wann warum in welcher Auflösung selbst Digitalisate zu erstellen sind.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unverzichtbar ist die Berücksichtigung konservatorischer Aspekte.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Informationen über die verschiedenen Scannertypen vom einfachen Buchkantenscanner über den Einzugsscanner bis zur Buchwippe mit Ansaugemechanismus, um zu wissen, was technisch möglich ist&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Das eigene Üben der Studenten am Einzugsscanner ist aus methodischer Sicht weniger wichtig als ihnen zu zeigen, warum man Vorlagen wie das &lt;em&gt;Book of Kells&lt;/em&gt; &lt;strong&gt;nicht &lt;/strong&gt;mit dem Einzugsscanner digitalisiert.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Textextraktion:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p&gt;Hier werden die Schritte von der Segmentierung bis zum OCR vorgestellt: das umfasst verschiedene Segmentierungssoftware (eg. LAREX) und die anschließende, möglichst automatisierte Texterkennung mit OpenSource Software wie &lt;em&gt;Tesseract&lt;/em&gt; oder &lt;em&gt;OCRopus&lt;/em&gt;. Insgesamt wird OpenSource Software angesichts der studentischen Zielgruppe anderen Lösungen vorgezogen. Im Bedarfsfall werden diese aber auch nicht ausgeschlossen.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Außerdem wird die Möglichkeit vorgeführt, neuronale Netze in &lt;em&gt;OCRopus&lt;/em&gt; für die automatische Texterkennung auf den Digitalisaten zu trainieren.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Auszeichnung/Anreicherung:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p&gt;Unverzichtbar ist die Auszeichnung sowohl der Digitalisate mit Metadaten als auch des ›Rohtextes‹. Für den extrahierten ›Rohtext‹ gilt das in zweifacher Hinsicht: Zum einen sind ihm Metadaten hinzuzufügen, welche die spätere, eindeutige Identifikation des Korpus und dessen Auffindbarkeit ermöglichen. Zum anderen muss der Text mit inhaltsbezogenen Elementen angereichert werden.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Konkret sind bei der digitalen Repräsentation eines Romans beispielsweise die Figuren, Orte, Zeitpunkte und gegebenenfalls weitere signifikante Entitäten im Text für das spätere, automatisierte Retrieval zu kodieren.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Für die visuelle Präsentation, beispielsweise auf einem Webportal, sind textstrukturierende Merkmale wie die Einteilung nach Kapiteln, Abschnitten, Fußnoten etc. zu kennzeichnen.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Dem unterschiedlichen Kenntnisstand der Teilnehmer geschuldet muss hier vor der Vorstellung der &lt;em&gt;TEI Guidelines&lt;/em&gt; zuvor zweifelsohne XML dargestellt werden. Wie ausführlich daneben DC und bibliotheksspezifische Formate (MARC21) thematisiert werden, ist noch nicht entschieden.&lt;/p&gt;
&lt;ol start="5"&gt;&lt;li&gt;
&lt;p&gt;Rechtliche Grundlagen der Bilddigitalisierung: Die Teilnehmer werden intensiv mit den juristischen Grundlagen der Bilddigitalisierung vertraut gemacht.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p&gt;Dazu gehören die Vorstellung des UrhG beziehungsweise speziell der § 60d und 60g UrhG in der novellierten Fassung des UrhWissG 2017,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;die Unterscheidung zwischen Immaterial- und Materialgüterrecht und was daraus für die Digitalisierung zweidimensionaler Objekte folgt,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;die Persönlichkeitsrechte des Urhebers und weiterer Betroffener sowie&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;der Umgang mit Werken, die unter der Creative Commons Lizenz stehen und die Möglichkeit, diese selbst zu benutzen.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Neben den in der Praxis der letzten Jahre von den einzelnen Dozenten entwickelten Folien wird für die Veranstaltung aktuell folgende Literatur benutzt:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;
&lt;p&gt;Kneißl, Michael: »Scannen wie die Profis : Text- und Bildvorlagen perfekt digitalisieren«. München: DTV. (2)2002.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Corbach, Almuth: &lt;em&gt;Bestandsschonendes Digitalisieren von schriftlichem Kulturgut&lt;/em&gt;. In: &lt;em&gt;Digital und analog. Die beiden Archivwelten. 46. Rheinischer Archivtag. Ratingen 21.-22. Juni 2012&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weitzmann, John H./Paul Klimpel: »Rechtliche Rahmenbedingungen für Digitalisierungsprojekte von Gedächtnisinstitutionen«. Berlin: »Zuse Institute Berlin«. &lt;em&gt;digiS – Servicestelle Digitalisierung Berlin&lt;/em&gt;. (3)2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Loewenheim, Ulrich/Adolf Dietz/Gerhard Schricker: &lt;em&gt;Urheberrecht. Kommentar.&lt;/em&gt; München: Beck. (4)2010.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jannidis, Fotis/Hubertus Kohle/Malte Rehbein [Hrsg.]: &lt;em&gt;Digital Humanities. Eine Einführung&lt;/em&gt;. Springer-Verlag GmbH Deutschland, 2017.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Die eine Stärke der Würzburger Lehrveranstaltung war und ist die Praxisorientierung, der nach der zweitägigen Einführung noch stärker mit einer drei Tage dauernden Übung Rechnung getragen wird. Bei dieser sollen die Studenten mit vorbereiteten Scans selbständig die Arbeitsschritte von der Segmentierung mit LAREX – einem derzeit am Lehrstuhl für Informatik VI der Universität Würzburg von Christian Reul entwickelten Segmentierungstools http://www.is.informatik.uni-wuerzburg.de/research_tools_download/larex/ – über die Verwendung von &lt;em&gt;Tesseract&lt;/em&gt; bis zum anschließenden Auszeichnen beziehungsweise der Anreicherung des digitalen Korpus mit den nötigen Metadaten selber vornehmen.&lt;/p&gt;
&lt;p&gt;Unverzichtbar für die gesamte Ausbildung im DH-Bereich ist neben dem Praxisbezug die Orientierung am neuesten Stand der Forschung in allen Teilbereichen. Dem wird beim Würzburger Modul &lt;em&gt;Digitalisierung in den Geisteswissenschaften&lt;/em&gt; erstens durch die genannte Zusammenarbeit mit dem Lehrstuhl für Informatik VI der Universität Würzburg Rechnung getragen. Zweitens ist der für die Veranstaltung verantwortlich zeichnende Dozent selbst Mitglied des Würzburger &lt;em&gt;Arbeitskreis Digitale Edition&lt;/em&gt; und arbeitet auch ganz praktisch in einem Editionsprojekt mit. Drittens ist an dieser Stelle die oben genannte personelle Verflechtung des Dozenten mit der AG &lt;em&gt;Referenzcurriculum Digital Humanities&lt;/em&gt; zu wiederholen.&lt;/p&gt;
&lt;p&gt;Denkbare Erweiterungen für eine zukünftig breiter angelegte Lehrveranstaltung der beschriebenen Art sind die Vorstellung a) des OCR von Handschriften, beispielsweise in der Kooperation mit einer &lt;em&gt;Transkribus&lt;/em&gt; https://transkribus.eu/wikiDe/index.php/Hauptseite anwendenden Institution, idealerweise DEA der Universität Innsbruck, und b) des Einsatzes virtueller Forschungsumgebungen und Transkriptionssoftware zur Herstellung digitaler Ressourcen.&lt;/p&gt;
&lt;p&gt;Aus Sicht des Lehrenden ist nach der erfolgreichen Durchführung die kritische Diskussion mit den Dozenten ähnlicher oder gleicher Veranstaltungen von anderen Institutionen unverzichtbar. Auch darum hofft er mit dem Vortrag wie die dem AG Treffen an der DHd2018 auf einen fruchtbaren Meinungsaustausch.&lt;/p&gt;
</p60_abstract>
  <sessionID>60</sessionID>
  <p61_paperID>177</p61_paperID>
  <p61_contribution_type>Vortrag</p61_contribution_type>
  <p61_acceptance>Poster</p61_acceptance>
  <p61_authors>Fichtner, Mark</p61_authors>
  <p61_organisations>Germanisches Nationalmuseum, Deutschland</p61_organisations>
  <p61_emails>m.fichtner@wiss-ki.eu</p61_emails>
  <p61_presenting_author>Fichtner, Mark</p61_presenting_author>
  <p61_title>Von Drupal 8 zur virtuellen Forschungsumgebung - Der WissKI-Ansatz</p61_title>
  <p61_abstract>&lt;p&gt;Die zunehmende Digitalisierung und der Umgang mit dem Semantic Web sind für die traditionellen Geisteswissenschaften eine zentrale Herausforderung der Zukunft. Dabei unterstützt das digitale Medium die Forschung nicht nur, sondern stellt auch Herausforderungen in Bezug auf den Wandel der Methoden, die Entwicklungsgeschwindigkeit und die Offenheit von Forschungsdaten. In den letzten Jahren haben sich zunehmend virtuelle Forschungsumgebungen als Forschungsplattform bei der Umsetzung von Forschungsprojekten etabliert. Ein steter Kritikpunkt aus Sicht der Benutzer ist dabei, dass der Umgang mit virtuellen Forschungsumgebungen in der Regel eine gewisse Affinität zum digitalen Medium und eine größere Einarbeitungszeit erfordert, als dies bei der traditionellen Erfassung von Daten im analogen Bereich nötig war. Zentraler Ansatz der Virtuellen Forschungsumgebungen ist indes das Gleichgewicht zwischen Komplexität bzw. einer großen Anzahl an Anwendungsmöglichkeiten auf der einen Seite und einer niedrigen Einstiegsschwelle mit einfacher Nutzbarkeit der Daten auf der anderen Seite zu wahren. Im Folgenden wird exemplarisch die Umsetzung eines solchen Ansatzes aus dem von der DFG geförderten Forschungsprojekt WissKI auf der Basis von Drupal 8 dargestellt. &lt;/p&gt;
</p61_abstract>
  <p62_paperID>154</p62_paperID>
  <p62_contribution_type>Vortrag</p62_contribution_type>
  <p62_acceptance>Poster</p62_acceptance>
  <p62_authors>Jacke, Janina
Horstmann, Jan
Meister, Jan Christoph</p62_authors>
  <p62_organisations>Universität Hamburg, Deutschland
Universität Hamburg, Deutschland
Universität Hamburg, Deutschland</p62_organisations>
  <p62_emails>janina.jacke@uni-hamburg.de
jan.horstmann@uni-hamburg.de
j-c-meister@uni-hamburg.de</p62_emails>
  <p62_presenting_author>Jacke, Janina
Horstmann, Jan</p62_presenting_author>
  <p62_title>Digital vs. Humanities. Didaktische Aufbereitung digitaler Methoden für die klassischen Geisteswissenschaften im Projekt forTEXT</p62_title>
  <p62_abstract>&lt;p&gt;In diesem Beitrag stellen wir das Projekt forTEXT (www.fortext.net) vor, das die Aufbereitung und Verfügbarmachung digitaler Methoden der Textinterpretation speziell für traditioneller arbeitende Geisteswissenschaftler im Kontext einer digitalen Forschungsumgebung zum Ziel hat. Erläutert werden zunächst forTEXTs konzeptionelle Dimensionen – im Anschluss daran präsentieren wir exemplarisch erste Ergebnisse aus dem Projekt: ein Lehrmodul zum manuellen taxonomiebasierten Annotieren.&lt;/p&gt;
&lt;p&gt;forTEXT orientiert sich insbesondere an zwei Paradigmen: Zum einen soll im Rahmen des Projekts der interpretative Umgang mit Texten (als genuin geisteswissenschaftliche Arbeitsweise) im Fokus stehen. Traditioneller arbeitende Literaturwissenschaftler sollen also in der digitalen Umsetzung ihrer individuellen Fragestellungen und Methoden unterstützt werden. Zum anderen wird darauf geachtet, dass die durch forTEXT zur Verfügung gestellten Informationen, Ressourcen und Tools weitgehend ohne technische Vorkenntnisse verständlich und nutzbar sind.&lt;/p&gt;
&lt;p&gt;Eine wichtige Komponente stellt forTEXTs individualisiertes Empfehlungssystem dar. In Form eines digitalen Fragebogens können Geisteswissenschaftler Angaben zu ihrem persönlichen Forschungsprojekt machen und bekommen eine individuelle Empfehlung darüber, welche digitalen Routinen, Ressourcen und Tools im Rahmen dieses Projekts förderlich sein könnten.&lt;/p&gt;
&lt;p&gt;Neben diesem Empfehlungssystem ist die forTEXT-Forschungsumgebung in drei Bereiche (Routinen, Ressourcen, Tools) gegliedert. Unter "Routinen" können forTEXT-Nutzer leicht verständliche Beschreibungen typischer digitaler Methoden zur Textinterpretation (z.B. Annotation, Topic Modeling etc.) finden sowie Lehrmodule, mithilfe derer diese Methoden erlernt oder unterrichtet werden können. Der Bereich "Ressourcen" enthält eine Liste und Beschreibungen bestehender (annotierter) Korpora, Tagsets etc., die von Geisteswissenschaftlern nachgenutzt werden können. Unter "Tools" werden Werkzeug-Suites und Funktionskomponenten gelistet, beschrieben und teilweise selbst entwickelt, mithilfe derer die relevanten DH-Methoden auf einfache Weise durchgeführt werden können.&lt;/p&gt;
&lt;p&gt;Abschließend beschreiben wir eines von forTEXTs Lehrmodulen (taxonomiebasiertes Annotieren) ausführlicher und zeigen, wie hier durch die Vermittlung des Prozess von digitalen Freitextannotationen hin zur Entwicklung und Nutzung einer Annotationstaxonomie forTEXTs (didaktische) Paradigmen umgesetzt werden können.&lt;/p&gt;
</p62_abstract>
  <p63_paperID>119</p63_paperID>
  <p63_contribution_type>Vortrag</p63_contribution_type>
  <p63_acceptance>Poster</p63_acceptance>
  <p63_authors>Haas, Gabriele
Koumpis, Adamantios
Handschuh, Siegfried</p63_authors>
  <p63_organisations>University of Passau, Deutschland
University of Passau, Deutschland
University of Passau, Deutschland</p63_organisations>
  <p63_emails>haasgab@fim.uni-passau.de
adamantios.koumpis@gmail.com
siegfried.handschuh@gmail.com</p63_emails>
  <p63_presenting_author>Haas, Gabriele</p63_presenting_author>
  <p63_title>Memes produzieren digitale Gefühle: Die Simpsons deuten Trump-Mania(c)</p63_title>
  <p63_abstract>&lt;p&gt;Einleitung&lt;/p&gt;
&lt;p&gt;Memes sind beliebte Vertreter Bildanhänge in Tweets. &lt;em&gt;„Ein Internet-Meme ist die humoristische/sarkastische Reaktion der Internetgemeinde auf ein (mediales) Ereignis.“ (Marx und Weidacher, 2014, S. 143)&lt;/em&gt; Memes im politischen Kontext sind aktuell wenig empirisch erforscht. Aus diesem Grund nimmt man zum einen die Wahl des US-Präsidenten inkl. des Vizepräsidenten vom 8. November 2016, zum anderen das Ereignis der Ersten 100 Amtstage von Präsident Donald John Trump zum Anlaß, die emotionale Diskussion via Twitter anhand von Memes in politischem Kontext zu analysieren. Als Reaktion auf die beiden politischen Ereignisse werden zwei Memes von den Simpsons auffällig. Im Kontext der Präsidentschaftswahl verbreitete sich schon zwei Tage nach dem Wahlstichtag ein statisches Bild (Meme) der Simpsons, worin schon im Jahr 2000 der Wahlausgang mit der geografischen Karte der Wahlergebnisse prophezeit wurde. Als Reaktion auf die Ersten 100 Tage von D. Trump im Amt wurde am 26. April 2017 ein Video von den Simpsons auf YouTube veröffentlicht, welches genau diesen Zeitraum seiner Amtshandlungen parodiert. Bereits einen Tag danach setzte die Diskussion zum Video auf Twitter ein.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;„I define an Internet meme as: (a) a group of digital items sharing common characteristics of content, form, and/or stance, which (b) were created with awareness of each other, and (c) were circulated, imitated, and/or transformed via the Internet by many users.” (Shifman L., 2014, S. 41)&lt;/em&gt; Auf Basis dieser Definition von L. Shifman zählen somit auch Videos zur Gattung der Memes.&lt;/p&gt;
&lt;p align="left"&gt; &lt;/p&gt;
&lt;p&gt;Forschungsstand&lt;/p&gt;
&lt;p&gt;Meinungen anderer Mitmenschen beeinflussen unsere eigenen Entscheidungen. Das ist aus der Psychologie schon lange bekannt. Auch die Sammlung und Auswertung von Meinungen wird schon lange betrieben. Mit dem Aufkommen des Web 2.0 ergeben sich viel bessere Möglichkeiten, große Datenmengen gezielt auf Meinungen hin zu analysieren. Die Sentiment Analyse zielt auf die Ergründung der Haltung, Stimmung, Meinung und die generelle Einstellung von Personen in Bezug auf ein speziell ausgewähltes Produkt, andere Personen, Dienstleistungen oder aktuellen Themen ab. Dieses Forschungsgebiet fällt in den Bereich der &lt;em&gt;Computerlinguistik&lt;/em&gt; bzw. der &lt;em&gt;linguistischen Datenverarbeitung&lt;/em&gt; &lt;em&gt;(Natural Language Processing)&lt;/em&gt;. Nach der Definition von &lt;em&gt;Liu Bing&lt;/em&gt; besteht eine Meinung aus einem 5-Tupel:&lt;/p&gt;
&lt;p align="center"&gt;&lt;em&gt;opinion = &lt;/em&gt;(&lt;/p&gt;
</p63_abstract>
  <p64_paperID>253</p64_paperID>
  <p64_contribution_type>Vortrag</p64_contribution_type>
  <p64_acceptance>Poster</p64_acceptance>
  <p64_authors>Sojer, Claudia
Tratter, Aaron Rudolf</p64_authors>
  <p64_organisations>Universität Innsbruck, Österreich
Universität Innsbruck, Österreich</p64_organisations>
  <p64_emails>claudia.sojer@uibk.ac.at
aaron.tratter@student.uibk.ac.at</p64_emails>
  <p64_presenting_author>Sojer, Claudia
Tratter, Aaron Rudolf</p64_presenting_author>
  <p64_title>Ambraser Heldenbuch: Transkription und wissenschaftliches Datenset</p64_title>
  <p64_abstract>&lt;p&gt;Seit Januar 2017 arbeitet eine Forschungsgruppe an der Universität Innsbruck unter der Leitung von Mario Klarer an dem ÖAW-go!digital-Projekt »Ambraser Heldenbuch: Transkription und wissenschaftliches Datenset«. Das Forschungsprojekt setzt sich zum Ziel, bis zum Jahr 2019 – dem 500. Todestag von Kaiser Maximilian I. – das &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; (Wien, Österreichische Nationalbibliothek, Cod. Ser. nova 2663) aus dem frühen 16. Jahrhundert zur Gänze zu transkribieren und als Forschungsdatenset online und offline öffentlich zugänglich zu machen. Das &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; wurde am Beginn des 16. Jahrhunderts von Kaiser Maximilian I. als Prunkhandschrift in Auftrag gegeben und vom Bozner Zollschreiber Hans Ried auf ca. 500 großformatigen Pergamentseiten ausgeführt. Im &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; sind einige der wichtigsten mittelhochdeutschen literarischen Texte in frühneuhochdeutscher Sprache teilweise als Unikate überliefert.&lt;/p&gt;
&lt;p&gt;Obwohl alle Texte des &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; in älteren gedruckten Editionen (meist in normalisierter Form) zugänglich sind, gibt es nur vereinzelte exakte Transkriptionen ausgewählter Werke, aber keine umfassende online zugängliche Gesamttranskription. Insbesondere die unternommene Fokussierung auf den exakten graphemischen Bestand und die Identifikation von Idiosynkrasien in diesen Einzeltexttranskriptionen bilden die Grundlage für die geplante Gesamttranskription des Kodex. Dabei wird eine allographische Transkription vorgenommen, sodass möglichst viele verschiedene Varianten der Grapheme unterschieden werden. Ein wichtiges Kriterium für die Zuordnung zu einer Variante sowohl bei den Buchstaben als auch bei den Superskripta spielt die Federführung Hans Rieds. Nur sehr wenige Transkriptionen bilden die verschiedenen Varianten der Grapheme in den Texten ab, sodass in diesem Bereich der Forschung noch Aufholbedarf besteht. Des Weiteren liegen die älteren Editionen fast ausschließlich in mittelhochdeutscher Sprache vor und sind somit Rücküberführungen und geben nicht den Zeichenwert der Vorlage wieder, obwohl 15 der 25 Texte im &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; als Unikate und somit nur in frühneuhochdeutscher Sprache vollständig überliefert sind. Mittels der genauen Transkription des &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; und Urkunden, die aus der Hand Hans Rieds stammen, wird der Weg für spätere Untersuchungen, inwiefern sich das Schreibverhalten Hans Rieds zwischen literarischen Texten und Gebrauchstexten unterscheidet, geebnet.&lt;/p&gt;
&lt;p&gt;Da die Transkription nicht nur auf der Buchstabenebene, sondern auf der Zeichenebene erfolgt, entsteht dadurch ein differenziertes und möglichst präzises Abbild des Manuskriptes. Die Grapheme werden durch unterschiedliche Schriftzeichen des Unicode-Zeichensatzes dargestellt. In der folgenden Abbildung sieht man einen Ausschnitt aus dem Nibelungenlied. In diesen zwei Zeilen wurde der Kleinbuchstabe &lt;em&gt;s&lt;/em&gt; von Hans Ried in vier verschiedenen Varianten geschrieben. In den Wörtern &lt;em&gt;Ross&lt;/em&gt;, &lt;em&gt;des&lt;/em&gt; und &lt;em&gt;Seyfrids&lt;/em&gt; tritt mit einem langen s eine kontextsensitive Variante des Buchstabens &lt;em&gt;s&lt;/em&gt; auf. Kontextsensitiv bedeutet in diesem Fall, dass es auf die Stellung des Buchstabens &lt;em&gt;s&lt;/em&gt; im Wort ankommt, ob es als langes oder rundes s geschrieben wird. Ansonsten treten drei unterschiedliche graphische Varianten des runden s auf.&lt;/p&gt;
&lt;p&gt;Abb. 1: f. 95vc ll. 7–8 ab imo&lt;/p&gt;
&lt;p&gt;Abb. 2: Transkription nach den neuesten Transkriptionsrichtlinien&lt;/p&gt;
&lt;p&gt;Die einzelnen Varianten der Buchstaben weisen eine relativ geringe Varianz auf, wodurch es nicht schwerfällt, diese voneinander zu unterscheiden. Anders verhält es sich jedoch bei den diakritischen Zeichen bzw. Superskripta. Es werden vier verschiede Superskripta unterschieden: Trema, Breve, Superskriptum o und Superskriptum a. Vor allem das Breve und das Superskriptum o weisen eine sehr starke Varianz auf, sodass rein graphisch nicht immer bestimmt werden kann, um welches Superskriptum es sich handelt. Neben der Federführung des Schreibers werden andere Kriterien herangezogen, so beispielsweise phonetische Merkmale, um diese Fälle zu behandeln. In der folgenden Abbildung treten alle zuvor genannten Superskripta auf. Bei den Wörtern &lt;em&gt;schon&lt;/em&gt; und &lt;em&gt;ergrummen&lt;/em&gt; wird die Problematik deutlich. Wegen der dünnen Strichstärke und der relativ großen Varianz bleibt stets ein gewisser Interpretationsspielraum bestehen.&lt;/p&gt;
&lt;p&gt;Abb. 3: f. 95ra ll. 17–21 ab imo&lt;/p&gt;
&lt;p&gt;Abb. 4: Transkription nach den neuesten Transkriptionsrichtlinien&lt;/p&gt;
&lt;p&gt;Die Transkription wird mit der Transkriptionsplattform &lt;em&gt;Transkribus&lt;/em&gt; durchgeführt, die im Rahmen des H2020 Projekts READ seit 2014 entwickelt wird und über 3600 registrierte Benutzer weltweit aufweist. Diese Plattform bietet nicht nur ein Experteninterface für die Erstellung einer zeichengetreuen Transkription, sondern sie bietet auch eine Reihe von Tools an, die eine enge Verknüpfung des Transkripts mit dem Bild der Seite bzw. der Wörter erlaubt.&lt;/p&gt;
&lt;p&gt;Bisherige Transkriptionen (und digitale Editionen) beschränken sich meist auf eine summarische Verlinkung des Textes auf Seitenebene mit dem zugehörigen Bild. Nur wenige digitale Editionen sind bisher den Schritt gegangen, Text und Bild auf Zeilen- oder Wortebene miteinander zu verknüpfen. Die Texte des &lt;em&gt;Ambraser Heldenbuches&lt;/em&gt; werden zuerst in &lt;em&gt;regions&lt;/em&gt; eingeteilt. Sie umfassen die (meist drei) Spalten auf einer Seite sowie die Foliierung. Darüber hinaus werden die Initialen, Lombarden und Bilder in &lt;em&gt;graphic regions&lt;/em&gt; segmentiert. Die Spalten werden dann zeilenweise in sogenannte &lt;em&gt;lines&lt;/em&gt; und &lt;em&gt;base lines&lt;/em&gt; unterteilt. In der folgenden Abbildung sieht man, dass der Anfang des Nibelungenliedes, wie oben beschrieben, segmentiert wurde. In dem Fenster unter dem Scan des Textes erfolgt dann die zeilenweise Eingabe der Transkription. Somit wird der Text direkt über das Eingabeinterface mit der ausgewählten Zeile des Manuskripts verlinkt und es ist keine spätere Angleichung nötig.&lt;/p&gt;
&lt;p&gt;Abb. 5: Screenshot der Transkriptionssoftware &lt;em&gt;Transkribus&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ein weiteres Merkmal des Projektes stellt die Aufbereitung des Datenmaterials dar. Mittels der Transkriptionssoftware können Buchstaben, Wörter oder Zeilen annotiert werden. Diese werden ausgewählt und mit dem entsprechenden Tag versehen. Das Projekt sieht vor, dass die Initialen getaggt werden und ihnen wird als Attribut zugewiesen, wie viele Zeilen die Initiale umfasst. In den nachfolgenden Abbildungen sieht man zuerst den Scan des Manuskriptes mit den segmentierten Zeilen und der Initiale. Abbildung 7 zeigt die Transkription mit den dazugehörigen Tags an. Der Buchstabe &lt;em&gt;H&lt;/em&gt; ist mit den Tag &lt;em&gt;bigInitial&lt;/em&gt; versehen, was man an der gelben Markierung erkennt. Abbildung 8 zeigt, dass dem Tag &lt;em&gt;bigInitial&lt;/em&gt; das Attribut &lt;em&gt;height&lt;/em&gt; angehört. Diesem Attribut wurde der Wert &lt;em&gt;7&lt;/em&gt; zugewiesen.&lt;/p&gt;
&lt;p&gt;Abb. 6: f. 229rb ll. 11–17&lt;/p&gt;
&lt;p&gt;Abb. 7: Transkription inkl. Tags&lt;/p&gt;
&lt;p&gt;Abb. 8: Attribut des Tags &lt;em&gt;bigInitial&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Neben den Initialen werden Lombarden, Rubrizierungen, Incipits, Explicits, die Enden von Versen und Strophen, Abbreviaturen, die Foliierung und Unklarheiten bei der Transkription getaggt. Dem Tag, mit dem Abbreviaturen versehen werden, wird durch das Attribut &lt;em&gt;expansion&lt;/em&gt; die aufgelöste Abkürzung zugewiesen.&lt;/p&gt;
&lt;p&gt;Gerade bei der Transkription der Superskripta können Unklarheiten auftreten. Diese Fälle können mit Hilfe des graphischen Bestandes nicht restlos geklärt werden, sodass ein Interpretationsspielraum vorhanden bleibt. In diesen Situationen werden andere Kriterien, beispielsweise phonetische Merkmale, herangezogen. Da aber auf graphischer Ebene nicht eindeutig bestimmt werden kann, um welches Zeichen es sich handelt, wird das Wort mit dem Tag &lt;em&gt;unclear&lt;/em&gt; versehen.&lt;/p&gt;
&lt;p&gt;Bis Ende 2019 wird zeitgleich zur allographischen Transkription auch an einer bedienungsfreundlichen Benutzeroberfläche für externe Benutzer und Benutzerinnen gearbeitet, die es ermöglichen sollte, all diese transkribierten Sonderformen des Originaltextes visualisieren zu können (z. B. zeichengetreue Transkription, normalisierte Transkription, Transkription mit Abbreviaturen, Transkription mit aufgelösten Abbreviaturen etc.).&lt;/p&gt;
&lt;p&gt;Um mit den Forschungsergebnissen ebenso eine breitere Öffentlichkeit zu erreichen, wird das &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; mit zwei weiteren Auftragswerken von Maximilian I. mit den neuesten Digitalisierungsmethoden bei den Maximilian-Feierlichkeiten im Jahr 2019 präsentiert. Das von Land Tirol und Stadt Innsbruck finanzierte Projekt sieht vor, drei unterschiedliche maximilianische Großauftragswerke mit modernsten Digitalisierungsmethoden zu erfassen, zu erläutern und zu vermitteln. Das Spektrum reicht von zweidimensionaler Buchkunst im &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt;, dreidimensionaler Memorialarchitektur mit Reliefdarstellungen am Grabmal in der Innsbrucker Hofkirche bis hin zum unvollendet gebliebenen figuralen Kaiserdenkmalsauftrag für den Dom zu Speyer. Alle drei Kunstwerke wurden oder werden in Form von Drittmittelprojekten an der Universität Innsbruck bearbeitet bzw. liegen bereits in Form von Großrohdaten vor.&lt;/p&gt;
&lt;p&gt;Für das &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; wird ein interaktiver Touchscreen-Tisch so ausgerichtet sein, dass dieser in seiner physischen Form einem geöffneten Codex auf einem Renaissance-Lesepult gleicht. Inszenierungsmodalitäten werden z. B. eine digitale Lupe sein, mit der man über bestimmte Teile der Handschrift streichen und Details die interessieren, mit einer Tippgeste von den beiden zentralen Screens, die das Buch darstellen, in einen der lateralen Screens heben und sich dort interaktiv und multimedial in die jeweiligen Details vertiefen kann.&lt;/p&gt;
&lt;p&gt;Ebenso ist ein breitenwirksamer Sammelband zum &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; geplant, der einem weitgefächerten Publikum fundierte Antworten zu Kontext, Geschichte, Entstehung, Protagonisten, Wirkung, Detailaspekte sowie Gegenwartsbezug rund um das &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; anbietet, das Publikum sozusagen von einem großen umfassenden Kontext, in die kleinen Details des &lt;em&gt;Ambraser Heldenbuchs&lt;/em&gt; einführt. Circa 10–15 internationale Expert/inn/en aus Österreich, Deutschland, der Schweiz, Italien, Amerika etc. arbeiten Teilaspekte rund um das &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; aus und begleiten den Text mit anschaulichem Bild- und Graphikmaterial. Diese Aufarbeitung von bestehenden Daten zu Maximilian I. und dem &lt;em&gt;Ambraser Heldenbuch&lt;/em&gt; soll einem nicht wissenschaftlichen aber interessierten Publikum die Themen im Maximilian-Jubiläumsjahr 2019 qualitativ hochwertig und wissenschaftlich fundiert näherbringen und hat in dieser Form ein Alleinstellungsmerkmal.&lt;/p&gt;
&lt;p&gt;Neben Text, Bild und Graphik werden verschiedene traditionelle und neue Medien verwendet, z. B. sollen auf den Seiten QR-Codes sowie relevante Scansoftware für Smartphones zur Verfügung gestellt werden, die zu zusätzlichem Video-, Photo-, Ton- und Textmaterial führen, beispielsweise zur Verlesung eines mittelhochdeutschen Epos durch eine/n Theatersprecher/in in der Originalsprache des Ambraser Heldenbuches, also in Frühneuhochdeutsch. Die Anthologie soll 2019 in Museen, Bibliotheken und Maximilian-Jubiläumsjahr-Locations aufgelegt werden.&lt;/p&gt;
</p64_abstract>
  <p65_paperID>243</p65_paperID>
  <p65_contribution_type>Vortrag</p65_contribution_type>
  <p65_acceptance>Poster</p65_acceptance>
  <p65_authors>Münster, Sander
Barthel, Kristina
Bruschke, Jonas
Friedrichs, Kristina
Kröber, Cindy
Maywald, Ferdinand
Niebling, Florian</p65_authors>
  <p65_organisations>TU Dresden, Deutschland
TU Dresden, Deutschland
JMU Würzburg, Deutschland
JMU Würzburg, Deutschland
TU Dresden, Deutschland
TU Dresden, Deutschland
JMU Würzburg, Deutschland</p65_organisations>
  <p65_emails>sander.muenster@tu-dresden.de
kristina.barthel@tu-dresden.de
jonas.bruschke@uni-wuerzburg.de
kristina.friedrichs@tu-dresden.de
cindy.kroeber@tu-dresden.de
ferdinand.maiwald@tu-dresden.de
florian.niebling@uni-wuerzburg.de</p65_emails>
  <p65_presenting_author>Münster, Sander</p65_presenting_author>
  <p65_title>Stadtgeschichtliche Forschung und Vermittlung anhand historischer Fotos als Forschungsgegenstand – Ein Zwischenbericht der Nachwuchsgruppe HistStadt4D </p65_title>
  <p65_abstract>&lt;p&gt;Anhand stadt- und baugeschichtlicher Forschungsfragen und Vermittlungsanliegen zur Historie der Stadt Dresden untersucht die durch das BMBF geförderte eHumanities-Nachwuchsgruppe HistStadt4D seit 2016 mit 4 Doktoranden und 4 Post-Docs methodische und technologische Ansätze, umfangreiche Repositorien historischer Medien und Kontextinformationen räumlich dreidimensional sowie zeitlich zusammenzuführen, zu strukturieren und zu annotieren sowie diese für Wissenschaftler und Öffentlichkeit mittels eines 4D-Browsers sowie einer ortsabhängigen Augmented-Reality-Darstellung (AR) als Informationsbasis, Forschungswerkzeug und zur Vermittlung geschichtlichen Wissens nutzbar zu machen. Zur DHd-Jahrestagung 2018 vorgestellt werden sollen die im ersten Jahr erzielten Ergebnisse, welche neben umfassenden Statuserhebungen in den jeweiligen Untersuchungsgebieten auch Prototypen der geplanten 4D-Browseranwendung sowie semi-automatischen 3D-Modellierung umfassen.&lt;/p&gt;
</p65_abstract>
  <sessionID>60</sessionID>
  <p66_paperID>189</p66_paperID>
  <p66_contribution_type>Vortrag</p66_contribution_type>
  <p66_acceptance>Poster</p66_acceptance>
  <p66_authors>Caria, Federico
Mathiak, Brigitte</p66_authors>
  <p66_organisations>Universität zu Köln, Deutschland
Universität zu Köln, Deutschland</p66_organisations>
  <p66_emails>federico.caria@live.it
bmathiak@uni-koeln.de</p66_emails>
  <p66_presenting_author>Mathiak, Brigitte</p66_presenting_author>
  <p66_title>Nutzertests an kritischen Editionen - Print oder digital?</p66_title>
  <p66_abstract>&lt;p&gt;Zusammenfassung&lt;/p&gt;
&lt;p&gt;In diesem Papier beschreiben wir zwei Nutzertests, die wir mit Digitalen Editionen durchgeführt haben. Im ersten Nutzertest vergleichen wir drei Digital Editionen miteinander. Dabei wird klar, dass digitale Editionen nicht immer einfach zu bedienen sind und viele der Nutzer Schwierigkeiten haben sich zurecht zu finden. Erstaunlich ist dabei, dass Nutzer Bedienbarkeit bei ihrer Wahl für relevanter halten als ein Mehr an Daten und Informationen. Im zweiten Nutzertest vergleichen kritische Editionen desselben Werks in drei verschiedenen Medien: als Buch, als PDF/eBook und als digital entstandene Online-Edition. Auch hier können wir feststellen, dass Bedienbarkeit ein relevanter Faktor bei der Wahl der Medien ist, den ein Mehr an Informationen nicht auszugleichen vermag. Das eBook gilt dabei als dasjenige, welchs am einfachsten zu bedienen ist. Es bietet die meisten Zusatzfeatures, die von den Nutzern am Häufigsten benutzt werden. Das Buch wird hingegen gerne für längere Lesepassagen genutzt. Die digitale Edition wurde allerdings von den Nutzern größtenteils nicht richtig verstanden.&lt;/p&gt;
&lt;p&gt;Im Folgenden werden wir zunächst die Hintergründe beleuchten, die uns dazu ermutigt haben diese Untersuchung durchzuführen. Dann werden wir dezidierter auf die Methode und die Ergebnisse eingehen und dabei insbesondere die Features behandeln, die von den Nutzern als besonders wichtig eingeschätzt wurden.&lt;/p&gt;
&lt;p&gt;Hintergrund&lt;/p&gt;
&lt;p&gt;In mehreren Umfragen konnte (Porter 2013, 2016) feststellen, dass zwar die Benutzung von digitalen Werkzeugen in den Geisteswissenschaften stark gestiegen ist, die Benutzung von digitalen Editionen allerdings über die Jahre konstant niedrig geblieben, oder sogar gesunken ist. Dies ist ein erstaunliches Ergebnis, wenn man bedenkt, dass immer mehr digitale Editionen verfügbar sind und diese zum Teil erhebliche Mehrwerte gegenüber den aus Printeditionen generierten PDF Versionen generieren.&lt;/p&gt;
&lt;p&gt;Unsere Hypothese ist daher, dass digitale Editionen schwieriger zu bedienen sind und weniger auf die Bedürfnisse der Endnutzer eingehen als dies bei Printeditionen der Fall ist.&lt;/p&gt;
&lt;p&gt;Methodik&lt;/p&gt;
&lt;p&gt;Wir haben beide Nutzerstudien mit einer ähnlichen Methodik durchgeführt. Die Nutzergruppe bestand in beiden Fällen aus fortgeschrittenen Studenten der Geisteswissenschaften, vornehmlich aus der Philosophie, Philologie und weiteren verwandten Gebieten. Auf diese Weise haben wir sichergestellt, dass alle Teilnehmer Vorerfahrung in der wissenschaftlichen Textarbeit hatten. Keiner der Teilnehmer war ausgewiesener Experte auf dem Gebiet der Editionen, die wir betrachtet haben. Dies stellt allerdings keinen systematischen Nachteil dar. Schließlich sind die behandelten Editionen nicht nur für Experten gedacht. Tatsächlich erleichtert diese Gegebenheit sogar es den Vergleich, da alle Teilnehmer auf demselben Stand sind.&lt;/p&gt;
&lt;p&gt;Die Teilnehmer werden für die Nutzerstudie in kleinen Gruppen in einen Raum gebeten und füllen dort zunächst einen Fragebogen zu ihren Vorerfahrungen und Präferenzen aus. Dann bekommen sie eine Liste von Aufgaben, die sie mit der vorgegebenen Edition lösen sollen. Ihre Vorgehensweise wird dabei mit Hilfe einer Screencapture Software aufgezeichnet. Nach Ablauf der Bearbeitungszeit bitten wir sie einen weiteren Fragebogen auszufüllen, in dem wir sie um pauschale Urteile zu der Edition bitten. Im Anschluss setzen sich die Teilnehmer zu einer Gruppendiskussion zusammen. Diese wird von uns an Hand eines Leitfadens geleitet und aufgezeichnet. Die Aufzeichnung wird anschließend transkribiert, kodiert und ausgewertet.&lt;/p&gt;
&lt;p&gt;Verwandte Literatur&lt;/p&gt;
&lt;p&gt;Nutzerstudien gelten als Standardwerkzeug um die Benutzbarkeit zu evaluieren. Sie werden in den Digitalen Geisteswissenschaften häufig eingesetzt, beispielsweise um digitale Ressourcen (Warwick 2006) oder digitale Werkzeuge für Historiker (Rücker et.al 2011) zu evaluieren. Nutzertests an digitalen Editionen (Kelly 2015, Santos 2015, Visconti 2010) konzentrieren sich typischerweise auf eine Edition und werten diese quantitativ aus. Bei der Entwicklung unserer Methodik haben wir uns vor Allem an der Untersuchung von Informationsbedürfnissen und Informationsverhalten orientiert (Barrett 2005, Belkin 1993, Buchanan 2005, Ellis 2003). Das Design der Aufgaben erfolgte in Anlehnung an (Unsworth 2000) Konzept der Primitive wissenschaftlicher Arbeit (Palmer et al 2009). Der Ansatz selbst basiert auf den Arbeiten von (Drucker 2011), allerdings erweitert, um digitale Editionen als Wissenswerkzeuge besser würdigen zu können (Bevan 1995, Porter 2016). Die transversale Analyse von (Rimmer 2008) zur Qualität von Forschung zwischen Print und Digital war dabei sehr hilfreich als Vergleich.&lt;/p&gt;
&lt;p&gt;Editionen im Vergleich&lt;/p&gt;
&lt;p&gt;In unserer ersten Nutzerstudie haben wir drei digitale Editionen miteinander verglichen. Die Studie selbst ist ausführlich in (Caria/Mathiak 2017) beschrieben. Daher folgt hier nur eine Zusammenfassung der Ergebnisse, um die zweite Nutzerstudie besser im Kontext verstehen zu können.&lt;/p&gt;
&lt;p&gt;Die drei Editionen, die wir miteinander verglichen haben, hatten wir vor der Studie aus etwa 40 möglichen Editionen ausgewählt. Dabei lag unsere Wahl bei den Editionen, von denen wir dachten, dass diese am besten zu bedienen sind. Trotzdem hatten viele der Teilnehmer Schwierigkeiten die Aufgaben korrekt zu bearbeiten, da sie zum Teil die dazu notwendigen, aber vorhandenen Funktionalitäten auf der Webseite nicht gefunden haben. Dementsprechend negativ waren die Urteile der Probanden. Auffällig war, dass es jedoch einen Widerspruch zwischen Wünschen und Präferenzen der Teilnehmer gab. Gewünscht wurde vor allem mehr Inhalt. Präferiert wurde jedoch die einzige Edition, die keine Faksimiles hatte, obwohl dies oft gewünscht wurde, weil sie die höchste Benutzbarkeit aufwies. Dies steht im Kontrast zu der allgemeinen Erfahrung, dass Wissenschaftler Inhalt vor Form bevorzugen (Kern/Mathiak 2015).&lt;/p&gt;
&lt;p&gt;Wir schließen daraus, dass es so eine Art unsichtbare Schwelle der Bedienbarkeit gibt, die erreicht werden muss, damit der Inhalt einer digitale Edition überhaupt zum Tragen kommen kann.&lt;/p&gt;
&lt;p&gt;Eine Frage des Mediums&lt;/p&gt;
&lt;p&gt;Die Frage, die sich daran unmittelbar anschließt ist, ob dies eine Erklärung für die Ergebnisse von (Porter 2016) ist. Mit anderen Worten: ob die digitalen Editionen deshalb nicht benutzt werden, weil ihnen die Bedienbarkeit fehlt, die das entsprechende Buch bzw. die digitalisierte Version eines Buches von Natur aus mitbringen.&lt;/p&gt;
&lt;p&gt;Wir haben dazu das Werk “Also sprach Zarathustra” von Friedrich Nietzsche ausgewählt. Als Print und PDF wurde die Ausgabe von (Nietzsche/Colli/Montinari 1997) repräsentiert. Die Onlineversion stammt von nietzschesource.org.&lt;/p&gt;
&lt;p&gt;Für den Eingangsfragebogen haben wir Fragen aus dem Fragebogen von Porter übersetzt. Die Antworten sind mit den Ergebnissen von Porter vergleichbar. In den Abbildungen 1 und 2 sind die am häufigsten genutzten digitalen und analogen Ressourcen unserer Nutzer angegeben. Abbildung 3 zeigt den präferierten Zugriffsweg zu verschiedenen Arten von Ressourcen.&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Abbildung 1: Die am häufigsten benutzten digitalen Ressourcen der letzten drei Monate.&lt;/em&gt;&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;&lt;em&gt;Abbildung 2: Die am häufigsten benutzten analogen Ressourcen der letzten drei Monate.&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;&lt;em&gt;Abbildung 3: Wie wurde auf diese Ressourcen vorwiegend zugegriffen (Print oder Digital)?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p align="left"&gt; &lt;/p&gt;
&lt;p&gt;Für den Nutzertest haben wir Aufgaben entwickelt, die den Nutzer auf eine natürliche Art an die Medien heranführen. Die gestellten Aufgaben waren vor allem interpretativ und verlangten von den Nutzern sich in den verschiedenen Primär- und Sekundärquellen zu orientieren und Änderungen mittels der Kommentare nachzuvollziehen. Im Anschluss an die Tests haben wir die klassischen Usability Metriken zu Effectiveness (Können die Ziele erreicht werden?), Efficiency (Wie viel Aufwand ist es die Ziele zu erreichen?) und Satisfaction (Wie zufrieden sind sie mit der Benutzbarkeit?) erhoben (vgl. Abb. 4).&lt;/p&gt;
&lt;p align="left"&gt; &lt;/p&gt;
&lt;p align="left"&gt;&lt;/p&gt;
&lt;p align="left"&gt;&lt;em&gt;Abbildung 4: Usability Metriken der drei Medien im Vergleich.&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p align="left"&gt; &lt;/p&gt;
&lt;p&gt;In der abschließenden Gruppendiskussion wurden schließlich die Vor- und Nachteile der verschiedenen Medien von den Nutzern diskutiert. Neben der Frage nach dem Medium, welches sie bevorzugen, waren auch die Kernfunktionalitäten der Editionen, also die Suche, Annotationen, Kompatibilität und Portabilität ein Thema.&lt;/p&gt;
&lt;p&gt;Die Nutzer waren sich einig, dass sie am Liebsten mit der PDF-Version arbeiten. Genannte Gründe waren: “die Suchfunktion”, “klare Navigationsstruktur”, “die Option Anmerkungen zu machen” und “bereits vertraut mit dem Interface”.&lt;/p&gt;
&lt;p&gt;Nietzsche.org wurde dabei durchaus aufgrund der Inhalte gewürdigt: „Die [Digitale Edition] sollte eigentlich die beste Wahl sein, dadurch, dass sie viele verschiedene Texte und Daten beinhaltet, allerdings habe ich lieber 5 Tabs bzw. PDF's offen. Somit kann ich fliegend hin und her wechseln, ohne dass ich wieder und wieder nach Texten in der digitalen Edition suchen muss.“&lt;/p&gt;
&lt;p&gt;Am Buch wurde hingegen vor Allem die Bequemlichkeit geschätzt. Es sei “besser für die Augen” und “lenkt nicht so ab”.&lt;/p&gt;
&lt;p&gt;Gewünschte Funktionalität&lt;/p&gt;
&lt;p&gt;Aus den Diskussionen konnten wir relativ klare und konkrete Kritikpunkte an der digitalen Edition identifizieren, die so oder so ähnlich auf die meisten der von uns untersuchten digitalen Editionen zutreffen (vgl. Abb. 5). Besonders häufig kam die Kritik an der Struktur bzw. Navigation innerhalb der Edition auf. Das Inhaltsverzeichnis gibt zwar eine relativ einfache Struktur vor, diese wurde allerdings von den Nutzern besser verstanden als die Struktur der Webseite. Ebenfalls sehr viele Nutzer arbeiten mit Annotationen. Dass dies auf der Webseite nicht unterstützt wurde, fiel unangenehm auf.&lt;/p&gt;
&lt;p&gt;Weitere häufige Beschwerden wurden über die Suchfunktion geäußert. Diese funktionierte nicht wie von den Nutzern erwartet. Hinzu kam die Komplexität der Webseite, welche durch die Suchfunktion und Navigation nicht ausreichend kompensiert wird.&lt;/p&gt;

&lt;p align="left"&gt;&lt;em&gt;Abbildung 5: Häufig geäußerte Kritik an der digitalen Edition&lt;/em&gt;.&lt;/p&gt;
&lt;p align="left"&gt; &lt;/p&gt;
&lt;p&gt;Fazit&lt;/p&gt;
&lt;p&gt;Kritische Editionen sind nicht nur Datenquellen, sondern dienen auch als Arbeitswerkzeuge. Dabei darf die Usability nicht als nettes Feature abgetan werden, denn sie sorgt dafür, dass Nutzer eher eine digitale Edition benutzen. Das Interface sollte daher nicht nur als letzter Schritt eines editorialen Prozesses gedacht werden, sondern von Anfang an berücksichtigt und systematisch erprobt werden.&lt;/p&gt;
&lt;p&gt;Als Kernfunktionalität sollten dabei mindestens eine Suchfunktion, eine Möglichkeit zum Kommentieren/Annotieren und zusätzlich noch eine Vergleichsfunktion in Betracht gezogen werden. Texte zu vergleichen gehört zu den häufigsten Tätigkeiten und aus unserer ersten Studie wissen wir, dass die Nutzer technische Unterstützung dazu sehr schätzen.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p66_abstract>
  <p67_paperID>274</p67_paperID>
  <p67_contribution_type>Vortrag</p67_contribution_type>
  <p67_acceptance>Poster</p67_acceptance>
  <p67_authors>Declerck, Thierry
Aman, Anastasija
Grünewald, Stefan
Lindemann, Matthias
Schäfer, Lisa
Skachkova, Natalia</p67_authors>
  <p67_organisations>DFKI GmbH, Deutschland
Universität des Saarlandes
Universität des Saarlandes
Universität des Saarlandes
Universität des Saarlandes
Universität des Saarlandes</p67_organisations>
  <p67_emails>declerck@dfki.de
aamann@coli.uni-saarland.de
stefang@coli.uni-saarland.de
malinux@t-online.de
lkschae@gmail.com
s9naskac@stud.uni-saarland.de</p67_emails>
  <p67_presenting_author>Declerck, Thierry</p67_presenting_author>
  <p67_title>Formalisierung von Märchen</p67_title>
  <p67_abstract>&lt;p&gt;Im Rahmen eines Projektes, das sich mit der automatisierten Analyse von Märchen in deutscher Sprache auseinandersetzt,  hat sich die Notwendigkeit ergeben, eine formale Repräsentation von Märchen zu bestimmen. Wir beschreiben in diesem Beitrag zum einen &lt;em&gt;welche &lt;/em&gt;Informationen in dieser Repräsentation enthalten sind und zum anderen, &lt;em&gt;wie &lt;/em&gt;diese Informationen in XML bzw. Python konkret codiert werden.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Codierte Information&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ein Märchen besteht im Sinne unseres Projektes aus den folgenden Bestandteilen:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;• Eine Menge von &lt;strong&gt;Orten&lt;/strong&gt;, an denen die Handlung spielt;&lt;/p&gt;
&lt;p&gt;• Eine Menge von &lt;strong&gt;Charakteren&lt;/strong&gt;, die an der Handlung beteiligt sind;&lt;/p&gt;
&lt;p&gt;• Eine zeitliche Abfolge von &lt;strong&gt;Szenen&lt;/strong&gt;, die jeweils an einem bestimmten Ort spielen und&lt;/p&gt;
&lt;p&gt;an denen jeweils eine Teilmenge der Märchencharaktere beteiligt ist;&lt;/p&gt;
&lt;p&gt;• Jede Szene besteht ihrerseits aus einer zeitlichen Abfolge von &lt;strong&gt;Dialogakten &lt;/strong&gt;zwischen&lt;/p&gt;
&lt;p&gt;den Märchencharakteren oder vom Erzähler zum Zuhörer; zusammengenommen bilden diese Dialogakte den Märchentext.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Im Folgenden werden die verschiedenen Bestandteile, sowie ihre Eigenschaften und Beziehungen untereinander, näher beschrieben.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Orte&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Die Orte, an denen das Märchen spielt, werden nur über ihren &lt;strong&gt;Typ &lt;/strong&gt;(Attribut type) charakterisiert. Mögliche Ortstypen sind dabei z.B. Wald, Schloss oder Stall. Daneben existiert außerdem der Typ Nirgendwo für Szenen ohne eindeutig bestimmbaren Ort (z. B. Abschnitte des Märchens, an denen nur der Erzähler beteiligt ist). Jeder Ort erhält eine spezifische ID der Form loc1, loc2 etc.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Charaktere&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Charaktere werden über eine Reihe von Eigenschaften charakterisiert, welche zum einen inhärente demographische Eigenschaften (Name, Alter, Geschlecht, Typ), sowie zum anderen externe Eigenschaften (Einstellung, Propp-Archetyp – s. (Propp 1977)) beinhalten. Beim &lt;strong&gt;Namen &lt;/strong&gt;(name) des Charakters handelt es sich um eine Zeichenkette, z.B. Rapunzel. (Wird ein Charakter auf mehrere Arten gerufen, so wird die häufigste Bezeichnung gewählt.) &lt;strong&gt;Alter &lt;/strong&gt;(age) des Charakters wird nicht in Zahlen, sondern in Stufen angegeben, da Märchen im Allgemeinen keine genauen Altersangaben enthalten; die möglichen Werte sind dabei toddler, child, teenager, young adult, adult und senior. Das &lt;strong&gt;Geschlecht &lt;/strong&gt;(gender) des Charakters wird den klassischen Vorstellungen folgend entweder mit male oder female angegeben. Zusätzlich gibt es den Wert none für geschlechtlich unterspezifizierte Charaktere wie Tiere, Monster usw. Der &lt;strong&gt;Typ &lt;/strong&gt;des Charakters unterscheidet z. B. zwischen human oder animal/monster. Für animal/monster unterscheiden wir zusätzlich nach &lt;strong&gt;Subtyp&lt;/strong&gt;, z.B. für Tiere nach Größe, also small, medium oder big, oder witch und demon für einen bestimmten Monstertyp. Eine binäre Feststellung der &lt;strong&gt;Einstellung bzw. Gesinnung &lt;/strong&gt;des Charakters verortet diesen auf der Gut-/Böse-Achse: evil oder neutral. Außerdem wird der &lt;strong&gt;Propp-Archetyp&lt;/strong&gt;des Charakters angegeben: hero, villain etc. (Propp, 1977). Jeder Charakter erhält eine spezifische ID von der Form ch1, ch2 usw. Außerdem gehören zu jedem Märchen zwei „Dummy“-Charaktere für Erzähler und Zuhörer, welche stets die IDs ch0 bzw. ch-1 und die Typen narrator bzw. listener zugewiesen bekommen. Dies ist nötig, um auch Passagen darstellen zu können, welche vom Erzähler gesprochen werden, der selbst ja kein eigentlicher Charakter der Handlung ist. Dies ist notwendig, um ein automatisches „Vorlesen“ des Märchens zu implementiern&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Szenen&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Szenen werden im Hinblick auf Zeit, Ort, beteiligte Charaktere sowie Propp Funktionen (Propp, 1977) beschrieben. Der &lt;strong&gt;Zeitpunkt &lt;/strong&gt;(time), zu dem die Szene spielt, wird anhand einer ID der Form t1, t2 usw. angegeben, wobei die IDs den linearen Ablauf der Zeit darstellen. Der &lt;strong&gt;Ort &lt;/strong&gt;(location), andem die Szene spielt, wird als String in Großbuchstaben angegeben, ausgewählt aus einer Liste mit Möglichkeiten. Der &lt;strong&gt;Übergang zur nächsten Szene&lt;/strong&gt; (transition) wird ebenfalls codiert, indem das Bewegungsverb, das den Übergang von einem Ort zum anderen beschreibt, oder die Phrase, die stattdessen den Szenenwechsel einleitet, angegeben wird. Die an der Handlung der Szene beteiligten &lt;strong&gt;Charaktere &lt;/strong&gt;werden mit ihren IDs angegeben, also z. B. ch2, ch3, ch5. Dabei werden alle Charaktere berücksichtigt, die in der Szene zugegen sind, auch wenn diese bspw. nicht sprechen. Die Propp-Funktionen und -Subfunktionen der Szene werden mit ihrem Symbol (nach der englischen Ausgabe Propp (1977)) angegeben, also z. B. A4 – theft of daylight. Jede Szene erhält eine spezifische ID der Form s1, s2 etc. Da die Märchenhandlung im Allgemeinen linear erzählt wird, ist der Index üblicherweise (aber nicht notwendigerweise) identisch mit demjenigen des Zeitpunkts der Szene, d. h. die Szene s1 wird üblicherweise zum Zeitpunkt t1 spielen usw. Jeder Szene sind &lt;strong&gt;Dialogakte &lt;/strong&gt;untergeordnet, denen der zu dieser Szene gehörige Text entspricht.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dialogakte&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dialogakte werden im Hinblick auf ihre Sprecher und Adressaten, ihren Inhalt sowie ihren Zeitpunkt beschrieben. Der &lt;strong&gt;Zeitpunkt &lt;/strong&gt;(time), zu dem der Dialogakt geäußert wird, wird anhand einer ID angegeben, welche eine Spezifizierung der ID des Zeitpunkts der zugehörigen Szene darstellt. Spielt z. B. Szene s5 zum Zeitpunkt t5, so haben die zugehörigen Dialogakte die Zeitpunkte t5.1, t5.2 usw. Der &lt;strong&gt;Sprecher &lt;/strong&gt;(speaker) des Dialogakts wird über seine ID angegeben. Der &lt;strong&gt;Adressat&lt;/strong&gt; bzw. die &lt;strong&gt;Adressaten &lt;/strong&gt;(receiver) des Dialogakts werden über eine Liste von Charakter-IDs angegeben, z.B. ch2, ch4, ch6. Passagen des Erzählers stellen dabei einen Spezialfall dar: Sie werden als Dialogakte des Erzählers mit dem Zuhörer bzw. Leser betrachtet, d. h. der Dummy-Charakter des Erzählers wird als Sprecher angegeben und der Dummy-Charakters des Zuhörers als Empfänger. Abgesehen davon werden sie behandelt wie Dialogakte zwischen Charakteren. Jeder Dialogakt erhält eine spezifische ID, die – unabhängig von der Szenenstruktur – linear hochgezählt wird, also d1, d2 usw.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;strong&gt;XML-Repräsentation&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Die oben beschriebenen Informationen lassen sich im XML-Format darstellen. Dabei wird eine XML-Baumstruktur genutzt, um die Hierarchie der verschiedenen Objekte zu repräsentieren.  Das Wurzelelement des Dokuments hat stets den Bezeichner Tale und die Attribute „presentationTitle“ und „annotator“, welche Titel und den Namen des Annotators des jeweiligen Märchens enthalten:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1: &lt;/strong&gt;Struktur des Tale-Wurzelelements (Beispiel).&lt;/p&gt;
&lt;p&gt;&lt;Tale presentationTitle="Froschkönig" annotator="Lisa Schäfer"&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;/Tale&gt;&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;
&lt;p&gt;Diesem Element untergeordnet sind die Elemente Characters, Locations und Text. Das Characters-Element enthält Character-Subelemente, die jeweils die gesammelten Informationen für einen Charakter speichern:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2: &lt;/strong&gt;Struktur des Characters-Elements (Beispiel).&lt;/p&gt;
&lt;p&gt;&lt;Characters&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;Character id="ch1" name="Frosch" age="adult" gender="male"&lt;/p&gt;
&lt;p&gt;type="animal_monster" subtype="small" attitude="neutral"&lt;/p&gt;
&lt;p&gt;archetype="hero"&gt; &lt;/Character&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;/Characters&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Analog dazu enthält das Locations-Element untergeordnete Location-Elemente, die&lt;/p&gt;
&lt;p&gt;jeweils einen Ort codieren:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3: &lt;/strong&gt;Struktur des Locations-Elements (Beispiel).&lt;/p&gt;
&lt;p&gt;&lt;Locations&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;Location id="loc1" type="WALD"&gt; &lt;/Location&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;/Locations&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Das Text-Element enthält schließlich den eigentlichen Märchentext. Dieser ist auf die verschiedenen Szenen – repräsentiert durch Scene-Elemente – aufgeteilt, welche wiederum die verschiedenen Dialogakte (Dialogue-Elemente) enthalten:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4: &lt;/strong&gt;Struktur des Text- und Scene-Elemente (Beispiel).&lt;/p&gt;
&lt;p&gt;&lt;Text&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;Scene id="s2" time="t2" location="loc1" characters="ch1,ch2"&lt;/p&gt;
&lt;p&gt;propp_functions="d|e" propp_subfunctions="D7|E10"&lt;/p&gt;
&lt;p&gt;transition="gehen"&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;Dialogue id="d5" time="t2.4" speaker="ch2" receiver="ch1"&gt; Ach,&lt;/p&gt;
&lt;p&gt;du bist’s, alter Wasserpatscher, &lt;/Dialogue&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;/Scene&gt;&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;&lt;/Text&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Beim Entwurf des XML-Schemas wurde besonders Wert auf Übersichtlichkeit und Leserlichkeit gelegt. Trotz der Vielzahl der codierten Informationen sind die resultierenden XML-Dateien daher vergleichsweise kompakt; so besteht die XML-Repräsentation des (vergleichsweise langen) Märchens &lt;em&gt;Hänsel und Gretel &lt;/em&gt;bspw. nur aus 226 Zeilen.&lt;/p&gt;
&lt;p&gt;Diese XML Repräsentation basiert auf und erweitert das Annotation Schema, das in (Scheidel &amp; Declerck, 2010)  beschrieben wird.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;Python-Repräsentation&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Auf Grundlage der oben beschriebenen XML-Struktur kann eine Python-Klassenstruktur aufgebaut werden, die ein Märchen sowie seine einzelnen Teile als Python-Objekte repräsentiert.&lt;/p&gt;
&lt;p&gt;Neben einer Oberklasse Tale gibt es für jeden der oben beschriebenen Teile eine eigene Python-Klasse, d. h. die Klassen Location, Character, Scene und Dialogue. (Insgesamt bestehen die Dateien zur Märchen-Repräsentation aus 288 Zeilen Code.) Jede Klasse enthält dabei als Attribute die oben beschriebenen Eigenschaften, wobei diese auch Verweise auf andere Elemente darstellen können. So verweisen bspw. Dialogue-Objekte auf die Character-Objekte von Sprecher und Empfängern. Der Python-Code dient als Interface für drei Anwendungen. Erstens können Märchen aus bestehenden XML-Dateien eingelesen werden; zweitens können XML-Dateien anhand einer anderweitig (z. B. durch automatische Klassifizierung) erzeugten Python-Märchenstruktur generiert werden; und drittens kann anderer Python-Code auf die Märchen-Information zugreifen, was die Grundlage für Anwendungen wie Text-to-Speech oder Visualisierung bildet. Sowohl die XML Kodierung als auch die Python Objekte interagieren  mit einer Märchen-Ontologie interagieren, die eine Erweiterung der in  (Koleva et al., 2012) beschriebenen Ontologie ist.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Somit haben wir eine formale Repräsentation von Märchen, die in verschiedenen Anwendungen zum Tragen kommen kann.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Literatur&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Propp, Vladimir ; Scott, Laurence (Hrsg.): &lt;em&gt;Morphology of the folktale&lt;/em&gt;. 2.&lt;/p&gt;
&lt;p&gt;überarbeitete Auflage. Austin, TX u.a., 1977&lt;/p&gt;
&lt;p&gt;Antonia Scheidel and Thierry Declerck. 2010. Apftml - augmented proppian fairy tale markup language. In Sándor Dar´anyi and Piroska Lendvai, editors, First International AMICUS Workshop on Automated Motif Discovery in Cultural Heritage and Scientific Communication Texts. Szeged University.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Nikolina Koleva, Thierry Declerck, and Hans-Ulrich Krieger. 2012. An ontology-based iterative text processing strategy for detecting and recognizing characters in folktales. In Jan Christoph Meister, editor, Digital Humanities 2012 Conference Abstracts, pages 467–470, Hamburg, 7. University of Hamburg, Hamburg University Press&lt;/p&gt;
</p67_abstract>
 </session>

 <session>
  <session_ID>86</session_ID>
  <session_title>Poster-Preisverleihung mit Empfang und Buffet</session_title>
  <session_start>2018-03-01 19:45</session_start>
  <session_end>2018-03-01 23:59</session_end>
  <session_room_ID>20</session_room_ID>
  <session_room>Hörsaalgebäude</session_room>
  <attendee_count>8</attendee_count>
 </session>

 <session>
  <session_ID>106</session_ID>
  <session_title>Öffnungszeiten Konferenzsekretariat</session_title>
  <session_start>2018-03-02 08:30</session_start>
  <session_end>2018-03-02 14:00</session_end>
  <session_room_ID>5</session_room_ID>
  <session_room>Hörsaal F, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>154</session_ID>
  <session_short>VP_8a</session_short>
  <session_title>Theorie der Digitalen Geisteswissenschaften III</session_title>
  <session_start>2018-03-02 09:00</session_start>
  <session_end>2018-03-02 10:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Hodel, Tobias</chair1>
  <attendee_count>3</attendee_count>
  <chair1_name>Tobias Hodel</chair1_name>
  <chair1_organisation>Staatsarchiv des Kantons Zürich</chair1_organisation>
  <chair1_email>tobias.hodel@hist.uzh.ch</chair1_email>
  <chair1_email2>tobiashodel@gmail.com</chair1_email2>
  <chair1_ID>1146</chair1_ID>
  <sessionID>154</sessionID>
  <presentations>3</presentations>
  <p1_paperID>244</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Althof, Daniel</p1_authors>
  <p1_organisations>BBAW, Deutschland</p1_organisations>
  <p1_emails>daniel.althof@bbaw.de</p1_emails>
  <p1_presenting_author>Althof, Daniel</p1_presenting_author>
  <p1_title> Neue Wahlverwandtschaften</p1_title>
  <p1_abstract>&lt;p&gt;Eine natürliche Verwandtschaft besteht zwischen der Geisteswissenschaft und dem Text sowie zwischen der Naturwissenschaft und der Zahl. Hermeneutik ist somit in den Geisteswissenschaften, Quantifizierung in den Naturwissenschaften die naheliegende Heuristik. Computation[1] und Naturwissenschaften geben ein starkes Team. Dagegen haben Geisteswissenschaften gegenüber dem Einsatz von Computern starke Vorbehalte. Die Gründe sind vielfach erörtert worden und sind in jedem Falle ernst zu nehmen. Die breite Diskussion um die Angemessenheit der neuen Methoden schlägt sich beispielhaft im Thema ‚distant reading‘ (Moretti 2005) nieder.[2] Generell wird dabei die Reichweite der Quantifizierung und Gefahren wie Verdinglichung problematisiert, die in der zugrundeliegenden Ontologie der Comuterisierung unvermeidlich angelegt sind (Berry 2011). Reduktionismus und Simplifizierung (in der Auslegung) literarischer Texte bilden einen Aspekt, der in den Geisteswissenschaften den Grundkonflikt (aus dem Themenkomplex KI) zwischen Mensch und Maschine wiederspiegelt.[3] Neben der Kritik am Umgang mit dem Gegenstand gibt es auch Kritik am Erkenntniswert der aufwändig gewonnenen Daten (Kirsch 2012). Ein weiterer, die Disziplin selbst betreffender Aspekt ist die fehlende Selbstreflexion durch alleinige Konzentration auf die Entwicklung von Tools. Sowohl die Methodenreflexion (Liu 2008) als auch das kritische Potential der Digital Humanities (DH) (Liu 2012b) werden nicht ausgeschöpft. Die DH, so die Linie der Kritik, zeigen sich weder dem speziellen Gegenstand der Geisteswissenschaften, nämlich (hauptsächlich) dem Text, noch den Kernaufgaben der Geisteswissenschaften, nämlich der Kritik auch der eigenen Methoden, nicht angemessen.&lt;/p&gt;
&lt;p&gt;Die Liste trennender Kräfte könnten detailliert ausgeweitet werden. Der Vortrag soll jedoch nicht noch einen Grund mehr zu dieser Liste hinzufügen. Vielmehr soll eine Dimension der DH freigelegt werden, die es aus der Kernkompetenz der DH, nämlich aus der Computation heraus vermag, dem Gegenstand und den Kernaufgaben der Geisteswissenschaften gerecht zu werden. Es ist also nicht Ziel des Vortrages, die Kluft zwischen den Paradigmen (aus Geistes- und Naturwissenschaften) vermittels einer Rückbesinnung auf eine Kompetenz der Geisteswissenschaft (äußerlich) zu überbrücken (wie es Alan Liu (Liu 2012b) oder auch David Berry (Berry 2017) tun). Es ist die Ambition, eine Wesensverwandtschaft in den Heuristiken zu enthüllen, die sich aus dem Charakter der Digitalität selbst entwickeln lässt. Hermeneutik und Computation müssen auf diese Weise also nicht extern (über pragmatische Argumente) verklammert, sondern können vermittelt über Digitalität immanent ineinander überführt werden.&lt;/p&gt;
&lt;p&gt;Es entsteht eine neue Wahlverwandtschaft zwischen Hermeneutik und Computation, wie sie (als Grundidee über Verschiebungen in Verbindungen) in Goethes Wahlverwandtschaften zugrunde liegt: Natürliche Verbindungen zwischen Elementen werden aufgelöst, sobald ein Drittes hinzukommt und eine neue Verbindung entsteht, die allein so nicht zustande kommen könnte. „Hier ist eine Trennung, eine neue Zusammensetzung entstanden, und man glaubt sich nunmehr berechtigt, sogar das Wort Wahlverwandtschaft anzuwenden, weil es wirklich aussieht, als wenn ein Verhältnis dem andern vorgezogen, eins vor dem andern erwählt würde.“ (Goethe 1809) Bei Goethe ist es der Gips, der entsteht, wenn Kalk und Schwefelsäure zusammenkommen. Hier sind es die DH, die entstehen, wenn Texte und Digitalisierung zusammengebracht werden. Der Vortrag soll diese Verbindung als stabile Brücke zwischen den eigentlich gegensätzlichen Paradigmen zeigen. Dabei lassen sich die Paradigmen freilich nicht als identisch zeigen. Jedoch lässt sich eine Brücke entdecken, die jeweils fest in den jeweiligen Paradigmen verankert ist. Das ist in Anbetracht der großen Opposition gegen Computation in den Geisteswissenschaften ein großer Fortschritt.&lt;/p&gt;
&lt;p&gt;Der erste Schritt, der hierfür nötig ist, besteht darin, einerseits das Objekt der Hermeneutik, den Text, andererseits das Mittel für Verarbeitung, den Code, in Grundzügen zu analysieren. Alan Liu und David Berry charakterisieren das Digitale mittels 10 Oppositionen, die ich aufgreifen möchte (Berry 2014a und Liu 2012a).&lt;/p&gt;
&lt;p&gt;1.       Linearität – Hypertextualität&lt;/p&gt;
&lt;p&gt;2.       Narrativ – Datenbank&lt;/p&gt;
&lt;p&gt;3.       Permanent – Flüchtig&lt;/p&gt;
&lt;p&gt;4.       Gebunden – Ungebunden&lt;/p&gt;
&lt;p&gt;5.       Individuell – Sozial&lt;/p&gt;
&lt;p&gt;6.       Tief – Flach&lt;/p&gt;
&lt;p&gt;7.       Fokussiert – Zerstreut&lt;/p&gt;
&lt;p&gt;8.       Close Reading – Distant Reading&lt;/p&gt;
&lt;p&gt;9.       Statisch – Prozessual&lt;/p&gt;
&lt;p&gt;10.   Digital – Real&lt;/p&gt;
&lt;p&gt;Diese Oppositionen beschreiben für Berry und Liu die Differenz zwischen dem Analogen und dem Digitalen. Es gilt hier nicht so sehr, auf jede einzelne Opposition einzugehen, als sie vielmehr zu ordnen und auf ihren Kern hin zu analysieren. Als erstes ist festzuhalten, dass die Oppositionen vieldeutig sind. Sie bilden einmal die Charakteristik des Gegenstandes der klassischen Hermeneutik auf der einen und die Charakteristik des Phänomens Digitalität auf der anderen Seite ab. Das teilt sich dabei so auf, dass der Hermeneutik die statisch-linearen Kategorien, der Digitalität die dynamisch-verzweigten Kategorien zugeordnet werden. Die Oppositionen bilden jedoch auch Hermeneutik und Digitalität genau mit entgegengesetzter Zuordnung ab, wenn man die Hermeneutik nicht auf ihren Gegenstand, sondern auf ihr Auslegungsgeschehen hin beschaut und unter Digitalität nicht das Oberflächenphänomen, d.h. das Sicht- und Nutzbare, versteht, sondern der darunterliegende Code gemeint ist. Dann nämlich fällt die Hermeneutik unter die dynamisch-verzweigten Kategorien, die Digitalität aber unter die statisch-linearen. Sodann lassen sich innerhalb der Oppositionen zwei Gruppen ausfindig machen. Die erste Gruppe bilden 1. Linearität – Hypertextualität, 2. Narrativ – Datenbank, 3. Permanent – Flüchtig, 4. Gebunden – Ungebunden, sowie 5. Statisch – Prozessual und 6. Digital – Real. Die zweite Gruppe setzt sich zusammen aus 1. Individuell – Sozial, 2. Tief – Flach, 3. Fokussiert – Zerstreut und 4. Close Reading – Distant Reading. Gruppe 1 bildet die immanente Logik der Sache ab – sei es auf Hermeneutik oder auf Code bezogen. Gruppe 2 umfasst die Charakteristik der Konsequenzen, die sich aus den Kategorien der Gruppe 1 ergeben. Sie sind also nicht auf die Sache bezogen. Die zweite Gruppe ignorieren wir für unsere Überlegungen vorerst.&lt;/p&gt;
&lt;p&gt;Was sich allerdings durch die Ambiguität der Kategorien ergeben hat, ist in Anbetracht der allgegenwärtigen Grundspannung zwischen Hermeneutik und Computation sehr interessant. Denn die zwei Lager erweisen sich in vielerlei Hinsicht als wesensverwandt. Code als Grundlage der Computation und Text als Grundlage der Hermeneutik sind je linear, folgen einem Narrativ, sind permanent, (an Abhängigkeiten) gebunden und statisch. Oft sind dies auch die Eigenschaften, aufgrund derer Computation als statisch-lineare für das vielschichtige Auslegungsgeschehen ungeeignet sei. Wie sich jedoch die Hermeneutik nicht im Gegenstand der Auslegung erschöpft, so ist die Digitalität auch nicht auf den Code zu beschränken. Hermeneutik und Digitalität sind dynamisch-verzweigte Prozesse – Code nicht zuletzt dadurch, dass dieser ‚läuft‘ (Berry 2011: 38) und eine vielschichtige Wirklichkeit erzeugt, die strukturell ähnlich ist zum Auslegugungsgeschehen der Hermeneutik. Sie sind Intertextuell, relational (d.h. nehmen Bezug auf andere Informationen, Daten, Texte), flüchtig im Sinne eines Prozesses, dessen Prozessualität entscheidend ist, ungebunden (d.h. nicht gebunden an endgültige Regeln, sondern der Veränderung gegenüber offen) und prozessual. Diese kreuzweise Überschneidung und ‚Deckungsgleichheit‘ von Hermeneutik und Digitalität lässt die Brücke sichtbar werden. Scheint also Computation bzw. Code ungeeignet für geisteswissenschaftliches Arbeiten, erfüllt zunächst rein äußerlich betrachtet Digitalität diese Bedingungen sehr wohl. Dabei ist zu beachten, dass die Digitalität ja selbst auf Code basiert. Code selbst ist in seiner Struktur eindimensional, in seiner Ausführung jedoch mehrdimensional. Die Digitalität selbst ist eine Prozessualisierung und Vervielschichtigung der Lebenswelt, die auf einer statischen und eindimensionalen Architektur (Code) aufsetzt. Die Digitalisierung der Lebenswelt ist damit eine Dynamisierung, die sowohl alles durchdringt (vgl. Berry 2014b), als auch den hermeneutischen Prozess prinzipiell tragen kann. Damit ergeben sich nicht nur strukturelle Überschneidungen mit dem Geschehen der Hermeneutik, die durch Applikationen, Visualisierungen, Verlinkungen etc. unterstützt werden kann. Es ergeben sich auch Überschneidungen mit dem Gegenstand der Hermeneutik, indem die Digitalisierung den Gegenstand selbst aus der (dem Auslegungsgeschehen ungünstigen) Statik und Linearität transformiert in die Multidimensionalität und immanenten Prozessualität (Berry 2011: 9f), auf die das Auslegungsgeschehen aufsetzt.&lt;/p&gt;
&lt;p&gt;Diese Brücke zwischen Digitalität und Hermeneutik freizulegen, en detail zu beleuchten und ihr durch Überlegungen zum Wesen von Code auf den Grund zu gehen, um diese Wahlverwandtschaft als immanente auszuweisen, sowie Konsequenzen für die Verortung der DH als Disziplin zu bestimmen, ist die Absicht meines Vortrages.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bibliographie &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Berry, David M. (2011): Philosophy of Software. Code and Mediation in the Digital Age, Palgrave Macmillan UK.&lt;/p&gt;
&lt;p&gt;Berry, David M. (2014a): Critical Theory and the Digital. Critical Theory and Contemporary Society, A&amp;C Black.&lt;/p&gt;
&lt;p&gt;Berry, David M. (2014b): Post-digital humanities: computation and cultural critique in the arts and humanities. Educause, 49 (3): 22-26.&lt;/p&gt;
&lt;p&gt;David M. Berry, Anders Fagerjord (2017): Digital Humanities: Knowledge and Critique in a Digital Age, John Wiley &amp; Sons.&lt;/p&gt;
&lt;p&gt;Goethe, Johann Wolfgang (1809): Die Wahlverwandtschaften, http://www.zeno.org/Literatur/M/Goethe,+Johann+Wolfgang/Romane/Die+Wahlverwandtschaften/Erster+Teil/Viertes+Kapitel, Zugriff 22. September 2017.&lt;/p&gt;
&lt;p&gt;Kirsch, Adam (2012): Technology Is Taking Over English Departments. The false promise of the digital humanities, https://newrepublic.com/article/117428/limits-digital-humanities-adam-kirsch, Zugriff 22. September 2017.&lt;/p&gt;
&lt;p&gt;Liu, Alan (2008): Local Transcendence: Essays on Postmodern Historicism and the Database, University of Chicago Press.&lt;/p&gt;
&lt;p&gt;Liu, Alan (2012a): The State of the Digital Humanities: A Report and a Critique. In: Arts and Humanities in Higher Education, 11.1-2 (2012): 8-41.&lt;/p&gt;
&lt;p&gt;Liu, Alan (2012b): Where is Cultural Criticism in the Digital Humanities?, In: Debates in the Digital Humanities, ed. Matthew K. Gold, University of Minnesota Press: 490-509.&lt;/p&gt;
&lt;p&gt;Marche, Stephen (2012): Literature Is not Data: Against Digital Humanities, https://lareviewofbooks.org/article/literature-is-not-data-against-digital-humanities, Zugriff 22. September 2017.&lt;/p&gt;
&lt;p&gt;Moretti, Franco (2005): Graphs, Maps, Trees: Abstract Models for a Literary History, Verso.&lt;/p&gt;
&lt;p&gt;Searle, John (1993): Die Wiederentdeckung des Geistes, Artemis &amp; Winkler.&lt;/p&gt;
&lt;br clear="all" /&gt;
&lt;p&gt;[1] Ich werde diesen terminus technicus hier verwenden, um damit die computerseitigen Prozesse der Digitalisierung abzubilden, die eine sozio-kulturelle Umwälzung ist. Datenverarbeitung als Alternativ-Begriff wäre zu eng gefasst.&lt;/p&gt;

&lt;p&gt;[2]Gewichtige Argumente gegen die Sinnhaftigkeit eines solchen Vorhabens versammelt Marche 2012.&lt;/p&gt;

&lt;p&gt;[3] Vgl. z.B. Searle 1993 und die Debatte um Semantik und Syntax.&lt;/p&gt;

</p1_abstract>
  <p2_paperID>255</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Arnold, Eckhart</p2_authors>
  <p2_organisations>Bayerische Akademie der Wissenschaften, Deutschland</p2_organisations>
  <p2_emails>arnold@badw.de</p2_emails>
  <p2_presenting_author>Arnold, Eckhart</p2_presenting_author>
  <p2_title>Positivistischer Methodenfetischismus als Anathema der digitalen Geisteswissenschaften</p2_title>
  <p2_abstract>&lt;p&gt;Positivistischer Methodenfetischismus als Anathema der digitalen Geisteswissenschaften&lt;br /&gt;======================================================================================&lt;br /&gt;Entsprechend dem Thema der diesjährigen DhD-Konferenz "Kritik der&lt;br /&gt;digitalen Vernunft" möchte ich mich in meinem Vortrag der Kritik&lt;br /&gt;fehlgeleiteter digitaler Geisteswissenschaften widmen, und dazu&lt;br /&gt;erstens anhand von Beispielen zeigen, wie sich fehlgeleiteter&lt;br /&gt;Technikeinsatz in den Geistes- und Sozialwissenschaften darstellt und&lt;br /&gt;zweitens Kriterien dafür vorschlagen, nach denen beurteilt werden&lt;br /&gt;kann, wann der der Technikeinsatz in den Geistes- und&lt;br /&gt;Sozialwissenschaften sinnvoll und wann fehlgeleitet ist. Das dient - so&lt;br /&gt;hoffe ich - der besseren Aufklärung unseres digital untersützten&lt;br /&gt;Erkenntnisvermögens über sich selbst.&lt;br /&gt;Es steht außer Frage, dass Computer nützliche Werkzeuge sind, und dass&lt;br /&gt;man digitale Methoden in den Geisteswissenschaften auf vielfache Weise&lt;br /&gt;gewinnbringend einsetzen kann. Dabei wäre es aber eine&lt;br /&gt;Fehleinschätzung zu glauben, dass die Digitaltechnik für die&lt;br /&gt;Geisteswissenschaften lediglich ein Hilfsmittel darstellt, die uns&lt;br /&gt;erlaubt, bestimmte Forschugsaufgaben in den Geisteswissenschaften&lt;br /&gt;schneller und besser zu erledigen, die Ergebnisse leichter zu&lt;br /&gt;verbreiten etc. Vielmehr ändert der massive Einsatz von Digitaltechnik&lt;br /&gt;auch den Charakter der Wissenschaften: Neue Themenfelder werden&lt;br /&gt;erschlossen, andere Fragestellungen als relevant empfunden, andere&lt;br /&gt;Verfahrensweisen als mustergültig oder auch nicht (mehr) akzeptabel&lt;br /&gt;empfunden und andere Fertigkeiten und Kenntnisse von den&lt;br /&gt;Wissenschaftlerinnen und Wissenschaftlern erwartet. Diese&lt;br /&gt;Veränderungen können durchaus an den Identitätskern dessen rühren, was&lt;br /&gt;ein geistes- oder gesellschaftswissenschaftliches Fach bisher&lt;br /&gt;ausgemacht hat. Das allein bedeutet noch nicht, dass diese&lt;br /&gt;Veränderungen schlecht sind, denn dass wissenschaftliche Fächer und&lt;br /&gt;ihre Untersuchungsgegenstände einem historischem Wandel unterliegen&lt;br /&gt;ist unvermeidlich. Es wirft aber die Frage auf, wann der Einsatz&lt;br /&gt;bestimmter technischer Mittel und die dadurch herbeigeführten&lt;br /&gt;Veränderungen legitim und zum Nutzen der Wissenschaft sind, und wann&lt;br /&gt;zum Schaden der Wissenschaft, so dass man besser Abstand davon nehmen&lt;br /&gt;sollte. Auf einer höheren Ebene schließt sich daran die weitergehende&lt;br /&gt;Frage an, nach welchen Kriterien dies beurteilt werden kann.&lt;br /&gt;Eine offensichtlich unzureichende Antwort ist die, dass der Einsatz&lt;br /&gt;technischer Mittel dann legitim ist, wenn er es erlaubt, irgendwelche&lt;br /&gt;(bestehenden) wissenschaftlichen Probleme besser zu lösen, denn diese&lt;br /&gt;Antwort lässt die Frage unbeantwortet, wie technische Mittel zu&lt;br /&gt;beurteilen sind, die vor allem neue Problemfelder erschließen, nicht alte&lt;br /&gt;Probleme besser lösen. &lt;br /&gt;Unzureichend wäre es aber auch, den Einsatz technischer Mittel in den&lt;br /&gt;Geisteswissenschaften auf jeden Fall gut zu heißen, wenn dadurch&lt;br /&gt;bekannte Probleme besser gelöst werden oder neue Problemfelder&lt;br /&gt;erschlossen werden. Diese - nur oberflächlich plausible - Antwort&lt;br /&gt;ignoriert nämlich, dass die Problemauswahl wertgesteuert erfolgt, und&lt;br /&gt;sie verkennt zudem, wie wissenschaftliche Verdrängungsprozesse&lt;br /&gt;realiter ablaufen. Sie öffnet zudem einem positivistischen&lt;br /&gt;Methodenfetischismus Tür und Tor, der - grob gesprochen - darin&lt;br /&gt;besteht, dass die Methode zum Kriterium der Relevanz erhoben wird und&lt;br /&gt;die Phänomene nicht mehr (oder nur noch durch den Blickwinkel einer&lt;br /&gt;bestimmten Methode) zur Kenntnis genommen werden.&lt;br /&gt;Am Beispiel der analytischen Philosophie, (und konkret dem Einsatz&lt;br /&gt;spieltheoretischer Modelle in der Analystischen Philosophie), möchte&lt;br /&gt;ich vor Augen führen, wie Verdrängungsprozesse zwischen&lt;br /&gt;wissenschaftlichen Schulen ablaufen. Solche strategischen&lt;br /&gt;Verdrängsprozesse, die man in Abgrenzung zum natürlichen&lt;br /&gt;Paradigmenwechsel als "imperialistisch" bezeichnen kann, leisten&lt;br /&gt;natürlich nur besonders technische Ansätze, aber mit technischen&lt;br /&gt;Ansätzen funktioniert das aus verschiedenen Gründen besonders gut. Die&lt;br /&gt;typischen Etappen sind diejenigen:&lt;br /&gt;1. Zunächst wird eine neue wissenschaftliche Methode propagiert, in&lt;br /&gt;diesem Fall die Spieltheorie, mit der man angeblich&lt;br /&gt;sozialphilosophische Probleme wissenschaftliche viel genauer (nämlich mit&lt;br /&gt;mathematischer Präzision) behandeln kann als das zuvor der Fall war.&lt;br /&gt;2. Anfangs funktioniert das noch nicht besonders gut. Aber jede&lt;br /&gt;wissenschaftliche Methode muss sich ja erst einmal die Chance&lt;br /&gt;bekommen, sich zu entwickeln. Speziell bei technischen Ansätzen&lt;br /&gt;erweist sich der anscheinend unbezwingbare Mythos des Vorsprungs durch&lt;br /&gt;Technik als sehr hilfreich für "die Sache". Zudem tun sich Kritiker&lt;br /&gt;aus dem Fach schwer einen Ansatz anzugreifen, dessen hochtechnische&lt;br /&gt;Einzelheiten sie nicht begreifen, selbst wenn sie die Dürftigkeit der&lt;br /&gt;Ergebnisse nicht übersehen können.&lt;br /&gt;3. Der entscheidende Durchsetzungstrick besteht nun darin, dass -&lt;br /&gt;anders als die oben beschriebene zweite Antwort stillschweigend&lt;br /&gt;unterstellt - ein fairer Vergleich mit den früheren Ansätzen&lt;br /&gt;hinsichtlich der vermeintlich überlegenen Problemlösungskapazität gar&lt;br /&gt;nicht mehr stattfindet. Hat die neue wissenschaftliche Schule erst&lt;br /&gt;einmal eine kritische Masse erreicht (d.h. sind es genug Leute&lt;br /&gt;geworden, um eigene Fachzeitschriften herauszugeben, die Gutachten&lt;br /&gt;dazu beizusteuern, und schließlich auch die Besetzung von Professuren&lt;br /&gt;entscheidend zu beeinflussen), dann braucht sie sich dem Wettbewerb&lt;br /&gt;mit dem Verlierer entweder gar nicht mehr zu stellen, oder sie kann&lt;br /&gt;die Regeln bestimmten, nach denen er geführt wird. (Im Fall der&lt;br /&gt;analytischen Philosophie wird dazu die Verlierposition gerne&lt;br /&gt;für unwissenschaftlich, sprachlich unklar oder frei von Argumenten&lt;br /&gt;eklärt, was viel bequemer ist als die Argumente zu kritisieren.)&lt;br /&gt;4. Der wesentliche Unterschied zwischen einem legitimen&lt;br /&gt;Paraidgmenwechsel - wie z.B. von Kuhn beschrieben - und einem&lt;br /&gt;imperialistischen Durchsetzungsprozess besteht darin, dass im&lt;br /&gt;letzteren Fall die Vergrößerung der Problemlösungskapazität (ein&lt;br /&gt;wesentliches Merkmal des Kuhn'schen "Paradigmenwechsels", der&lt;br /&gt;ansonsten auch teilweise irrational verläuft) nur scheinbar&lt;br /&gt;stattgefunden hat. Der Prozess ist dann vollendet, wenn eine neue&lt;br /&gt;Generation von Wissenschaftlern erzogen worden ist, die mit dem&lt;br /&gt;erfolgreich verdrängten Ansatz gar nicht mehr in Berührung gekommen&lt;br /&gt;ist und daher den relativ defizitären Charakter der Verdränger-Schule&lt;br /&gt;gar nicht bemerken können.&lt;br /&gt;5. Speziell im Fall der Spieltheorie in der Sozialphilosophie ist zu&lt;br /&gt;beobachten:&lt;br /&gt;   a) dass der Kontakt zur Empirie, insbesondere der Feldforschungs&lt;br /&gt;      verloren gegangen (oder nie recht hergestellt worden)&lt;br /&gt;      ist. M.a.W.: Die Phänomene werden nicht mehr zur Kenntnis&lt;br /&gt;      genommen (das zweite Hauptmerkmal des positivistischen&lt;br /&gt;      Methodenfetischismus).&lt;br /&gt;      &lt;br /&gt;   b) dass die Ergebnisse purer Modellstudien, insbesondere von sehr&lt;br /&gt;      leicht anzufertigenden Computersimulationen ohne empirischen&lt;br /&gt;      Bezug als publikationswürdig angesehen werden. M.a.W.: Die&lt;br /&gt;      Methode wird zum Kriterium der Relevanz gemacht (das erste&lt;br /&gt;      Hauptmerkmal des positivitischen Methodenfetischismus).&lt;br /&gt;Der Vorgang gleicht einer Gehirnamputation, gegen die sich der Patient&lt;br /&gt;zunächst vielleicht sträubt. Ist sie aber erst einmal vollzogen, so&lt;br /&gt;ist er gerade auf Grund dieser Operation gar nicht mehr in der Lage zu&lt;br /&gt;registrieren, dass jemals etwas in der Art überhaupt vorgefallen ist.&lt;br /&gt;(An dieser Stelle kann man natürlich fragen, wie man denn dann solche&lt;br /&gt;Verdrängungsprozesse überhaupt als solche identifizieren und&lt;br /&gt;kritisieren kann, wenn es doch die Fachleute selbst am Ende nicht mehr&lt;br /&gt;können? Man kann, wenn man zu der Mühe bereit ist, die historische&lt;br /&gt;ältere verdrängte mit der historisch neueren verdrängenden Position&lt;br /&gt;unvoreingenommen zu vergleichen. Das setzt allerdings historische&lt;br /&gt;Bildung oder zumindest die Befähigung dazu voraus.)&lt;br /&gt;&lt;br /&gt;Inwiefern betrifft dies nun die digitalen Geisteswissenschaften?&lt;br /&gt;Bestimmte Bereiche der digitalen Geisteswissenschaften - vornehmlich&lt;br /&gt;solche, wo der Computer tatsächlich vor allem Hilfsmittel ist, wie&lt;br /&gt;z.B. beim elektronischen Publizieren oder beim transkribieren und&lt;br /&gt;editieren - bleiben davon tatsächlich unbetroffen. Aber es gibt andere&lt;br /&gt;Bereiche, wo der Technik-Einsatz in den digitalen&lt;br /&gt;Geisteswissenschaften viel tiefer in den Prozess des Verstehens- und&lt;br /&gt;Erklärens eingreift. Sofern man die auf die Geisteswissenschaften&lt;br /&gt;angewandte Spieltheorie nicht als digitale Geisteswissenschaften&lt;br /&gt;gelten lassen will (warum wird sie eigentlich nicht dazu gezählt?),&lt;br /&gt;dann könnte man hier das Distant-Reading oder auch die&lt;br /&gt;Netzwerkanalysen in der Literaturwissenschaft anführen (was nicht&lt;br /&gt;heisst, dass sie (schon?) denselben Mist gebaut haben, wie die&lt;br /&gt;analytische Philosophie mit der Spieltheorie).&lt;br /&gt;Wenn man bereit ist zuzugestehen, dass der Technik-Einsatz in der&lt;br /&gt;beschriebenen Weise schief gehen kann - ähnlich wie eine Revolution ja&lt;br /&gt;auch auf die Weise scheitern kann, dass sie zwar die Herrscher&lt;br /&gt;erfolgreich beseitigt, dann aber noch schlimmere an ihre Stelle setzt&lt;br /&gt;- dann bleibt immer noch die Frage anhand welcher Kriterien man den&lt;br /&gt;legitimen und sinnvollen Technikeinsatz von einem fehlgeleiteten&lt;br /&gt;unterscheiden kann. Die Frage ist deshalb nicht pauschal zu&lt;br /&gt;beantworten, weil ihre Antwort, von dem Fachgebiet, der konkreten&lt;br /&gt;Technologie und - wie oben angedeutet - auch von Relevanzfragen&lt;br /&gt;(d.h. Fragen derart, was ein legitimes und wichtiges Problem in einem&lt;br /&gt;Wissenschaftsfach ist) abhängt, die immer auch Wertfragen und so&lt;br /&gt;gesehen vielleicht gar nicht objektiv zu entscheiden sind. Dennoch&lt;br /&gt;kann man womöglich Faustregeln dafür aufstellen:&lt;br /&gt;1. Aus welchen Gründen auch immer eine bestimmte wissenschaftliche&lt;br /&gt;Problemstellung als relevant erachtet wird, es sollte nicht bloß aus&lt;br /&gt;dem Grund sein, dass sie die Verwendung einer interessanten&lt;br /&gt;Technologie erlaubt. (Klingt trivial, ist es in der Praxis aber nicht&lt;br /&gt;immer, besonders dann nicht, wenn der Technikeinsatz mit&lt;br /&gt;Forschungsgeldern prämiert wird. Und es kann vielleicht sogar&lt;br /&gt;Ausnahmen geben, z.B. dann, wenn ein Forschungsprojekte rein der&lt;br /&gt;Methodenentwicklung dient.)&lt;br /&gt;2. Technik ist in den Geisteswissenschaften nicht das&lt;br /&gt;Wesentliche. Wenn sie die Arbeit nicht einfacher macht oder ihr&lt;br /&gt;Einsatz nicht zu einem Surplus an Erkenntnis führt, der den Aufwand&lt;br /&gt;rechtfertigt, dann ist sie Fehl am Platze. (Auch trivial, aber nicht&lt;br /&gt;immer wird die Rechnung aufgemacht.)&lt;br /&gt;&lt;/p&gt;
</p2_abstract>
  <p3_paperID>270</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Garcés, Juan
Bräuer, Johannes</p3_authors>
  <p3_organisations>Sächsische Landesbibliothek – Staats- und Universitätsbibliothek Dresden (SLUB), Deutschland
Technische Universität Dresden, Institut für Philosophie</p3_organisations>
  <p3_emails>juan.garces@slub-dresden.de
johannes.braeuer@tu-dresden.de</p3_emails>
  <p3_presenting_author>Garcés, Juan</p3_presenting_author>
  <p3_title>Kritik der Digitalität (am Beispiel der digitalen Textwissenschaft)</p3_title>
  <p3_abstract>&lt;p&gt;In diesem Vortrag werde ich argumentieren, dass die im Rahmen der digitalen Wende in den textorientierten digitalen Geisteswissenschaften eingefundene Digitalität ontologisch eher eine Evolution, epistemologisch aber das Potential einer Revolution in sich birgt. Ich werde mich mit der derzeitigen Forschung bezüglich der ontologischen und epistemologischen Konsequenzen der Digitalität – vor allem, aber nicht ausschließlich mit Galloway und Berry – auseinandersetzen. Letztendlich werde ich für eine erneute Priorität des Erkenntnisgewinns – gegenüber der grundlegenden digitalen Daten und Analysealgorithmen – plädieren.&lt;/p&gt;
&lt;p&gt;Die Frage, mit der sich dieser Vortrag kritisch auseinandersetzt, lässt sich am besten an dem für die Geisteswissenschaften immer noch zentralen Textbegriff veranschaulichen. Die Digitalisierung von (historischen) Texten – im Sinne einer „gründlichen“ digitalen Erschließung – läutet, so etwa nach der Darstellung von Patrick Sahle, den Ausgang des Textes aus einem dem Zeitalter der Drucktechnologie verschuldeten „zentralisierenden“ Textbegriff ein. Der ins Digitale remedialisierte Text kann jetzt aus diesem Stadium heraus wieder vielfältig transformiert werden und dadurch „multiplen“ Textbegriffen gerecht werden. Die digitale Erschließung erzeugt somit nicht nur einen Text, der sich nun besser als „Ansammlungen von Informationen“ bezeichnen lässt, aber auch aufgrund der neuen medialen Form und der medientechnologischen Möglichkeiten nicht nur vielfältig darstellen sondern auch &lt;em&gt;anders&lt;/em&gt; analysieren lässt. Welche &lt;em&gt;wesentlichen&lt;/em&gt; Änderungen bringt Digitalität für das Verständnis historischer Texte mit sich? Und was bedeutet dies für die Geisteswissenschaften, die in ihrer jetzigen Gestalt immer noch Kinder des Druckzeitalters sind?&lt;/p&gt;
&lt;p&gt;Unzweifelhaft bringt die computerbetriebene Verarbeitung digital erschlossener Texte einen beachtlichen quantitativen Fortschritt mit sich: aus größeren handhabbaren Datenmengen lässt sich nun Wissen extrahieren und analysieren, das dieser Masse an Informationen gerecht werden kann. Neben den für das Druckzeitalter typischen akribischen Lektüren gesellen sich jetzt „Lektüren aus der Distanz“ (Moretti; vgl. Jockers‘ Makroanalyse), die zuvor zwar denkbar aber praktisch nur schwer oder völlig undurchführbar zu sein schienen.  Der Ausdruck ruft das Bild einer Neuorientierung in Bezug auf den wissenschaftlichen Gegenstand hervor. Die neugefundene Distanz zu den Gegenständen ermöglicht „eine spezifische Form der Erkenntnis“ (Moretti), die im Falle Morettis einem Schritt der visualisierten Abstraktion (Kurven, Karten und Stammbäume) und einer anschließend darauf basierenden Analyse entspringt. Grundlegend scheint hierbei der geisteswissenschaftliche Zugang zum Wissensgegenstand über ein abstrahiertes digitales Objekt, das als Surrogat für den Gegenstand (beim Text, beispielsweise, das „Werk“, jetzt zunehmend die „Werke“) selbst steht. Diese Abstraktion kommt aber nicht ohne ihre epistemologische Konsequenzen, die von Galloway folgendermaßen beschrieben werden:&lt;/p&gt;
&lt;p&gt;„[I]n order to be in a relation with the world informatically, one must erase the world, subjecting it to various forms of manipulation, preemption, modeling, and synthetic transformation. The computer takes our own superlative power over worlds as the condition of possibility for the creation of worlds. Our intense investment in worlds – our acute fact finding, our scanning and data mining, our spidering and extracting – is the precondition for how worlds are revealed. The promise is not one of revealing something as it is, but in simulating a thing so effectively that “what it is” becomes less and less necessary to speak about, not because it is gone for good, but because we have perfected a language for it.“ (Interface Effect, 13)&lt;/p&gt;
&lt;p&gt;Doch wie grundlegend innovativ für die textbasierten Geisteswissenschaften ist dieses Modell? Texte wurden im prädigitalen Zeitalter dann „erfolgreich“ ediert, wenn die Abstraktionsvorgänge des Editionsprozesses bei der wissenschaftlichen Interpretation größtenteils in Vergessenheit geraten konnten. In vielen literaturwissenschaftlichen Lektüren, beispielsweise, fängt die wissenschaftliche Lektüre erst dann an, &lt;em&gt;nachdem&lt;/em&gt; der Editionsprozess die grundlegende Voraussetzung geschaffen hatte. Die epistemologischen Prozesse, die zu dieser Voraussetzung geführt hatten, wurden zwar – besonders im deutschsprachigen Raum – oft und gründlich durch die Editionstheorie und Textkritik ans Licht gebracht. Dieser Bereich der kritischen Auseinandersetzung mit der Editorik blieb aber auf eine relativ spezialisierte Gruppe beschränkt. Die wissenschaftliche Edition, die sich als Standardwerk durchsetzte, wurde &lt;em&gt;de facto&lt;/em&gt; das Surrogat für das Werk. Das neue Interesse an der Editorik im Rahmen des medientechnologischen Wandels sollte man vielmehr an den räumlich uneingeschränkten Möglichkeiten der Erschließungsebenen und dem Potential der maschinellen Verarbeitung sehen, die aus der neuen recodierten Erschließung von Texten ein Desiderat macht. In diesem Kontext stellen sich neue praktische und theoretische Fragen, deren Beantwortung den Prozess der Erstellung eines Surrogats – und somit die erschlossenen Daten – wieder in den Mittelpunkt stellt. In diesem Sinne stellt die grundlegende Geste der Abstraktion, aufgrund dessen man eine Interpretation des Sachverhaltes betreibt &lt;em&gt;kein wesentliches Novum&lt;/em&gt; der Digitalität an sich, sondern ein wesentlicher Schritt, der – abstrakt gesehen – für beide Epochen grundlegend ist.&lt;/p&gt;
&lt;p&gt;Weiterführend sind hier die Beobachtungen Galloways, der Digitalität nicht als neue ontologische Kondition sieht, sondern eher als ontologische Evolution, als Remedialisierung der Konditionen des Daseins. Konkret stellt Digitalität die Eigenschaften eines neuen Mediums dar, das vorwiegend durch die Einbindung in ein Netzwerk hervorgerufen wurde. Genealogisch ist diese netzwerkartige Verstrickung von Mensch und Maschine auf verschiedene Konzepte zurückzuführen, die sich um und nach dem Zweiten Weltkrieg, mit einer neuen Art der Informationsverarbeitung beschäftigte.&lt;/p&gt;
&lt;p&gt;Epistemologisch hingegen birgt Digitalität das Potential einer epistemologischen Revolution der textbasierten Geisteswissenschaften, die hier beispielsweise in ihrer Beziehung zu den Naturwissenschaften angedeutet werden kann. Textbasierte Geisteswissenschaften sind nämlich aufgrund ihres textbasierten Fokus auch vorwiegend Datenwissenschaften, die nur aufgrund einer unglücklichen und übertriebenen Abgrenzung gegenüber den Naturwissenschaften – etwa durch Dilthey – ihr methodisches Proprium im hermeneutischen Verstehen (in Abgrenzung zum Erklären) fanden. Implizit und oft verborgen, spielt die Empirie schon immer eine nicht unwichtige Rolle in den Geisteswissenschaften, die jetzt mithilfe der Digitalität zum kritischen Vorschein kommen kann. Umgekehrt kann die Wiederentdeckung der Empirie den Geisteswissenschaften erlauben, mit den Naturwissenschaften über ihre impliziten hermeneutischen Vorannahmen und Deutungsparadigmen in einen kritischen Austausch zu kommen.&lt;/p&gt;
&lt;p&gt;Abschliessen will ich diesen Gedankengang mit einem meines Erachtens außerordentlich wichtigen, aber in den Digital Humanities nicht genügend rezipierten Entwurf von Willard McCarty. Dieser hat in seiner Monographie &lt;em&gt;Humanities Computing&lt;/em&gt;, die heuristische Interaktion der Größen digital erschlossene/modellierte Daten, rechnerbasierte Analyse(algorithmen) und Erkenntnisfortschritt folgendermaßen zusammengefasst. Die Rolle des Computers ist es, den Forscher zur rigoros disziplinierten Anwendung der (empirischen) Versuch-und-Irrtum-Methode zu zwingen und dadurch heuristisch auf die Punkte der unausweichlichen Nichtübereinstimmung von digital erschlossenem Objekt und zu untersuchendem Sachverhalt hinzuweisen. Bei dieser Interaktion steht die Finalität des epistemologischen Fortschritts klar im Mittelpunkt. Die Nachnutzbarkeit und Langzeitarchivierung der erschlossenen Daten sowie die Implementation der Algorithmen verlieren dadurch nicht an Wichtigkeit. Letztendlich, gewinnt aber das Schaffen von Wissen an rechtmäßiger Priorität. Denn es sind die neuen – nicht nur die gewisseren – Erkenntnisse, die der Aufnahme der für eine spezifische geisteswissenschaftliche Disziplin relevanten Aspekte Digital Humanities und damit einer epistemischen Revolution den Weg bereiten.&lt;/p&gt;
&lt;p&gt;Bibliographie&lt;/p&gt;
&lt;p&gt;Berry, David M., &lt;em&gt;The Philosophy of Software: Code and Mediation in the Digital Age&lt;/em&gt;, Houndmills &amp; New York, 2011.&lt;/p&gt;
&lt;p&gt;-----, &lt;em&gt;Critical Theory and the Digital&lt;/em&gt;, New York &amp; London, 2014.&lt;/p&gt;
&lt;p&gt;Dilthey, Wilhelm, Einleitung in die Geisteswissenschaften: Versuch einer Grundlegung für das Studium der Gesellschaft und der Geschichte, Leipzig, 1883.&lt;/p&gt;
&lt;p&gt;Evens, Aden, &lt;em&gt;Logic of the Digital&lt;/em&gt;, London u.a., 2015.&lt;/p&gt;
&lt;p&gt;Galloway, Alexander R., &lt;em&gt;The Interface Effect&lt;/em&gt;, Cambridge &amp; Malden, 2012.&lt;/p&gt;
&lt;p&gt;-----, “The Cybernetic Hypothesis”, &lt;em&gt;differences&lt;/em&gt; 25:1 (2014) 107-131.&lt;/p&gt;
&lt;p&gt;Moretti, Franco, &lt;em&gt;Kurven, Karten, Bäume: Abstrakte Modelle für die Literaturgeschichte&lt;/em&gt;, übers. Florian Kessler, Frankfurt am Main, 2009.&lt;/p&gt;
&lt;p&gt;McCarty, Willard, &lt;em&gt;Humanities Computing&lt;/em&gt;, Houndmills &amp; New York, 2005.&lt;/p&gt;
&lt;p&gt;Ramsay, Stephen, &lt;em&gt;Reading Machines: Towards an Algorithmic Criticism&lt;/em&gt;, Urbana &amp; Springfield, 2011.&lt;/p&gt;
&lt;p&gt;Sahle, Patrick, &lt;em&gt;Digitale Editionsformen: Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels&lt;/em&gt;, Teil 3: &lt;em&gt;Textbegriffe und Recodierung&lt;/em&gt;, Norderstedt, 2013.&lt;/p&gt;
&lt;p&gt;Jockers, Matthew L., &lt;em&gt;Macroanalysis: Digital Methods &amp; Literary History&lt;/em&gt;, Urbana &amp; Springfield, 2013.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>155</session_ID>
  <session_short>VP_8b</session_short>
  <session_title>Sentimentanalyse</session_title>
  <session_start>2018-03-02 09:00</session_start>
  <session_end>2018-03-02 10:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Schöch, Christof</chair1>
  <attendee_count>5</attendee_count>
  <chair1_name>Christof Schöch</chair1_name>
  <chair1_organisation>Universität Trier</chair1_organisation>
  <chair1_email>schoech@uni-trier.de</chair1_email>
  <chair1_ID>1008</chair1_ID>
  <sessionID>155</sessionID>
  <presentations>3</presentations>
  <p1_paperID>157</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Mellmann, Katja
Du, Keli</p1_authors>
  <p1_organisations>Universität Göttingen, Deutschland
Universität Göttingen, Deutschland</p1_organisations>
  <p1_emails>katja.mellmann@phil.uni-goettingen.de
keli.du@stud-mail.uni-wuerzburg.de</p1_emails>
  <p1_presenting_author>Mellmann, Katja
Du, Keli</p1_presenting_author>
  <p1_title>Sentimentanalyse in unstrukturierten Texten (am Bsp. literaturgeschichtlicher Rezeptionsanalyse)</p1_title>
  <p1_abstract>&lt;p&gt;Wir wollen im Rahmen des Themas “Kritik der digitalen Vernunft” einen konstruktiven Umgang mit ‘schmutzigen Texten’ vorstellen. Wir unterscheiden dazu grundsätzlich zwei Zielperspektiven: Korpusanalyse als Untersuchung mit validen Ergebnissen und Korpusanalyse als Heuristik zur vorläufigen Trenddarstellung.&lt;/p&gt;
&lt;p&gt;Bei den angezielten Bewegungsprofilen handelt es sich im Rahmen unseres Forschungsprojekts vor allem um Zäsuren in der Bewertung literarischer Autoren. Wir untersuchen in einem Pilotprojekt die Rezeption von Literatur in literaturkritischen Zeitschriften des ausgehenden 19. Jahrhunderts mittels einer Sentimentanalyse der Textumgebung von Autorerwähnungen. An einem Testkorpus optimieren wir durch den Vergleich automatisierter mit manuellen Analysen eine an das historische Genre ‘Literaturkritik um 1900’ angepasste Sentimentwortliste.&lt;/p&gt;
&lt;p&gt;Der geplante Vortrag wird die Form eines Erfahrungsberichtes mit Zwischenergebnissen haben und sich insbesondere der Frage zuwenden, wie die Methode der Sentimentanalyse für literaturwissenschaftliche Fragestellungen sinnvoll eingesetzt werden kann an Texten, die erstens einem besonders komplizierten Genre gehorchen und zweitens nur in unstrukturierter Form vorliegen.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>184</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Wodausch, David
Fiedler, Maik
Heuwing, Ben
Mandl, Thomas</p2_authors>
  <p2_organisations>Universität Hildesheim, Deutschland
Georg-Eckert-Institut – Leibniz-Institut für internationale Schulbuchforschung (GEI)
Universität Hildesheim, Deutschland
Universität Hildesheim, Deutschland</p2_organisations>
  <p2_emails>wodausch@uni-hildesheim.de
fiedler@gei.de
heuwing@uni-hildesheim.de
mandl@uni-hildesheim.de</p2_emails>
  <p2_presenting_author>Mandl, Thomas</p2_presenting_author>
  <p2_title>Hinterlistig – schelmisch – treulos – Sentiment Analyse in Texten des 19. Jahrhunderts: Eine exemplarische Analyse für Länder und Ethnien</p2_title>
  <p2_abstract>&lt;p align="center"&gt;&lt;strong&gt;Hinterlistig – schelmisch – treulos – &lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;strong&gt;Sentiment Analyse in Texten des 19. Jahrhunderts: Eine exemplarische Analyse für Länder und Ethnien&lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt;David Wodausch, Maik Fiedler, Ben Heuwing, Thomas Mandl&lt;/p&gt;
&lt;p align="center"&gt;Universität Hildesheim&lt;br /&gt; Institut für Informationswissenschaft und Sprachtechnologie&lt;br /&gt; Hildesheim &lt;br /&gt; mandl@uni-hildesheim.de&lt;/p&gt;
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt;Georg-Eckert-Institut – Leibniz-Institut für internationale Schulbuchforschung (GEI) &lt;br /&gt; Braunschweig&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sentiment Analyse ist eine Standard-Methode des Text Mining und prüft, was in Texten meinungsbehaftet behandelt wird. Damit ist es auch für Digital Humanities sehr interessant. Viele Ansätze basieren auf einfachen Listen, sogenannten Sentiment Lexika, welche sich aber nicht vollständig auf andere Domänen und auch kaum auf andere Zeiträume übertragen lassen. Dieses Abstract zeigt beispielhaft, wie eine solche Liste methodisch erstellt werden kann. Am Beispiel von Schulbüchern des 19. und frühen 20. Jhds. wird gezeigt, dass ein aktuelles Sentiment Lexikon für diesen Zeitraum ungeeignet ist. Am Beispiel der Behandlung des Judentums in den historischen Schulbüchern wird gezeigt, dass quantitative Ansätze des Text Mining alleine nicht ausreichen, um die Komplexität der Bewertung zu erfassen.&lt;/p&gt;
&lt;p&gt;Einleitung&lt;/p&gt;
&lt;p&gt;Text Mining ist in den Digital Humanities eine häufig eingesetzte Technologie mit vielen Facetten. Die Sentiment Analyse versucht zu analysieren, welche Meinungen ein Text zu einer Entität ausdrückt. Dazu werden die im Kontext von Erwähnungen der Entität vorkommenden Stellen ausgewertet (Liu 2012). Häufig ist eine Meinung nicht leicht maschinell zu erkennen, so dass leichtgewichtige linguistische Ansätze (Struß 2016) oder auch tiefgehende Analysen notwendig sind (Somasundaran et al. 2008). Für viele Anwendungen werden aber einfache listenbasierte Ansätze genutzt, welche das Vorkommen von positiven und negativen Wörtern einer Sprache im Umfeld einer Entität auswerten.&lt;/p&gt;
&lt;p&gt;In der hier geschilderten Untersuchung sollte geprüft werden, ob eine Liste für historische Texte effizient erstellt werden könnte und inwieweit damit fachwissenschaftliche Fragestellungen analysiert werden könnten. Im Rahmen des Projektes „Welt der Kinder“ (De Luca 2014) stand eine Kollektion von  über 3500 Schulbüchern des 19. und frühen 20. Jhd. ebenso zur Verfügung wie Werkzeuge zur Analyse (wdk.gei.de). Die Basis bildete eine Analyse der Informationsbedürfnisse der Historiker (Heuwing et al. 2016). Unter anderem wurden Topic Models und darauf basierende Filter entwickelt (Schober &amp; Gurevych 2015).&lt;/p&gt;
&lt;p&gt;Schulbücher eignen sich besonders als Gegenstand der Forschung zum 19. Jhd. Diese Bücher waren oft die einzigen Medien, welche Kinder ausgesetzt werden. Sie prägten somit ihr Weltbild nachhaltig (Fuchs 2014).   &lt;/p&gt;
&lt;p&gt;Methode&lt;/p&gt;
&lt;p&gt;Ziel des Vorgehens war es, ein erstes und vorläufiges Sentiment Wörterbuch aus Schulbüchern des 19. Jhds. zu erstellen und zu prüfen, inwieweit dies sich von einer aktuellen Liste unterscheidet. Dazu wurde folgendes methodische Vorgehen gewählt. Durch die Auswahl von Begriffen, die häufig mit Sentiment verbunden sind, sollten Textstellen identifiziert werden, die dann einer manuellen Auswertung unterzogen wurden. Bei dieser Bewertung wurden die meinungstragenden Phrasen als positiv oder negativ bewertet.&lt;/p&gt;
&lt;p&gt;Konkret wurde zunächst aus dem Gesamtkorpus drei Untermengen ausgewählt, wobei Anfragen gewählt wurden, die nach ersten Hypothesen zu Aussagen mit Meinungen führen würden.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;In einer Untermenge, welche zu Völkern und Staaten generiert wurde, erfolgte eine Recherche nach den Suchbegriffen &lt;em&gt;Deutschland&lt;/em&gt; und &lt;em&gt;Frankreich&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Der Name Napoleon Bonaparte wurde in Jungen und Mädchenbüchern getrennt recherchiert, um so mögliche unterschiedliche Darstellungen zu erkennen.&lt;/li&gt;
&lt;li&gt;Mit dem Wortfeld Jude wurde in Büchern aus zwei Zeiträumen gesucht. Diese umfassten 1850-1859 und 1910-1919. Dabei stand die Hypothese im Raum, dass die verwendete Sprache des Zeitraums 1850 bis 1859 einen weniger radikalisierten Antisemitismus aufweist als die Analyse der Textquellen von 1910 bis 1919. Diese Einschätzung ist auf den ab 1879 bis nach den Ersten Weltkrieg sich radikalisierenden Antisemitismus zurückzuführen.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Das Werkzeug AntConc erlaubt es, Konkordanzen zu extrahieren. Die Suchbegriffe wurden als zentrale Wörter dargestellt und AntConc liefert dann den Kontext der Phrase. Die Sortierung der Phrasen erfolgt nach dem Maß Mutual Information.&lt;/p&gt;
&lt;p&gt;So wurden zu jeder Suche und damit zu jedem der sechs Sub-Korpora  jeweils ca. 100 Phrasen manuell ausgewertet und klassifiziert. Dabei wurde festgehalten, ob eine meinungstragende Phrase vorliegt. Zudem wurde über die Polarität entschieden, also ob die Meinung positiv oder negativ ist. Diese manuelle Annotation semantischer Werte in Textabschnitten erfolgte, um darauf aufbauend das Sentiment Lexikon zu erstellen.&lt;/p&gt;
&lt;p&gt;Ergebnisse&lt;/p&gt;
&lt;p&gt;Für die erste Textmenge ergaben sich dabei 270 unterschiedliche Kollokationen, die eine positive oder negative interpretierte Meinung zu Deutschland oder Frankreich besitzen. Dabei wird Frankreich negativer dargestellt, 49% der meinungstragenden Phrasen sind negativ. Bei Deutschland sind es 39%. Diese beziehen sich beispielsweise auf die Situation vor 1871 und kennzeichnen diese als &lt;em&gt;Zersplitterung&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Für die Textstellen zu Napoleon Bonaparte konnten 174 wertende Phrasen erkannt werden. Allerdings ergaben sich keine nennenswerten Unterschiede in der Darstellung der Person in Jungen und Mädchenschulbüchern.&lt;/p&gt;
&lt;p&gt;Für die qualitative Analyse der beiden Stichproben aus den Subkorpora für das Judentum wurden für den Veröffentlichungszeitraum von 1850 bis 1859 insgesamt 720 und beim zweiten Subkorpus von 1910 bis 1919 insgesamt 825 Konkordanzen untersucht. Dabei wurden insgesamt 231 Kollokationen aus 155 wertenden Konkordanzen gefunden. Insgesamt waren die meisten Vorkommen von Begriffen aus dem Wortfeld &lt;em&gt;Jude&lt;/em&gt; und &lt;em&gt;Judentum&lt;/em&gt; also nicht meinungsbehaftet.&lt;/p&gt;
&lt;p&gt;Die Auswertung ergab, dass durchschnittlich 76,5% aller wertenden Kollokationen in den untersuchten Konkordanzen der Subkorpora als negativ klassifiziert wurden (Subkorpus (1): 102 negativ (77%); Subkorpus (2): 75 negativ (76%)). Somit ist das Verhältnis zwischen negativ wertenden und positiv wertenden Kollokationen beider Sub-Korpora annähernd gleich, wobei negative Äußerungen klar überwiegen. Allerdings ergibt sich aus dieser rein quantitativen Betrachtung keine Meinungsentwicklung gegenüber Juden während des Kaiserreichs, wie sie die Hypothese vermutet hatte.  &lt;/p&gt;
&lt;p&gt;In der Folge wurden die Kollokationen manuell in sechs, in sich homogene Gruppen geclustert, welche jeweils den lokalen Kontext und das Auftreten einer Kollokation beschreiben: Religion; Synonym für Jude; positive Eigenschaft Jude; negative Eigenschaft Jude; Juden im Gesellschaftsleben und Sonstiges. Entwickelt bzw. festgelegt wurden die sechs Cluster auf Grundlage der Kollokationen.&lt;/p&gt;
&lt;p&gt;Die Analyse der Vorkommenshäufigkeit der sechs Cluster ergab eine zeitliche Veränderung. Im Zusammenhang mit Religion wurde das Judentum im früheren Subkorpus ab 1850 häufiger erwähnt als im Zeitraum von 1910 bis 1919. Im späteren Zeitraum wird das Judentum häufiger im Zusammenhang mit dem gesellschaftlichen Leben erwähnt. Diese spiegelt möglicherweise den Wandel des Judenhasses während der zweiten Hälfte des 19. Jahrhunderts auf Grundlage religiöser Motive zu einer biologisch-rassischen Begründung.&lt;/p&gt;
&lt;p&gt;Insgesamt wurden 225 einmalig auftretende meinungstragende Begriffe gefunden. Von diesen sind nur 44% in einer aktuellen Liste des Deutschen vorhanden (Sentiment Wortschatz der Universität Leipzig). Zwar ist die verwendete Liste deutlich länger und aus umfangreicheren Studien hervorgegangen, doch allein dieser Vergleich zeigt, dass das historische Vokabular sich deutlich unterscheidet. Nicht enthalten sind beispielsweise: &lt;em&gt;verachtet, zerstreut, unkünstlerisch&lt;/em&gt; oder &lt;em&gt;halsstarrig&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Auffällig ist, dass im historischen Korpus deutlich mehr meinungstragende Substantive enthalten sind als im aktuellen Korpus, in dem Adjektive überwiegen. Dies müsste weiter untersucht werden. Möglicherweise wurden Substantive mit spezifischen Zielen wie etwa einer oberflächlichen Objektivierung eingesetzt. Beispiele für Substantive sind: &lt;em&gt;Zarenwahnsinn, Blitzesschnelle, Engherzigkeit &lt;/em&gt;oder &lt;em&gt;Schlappe&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Die vorläufigen Ergebnisse zeigen, dass nur eine Verknüpfung des iterativen Einsatzes analoger und digitaler Methoden verschiedene Perspektiven auf Texte einnehmen kann und zu sinnvollen Ergebnissen führen kann. Dies fordert auch Stephen Ramsay unter dem Schlagwort Algorithmic Criticism (Ramsay 2008).&lt;/p&gt;
&lt;p&gt;Mit Criticism wird nicht die Überprüfung hermeneutischer Hypothesen mit algorithmischen Analysen gemeint, sondern die Reflexion, Explikation und Differenzierung von Geisteswissenschaften und Algorithmen sowie die Einbindung multiperspektivischer Zugänge zu Untersuchungsgegenständen (vgl. Bender, 2016, S. 300f.).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bibliographie &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bender, Michael (2016): Forschungsumgebungen in den Digital Humanities: Nutzerbedarf, Wissenstransfer, Textualität. Walter de Gruyter GmbH &amp; Co KG&lt;/p&gt;
&lt;p&gt;De Luca, Ernesto William (2014): Welt der Kinder. Georg-Eckert-Institut – Leibniz-Institut für internationale Schulbuchforschung. URL: http://welt-der-kinder.gei.de&lt;/p&gt;
&lt;p&gt;Fuchs, Eckhardt (2014): Das Schulbuch in der Forschung. Analysen und Empfehlungen für die Bildungspraxis. Georg-Eckert-Institut für internationale Schulbuch-forschung. Band 4. V &amp; R unipress in Göttingen&lt;/p&gt;
&lt;p&gt;Heuwing, Ben; Mandl, Thomas; Womser-Hacker, Christa (2016): Combining contextual interviews and participative design to define requirements for text analysis of historical media. In: Information Research 21 (4) http://www.informationr.net/ir/21-4/isic/isic1606.html&lt;/p&gt;
&lt;p&gt;Liu, Bing (2012): Sentiment Analysis and Opinion Mining. Morgan &amp; Claypool Publishers&lt;/p&gt;
&lt;p&gt;Ramsay, Stephen (2008): Algorithmic Criticism. In: Cite as: A Companion to Digital Literary Studies, ed. Susan Schreibman and Ray Siemens. Oxford: Blackwell, http://www.digitalhumanities.org/companionDLS/&lt;/p&gt;
&lt;p&gt;Schnober, Carsten, and Iryna Gurevych. “Combining Topic Models for Corpus Exploration: Applying LDA for Complex Corpus Research Tasks in a Digital Humanities Project.” In Proceedings of the 2015 Workshop on Topic Models: Post-Processing and Applications, TM ’15. New York, NY, USA: ACM, 2015, S. 11–20.&lt;/p&gt;
&lt;p&gt;Somasundaran, Swapna, Josef Ruppenhofer, and Janyce Wiebe. "Discourse level opinion relations: An annotation study." Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue. Association for Computational Linguistics, 2008.&lt;/p&gt;
&lt;p&gt;Struß, Julia Maria (2016): Multilinguales aspektbasiertes Opinion Mining: Entwicklung eines ressourcenarmen Extraktionsverfahrens und Untersuchung von NutzerperspektivenMehrsprachige Produktsuche und Meinungsanalyse. Dissertation Universität Hildesheim.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>205</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Schmidt, Thomas
Burghardt, Manuel
Dennerlein, Katrin</p3_authors>
  <p3_organisations>Lehrstuhl für Medieninformatik, Universität Regensburg
Lehrstuhl für Medieninformatik, Universität Regensburg
Institut für Deutsche Philologie, Julius-Maximilians-Universität Würzburg</p3_organisations>
  <p3_emails>thomas.schmidt@sprachlit.uni-regensburg.de
manuel.burghardt@ur.de
katrin.dennerlein@uni-wuerzburg.de</p3_emails>
  <p3_presenting_author>Schmidt, Thomas
Burghardt, Manuel
Dennerlein, Katrin</p3_presenting_author>
  <p3_title>"Kann man denn auch nicht lachend sehr ernsthaft sein?" – Zum Einsatz von Sentiment Analyse-Verfahren für die quantitative Untersuchung von Lessings Dramen</p3_title>
  <p3_abstract>&lt;p&gt;Der vorliegende Beitrag untersucht Grenzen und Möglichkeiten des Einsatzes von Sentiment Analysis-Verfahren (SA) im Bereich der Dramenanalyse. Es werden erstmals systematisch verschiedene Methoden der SA für Dramen getestet und evaluiert. Zudem wird exploriert, inwiefern bisher in der Literaturwissenschaft erforschte Aspekte von Dramen mithilfe der SA erfasst werden und inwiefern die SA auch für die Gewinnung neuer literaturwissenschaftlicher Erkenntnisse eingesetzt werden kann.&lt;/p&gt;
&lt;p&gt;Das im Rahmen dieser Studie verwendete Lessing-Korpus umfasst ein mit Strukturinformationen annotiertes Dramenkorpus mit 11 Dramen, bestehend aus insgesamt 8.224 Einzelrepliken. Mit dem am besten evaluierten SA-Verfahren wurde schließlich eine webbasierte Anwendung  (vgl. http://bit.ly/2fKaxya) zur Analyse und Visualisierung von Sentiment-Verteilungen und -Verläufen implementiert.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>156</session_ID>
  <session_short>VP_8c</session_short>
  <session_title>Sofwareentwicklung in den Digitalen Geisteswissenschaften</session_title>
  <session_start>2018-03-02 09:00</session_start>
  <session_end>2018-03-02 10:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Gietz, Peter</chair1>
  <attendee_count>7</attendee_count>
  <chair1_name>Peter Gietz</chair1_name>
  <chair1_organisation>DAASI International</chair1_organisation>
  <chair1_email>peter.gietz@daasi.de</chair1_email>
  <chair1_ID>1019</chair1_ID>
  <sessionID>156</sessionID>
  <presentations>3</presentations>
  <p1_paperID>153</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Fechner, Martin</p1_authors>
  <p1_organisations>Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland</p1_organisations>
  <p1_emails>fechner@bbaw.de</p1_emails>
  <p1_presenting_author>Fechner, Martin</p1_presenting_author>
  <p1_title>Eine nachhaltige Präsentationsschicht für digitale Editionen </p1_title>
  <p1_abstract>&lt;p&gt;&lt;strong&gt;Einleitung&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Die Anforderungen an die Erstellung von Editionen sind im letzten Jahrzehnt gestiegen.[1] Es ist unstreitig, dass digitale Editionen Annäherungen an den edierten Text ermöglichen, die weit über den statischen Druck hinausgehen (Sahle 2016). Gleichzeitig gibt es aber auch noch immer Anforderungen, etwa die Zitierbarkeit, die im Druck gelöst sind, zu denen es aber in der Webpublikation noch keine einheitliche Entsprechung gibt. Insbesondere stellt sich die Frage nach der Nachhaltigkeit und der Langzeitarchivierung.[2]&lt;/p&gt;
&lt;p&gt;Ein Lösungsansatz besteht darin, Techniken der Data Curation zu etablieren (Pempe 2012: 141f.). Damit ist aber ein Aufwand verbunden, der linear mit der Zahl der Webpublikationen wächst und polynomial, wenn auch die Schnittstellen zu verknüpften Ressourcen gepflegt werden müssen. Eine besondere Herausforderung stellt sich, wenn man sinnvollerweise annimmt, dass die Darstellung und die Funktionalitäten Teil der Edition selbst sind (Ralle 2016, Pierazzo 2015: 127-146, Porter 2016 und Turska et al. 2016). Wenn also nur die Forschungsdaten nachhaltig archiviert werden, geht die ursprüngliche Präsentation letztendlich verloren. Es wird zwar momentan auf die Einführung von Standards gesetzt, [3] einen wirklichen Mehrwert für den Gebrauch von digitalen Editionen entfalten diese aber erst, wenn sie durch das Angebot von technischen Schnittstellen unterstützt werden.[4] Bisher fehlt es jedoch an einem klar definierten Interface, welches die Forschungsdaten in eine nachhaltige funktionale Präsentationsform mit allen Aspekten einer digitalen Edition übersetzt.[5]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Entwurf einer Schnittstelle&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hier wird nun ein System für die Nachhaltigkeit von Webpublikationen entworfen. Es verbindet die Daten mit der Präsentationsschicht und kann mithilfe einer projektspezifischen, archivierbaren Konfiguration über eine Schnittstelle gesteuert werden. Durch den Einsatz einer solchen Schnittstelle können Webpublikationen inklusive der entsprechenden Funktionalitäten aus den Forschungsdaten reproduziert werden.[6] Digitale Editionen unterscheiden sich technisch zwar zurzeit noch stark voneinander (Robinson 2016), der hier gemachte Vorschlag und das sich anschließende Software-Beispiel zeigen daher, wo es schon jetzt Möglichkeiten zur Standardisierung gibt und wie diese aussehen könnten.&lt;/p&gt;
&lt;p&gt;Vorbild für das System ist der erfolgreiche IIIF-Standard, der zu einem ähnlichen Zweck für Bilder eingeführt wurde (Cramer 2011). Der IIIF-Standard sieht eine Aufteilung von Server- und Clientstruktur vor, und als Austauschformat fungiert eine Manifest-Datei. Übertragen auf Digitale Editionen heißt dies, dass eine archivierbare Manifestdatei notwendige Definitionen festhält, mit denen es einem Viewer möglich ist, die vollständige Präsentation und Funktionalität der jeweiligen digitalen Edition herzustellen.[7]&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Manifestdatei&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Der Vorschlag für eine Manifestdatei, aus der die Funktionalitäten hergestellt werden können, lautet wie folgt (vgl. auch die nachstehende Tabelle):&lt;/p&gt;
&lt;p&gt;Ganz konkret sollten die notwendigen Metadaten der digitalen Edition definiert werden. Dabei kann darüber diskutiert werden, welche Informationen für eine digitale Edition notwendig und welche für wünschenswert gehalten werden.&lt;/p&gt;
&lt;p&gt;Mit der Definition einer Gliederung der Materialien, also von Editionstexten, Kommentaren und Begleitmaterial, können sinnvolle hierarchische Navigationselemente in der Darstellung umgesetzt werden.&lt;/p&gt;
&lt;p&gt;In der Präsentationsoberfläche muss es möglich sein, zu den verschiedenen Datentypen zu navigieren und entsprechende Überblickslisten anzeigen zu lassen. Dafür werden die Typen von Datenobjekten (etwa Textsorten oder Register) definiert, für die Unterstützung von facettierten Filtern, müssen diese entsprechend festgelegt werden.&lt;/p&gt;
&lt;p&gt;Die Darstellung der Dokumente in der Einzelansicht kann unter Einbindung von Schnittstellen für die Transformation geschehen.[8] Auch können für die Navigation zu Abschnitten im Dokument, die entsprechenden Teile definiert werden. Zu jedem Objekttyp sollten auch die vorhandenen Beziehungen zu Teilen, als auch zu anderen Objekttypen festgehalten werden. Damit können verschiedene Forschungsdaten in einer dynamischen Ansicht zusammengeführt werden.&lt;/p&gt;
&lt;p&gt;Für eine nachhaltige Einbindung externer Ressourcen sollte definiert werden, auf welche Ressourcen die Edition Bezug nimmt und welche dazu Schnittstellen genutzt werden. Die Integration externer Ressourcen kann problematisch sein, wenn deren Verfügbarkeit noch nicht gesichert ist.[9]&lt;/p&gt;
&lt;p&gt;&lt;em&gt;[Tabelle 1: Definitionen für die Schnittstelle]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prototyp&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Im Forschungprojekt „ediarum“ wird momentan ein Prototyp entwickelt, der das vorgestellte Konzept umsetzt und die Anforderungen der Darstellung erfüllen soll (Dumont/Fechner 2014 und http://www.bbaw.de/telota/software/ediarum). Dieser enthält eine Programmbibliothek, die die Funktionalitäten zur Anzeige bereitstellt. Kern der spezifischen Darstellung einer digitalen Edition wird durch eine Manifestdatei nach obigem Konzept gebildet. Mit diesem Prototypen ist es bereits möglich mithilfe der Manifestdatei und wenigen Anpassungen, die vor allem das Layout betreffen, eine Webseite für eine Digitale Edition zu erstellen. Mit dem Einsatz des Prototyps für mehrere Editionen werden die einzelnen Funktionalitäten und Konfigurationsmöglichkeiten ausgetestet und verbessert. Schließlich soll er als Viewer zur Verfügung stehen, der zur Präsentation lediglich die Manifestdatei und Zugang zu den Daten über eine entsprechende Serverinfrastruktur benötigt. Durch das Zusammenspiel der Kernkomponente, die alle Funktionalitäten bereitstellt, und der projektspezifischen Komponente, die im Layout angepasst werden kann, wird eine hohe Flexibilität erreicht. Somit kann für jedes Projekt kann ein individueller Auftritt erzeugt werden.&lt;/p&gt;
&lt;p align="left"&gt;&lt;em&gt; [Abbildung 1: Manifestdatei des Prototyps]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fazit&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dieser Artikel schlägt die Entwicklung eines neuen Standards vor, um Nachhaltigkeit digitaler Editionen zu verbessern. Denn es braucht für die Langzeitarchivierung digitaler Editionen auch eine Standardisierung ihrer Funktionalitäten. Editionen sind zwar sehr unterschiedlich, doch mit dem hier beschriebenen Interface und Austauschformat wurde beispielhaft ein praktikabler Ansatz vorgestellt, um diese Lücke zu schließen. Eine Weiterentwicklung des hier gemachten Entwurfs und die Integration weiterer Standards kann in Zukunft die Unabhängigkeit digitaler Editionen von einzelnen technischen Systemen erhöhen und unterstützt damit die Langzeitarchivierung.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bibliographie&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cramer, Tom&lt;/strong&gt; (2011): &lt;em&gt;The International Image Interoperability Framework (IIIF): Laying the Foundation for Common Services, Integrated Resources and a Marketplace of Tools for Scholars Worldwide.&lt;/em&gt; Blogpost. URL: https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dumont, Stefan / Fechner, Martin&lt;/strong&gt; (2014): “Bridging the Gap: Greater Usability for TEI encoding”, in: &lt;em&gt;Journal of the Text Encoding Initiative&lt;/em&gt; 8. URL: http://jtei.revues.org/1242&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Eggert, Paul&lt;/strong&gt; (2016): “The reader-oriented scholarly edition”, in: &lt;em&gt;Digital Scholarship in the Humanities &lt;/em&gt;31, 4: 797–810. DOI: 10.1093/llc/fqw043&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Förderkriterien für wissenschaftliche Editionen in der Literaturwissenschaft.&lt;/em&gt; In: Informationen für Geistes- und Sozialwissenschaftler/innen (11) 2015.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Holmes, Martin&lt;/strong&gt; (2017): “Whatever happened to interchange?” In: &lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt; 32, suppl_1: i63–i68. DOI: 10.1093/llc/fqw048&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Meier, Wolfgang&lt;/strong&gt; (2017):&lt;em&gt; teiPublisher. The instant publishing toolbox.&lt;/em&gt; Version v2.2.0, Stand 8. September 2017. URL: http://teipublisher.com/index.html&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Meier, Wolfgang / Turska, Magdalena&lt;/strong&gt; (2016): “TEI Processing Model Toolbox: Power To The Editor”, in: &lt;em&gt;Digital Humanities 2016: Conference Abstracts:&lt;/em&gt; 936. URL: http://dh2016.adho.org/abstracts/401&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pempe, Wolfgang&lt;/strong&gt; (2012): „Geisteswissenschaften“, in: &lt;em&gt;Langzeitarchivierung von Forschungsdaten: Eine Bestandsaufnahme&lt;/em&gt;. Hg. v. Heike Neuroth et al., Version 1.0, Stand 2012: 137-159. URN: urn:nbn:de:0008-2012031401&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pierazzo, Elena&lt;/strong&gt; (2015): &lt;em&gt;Digital Scholarly Editing: Theories, Models and Methods&lt;/em&gt;. Abingdon, England.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Porter, Dot&lt;/strong&gt; (2016): &lt;em&gt;"What is an edition anyway?" My Keynote for the Digital Scholarly Editions as Interfaces conference, University of Graz. &lt;/em&gt;&lt;em&gt;Blogpost&lt;/em&gt;, in: Dot Porter Digital. URL: http://www.deporterdigital.org/?p=309&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ralle, Inga Hanna&lt;/strong&gt; (2016): „Maschinenlesbar – menschenlesbar. Über die grundlegende Ausrichtung der Edition“, in: &lt;em&gt;Editio&lt;/em&gt; 30, 1: 144-156. DOI: 10.1515/editio-2016-0009&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Robinson, Peter M. W.&lt;/strong&gt; (2016): “Project-based digital humanities and social, digital, and scholarly editions”, in: &lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt; 31, 4: 875–889. DOI: 10.1093/llc/fqw020&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sahle, Patrik&lt;/strong&gt; (2014): Kriterienkatalog für die Besprechung digitaler Editionen. Version 1.1, Stand Juni 2014. URL: https://www.i-d-e.de/publikationen/weitereschriften/kriterien-version-1-1/&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sahle, Patrik&lt;/strong&gt; (2016): “What is a Scholarly Digital Edition?” In: &lt;em&gt;Digital Scholarly Editing: Theories and Practices:&lt;/em&gt; 19-40. DOI: 10.11647/obp.0095.02&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shillingsburg, Peter&lt;/strong&gt; (2016): “Reliable social scholarly editing”, in: &lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt; 31, 4: 890–897. DOI: 10.1093/llc/fqw044&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steinke, Tobias&lt;/strong&gt; (Redaktion) (2005): &lt;em&gt;LMER Langzeitarchivierungsmetadaten für elektronische Ressourcen.&lt;/em&gt;Version 1.2, Stand 7. April 2005. URN: urn:nbn:de:1111-2005041102&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Turska, Magdalena / Cummings, James / Rahtz, Sebastian&lt;/strong&gt; (2016): Challenging the Myth of Presentation in Digital Editions, in: &lt;em&gt;Journal of the Text Encoding Initiative&lt;/em&gt; 9. DOI: 10.4000/jtei.1453&lt;/p&gt;
&lt;br clear="all" /&gt;
&lt;p&gt;[1] Ziele von Editionen finden sich bei (Förderkriterien 2015, Eggert 2016, Ralle 2016 und Sahle 2016).&lt;/p&gt;

&lt;p&gt;[2] Für die Forschungsdaten selbst ist die Langzeitarchivierung grundsätzlich gelöst. Digitalen Editionen wird jedoch nur geringe Zuverlässigkeit zugeschrieben  (Pierazzo 2015: 169).&lt;/p&gt;

&lt;p&gt;[3] So wird das Format der Text Encoding Initiative (TEI) verbreitet eingesetzt, das nur ein erster Schritt zur Standardisierung ist (Holmes 2017). Weitere Standards und Identifikatoren sind etwa die GND für Personen, GeoNames-IDs für Orte oder der Canocical Text Service (CTS) für Zitationen, das DITA- oder DocBook-Format für (technische) Dokumentationen. Als Langzeitarchivierungsformat für Metadaten gibt es etwa LMER (Steinke 2005).&lt;/p&gt;

&lt;p&gt;[4] Gute Ansätze einer Präsentationsoberfläche für Einzeldokumente bietet der "teiPublisher", der auf dem Datenformat ODD aufbaut (Meier 2017, Meier/Turska 2016 und Turska et al. 2016).&lt;/p&gt;

&lt;p&gt;[5] Die Benutzbarkeit digitaler Editionen leidet unter mangelnden Interfaces (Robinson 2016). Pierazzo sieht es als Nachteil, dass viele digitale Editionen unterschiedliche User Interfaces besitzen, hält aber eine zukünftige Angleichung für wahrscheinlich (Pierazzo 2015: 162).&lt;/p&gt;

&lt;p&gt;[6] Es gibt dabei sehr unterschiedliche Anforderungen, die an die Präsentation digitaler Editionen gestellt werden (Shillingsburg 2016, Ralle 2016: 154f. und Sahle 2014).&lt;/p&gt;

&lt;p&gt;[7] Für die Langzeitarchivierung könnte eine Kompatibilität der Manifestdatei etwa mit LMER hergestellt werden (Steinke 2005).&lt;/p&gt;

&lt;p&gt;[8] TEI-Dokumente können etwa mit ODD zur Einzelpräsentation transformiert werden (Meier 2017).&lt;/p&gt;

&lt;p&gt;[9] Externe Daten könne auch in einer „Standalone“ Version überführt werden (Holmes 2017).&lt;/p&gt;

</p1_abstract>
  <p2_paperID>171</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Bürgermeister, Martina
Schneider, Gerlinde
Makowski, Stephan
Jeller, Daniel
Bigalke, Jan
Theisen, Christian
Vogeler, Georg</p2_authors>
  <p2_organisations>ZIM-ACDH, Universität Graz, Österreich
ZIM-ACDH, Universität Graz, Österreich
CCeH, Universität Köln, Deutschland
ICARUS, Wien, Österreich
CCeH, Universität Köln, Deutschland
CCeH, Universität Köln, Deutschland
ZIM-ACDH, Universität Graz, Österreich</p2_organisations>
  <p2_emails>martina.buergermeister@uni-graz.at
gerlinde.schneider@uni-graz.at
stephan.makowski@uni-koeln.de
daniel.jeller@icar-us.eu
JBigalke@smail.uni-koeln.de
ctheise1@smail.uni-koeln.de
georg.vogeler@uni-graz.at</p2_emails>
  <p2_presenting_author>Bürgermeister, Martina
Schneider, Gerlinde</p2_presenting_author>
  <p2_title>„Software Aging“ in den DH: Kritik des reinen Forschungswillens</p2_title>
  <p2_abstract>&lt;p&gt;Dieser Beitrag behandelt die Frage, warum in der DH entwickelte und angewandte  Software häufig schnell altert. Jede Software altert relativ zu der Umgebung, in der sie eingesetzt wird, unabhängig von der Qualität am Beginn ihrer Verwendung. Wandeln sich Hardware, Infrastruktur oder die Anforderungen an die Software, dann wird sie, um weiter brauchbar zu sein, angepasst. Je nach Beschaffenheit können sich diese Anpassungen positiv, oftmals aber auch negativ auf die Lebensdauer und Fitness einer Software auswirken.&lt;/p&gt;
&lt;p&gt;Aus der Praxis behaupten wir, dass kontextuelle und inhaltliche Spezifika von DH Software dazu führen, dass eine langfristige Lauffähigkeit und Brauchbarkeit erschwert werden. Unser Beitrag bringt allgemein die Bedeutung und Relevanz des Themas „Software Evolution“ (1)  nahe, beschreibt Spezifika der Software Evolution aus der DH-Praxis (2) und zeigt welche konkreten Maßnahmen im Projekt &lt;em&gt;monasterium.net&lt;/em&gt; (3) gesetzt werden.&lt;/p&gt;
</p2_abstract>
  <p3_paperID>298</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Barabucci, Gioele</p3_authors>
  <p3_organisations>Universität zu Köln, Deutschland</p3_organisations>
  <p3_emails>gioele.barabucci@uni-koeln.de</p3_emails>
  <p3_presenting_author>Barabucci, Gioele</p3_presenting_author>
  <p3_title> Funktionale und deklarative Programmierung-basierte Methode für nachhaltige, reproduzierbare und verifizierbare Datenkuration.</p3_title>
  <p3_abstract>&lt;p&gt;&lt;strong&gt;Zusammensetzung&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Das Sichansammeln von digitalen Daten im Bereich Digital Humanities macht deren Aufrechterhaltung von Datenkuratoren immer mehr notwendig. Der übliche Modus Operandi der Datenkuratoren (manuelle Konversionen und Anpassungen) ist aber untragbar wegen der ständigen Steigerung der Zahl und des Umfangs der zu verarbeitenden Quellen.&lt;/p&gt;
&lt;p&gt;Dieser Vortrag stellt eine neue Methode für die Kuration digitalen Daten vor, die auf den Prinzipien der funktionalen Programmierung, der unix-Tools und der XML-Technologien basiert. Diese Methode wurde vom Cologne Center für eHumanties der Universität zu Köln seit 2014 innerhalb des Lazarus-Projekts und danach in verschieden anderen DH-Projekten angewendet.&lt;/p&gt;
&lt;p&gt;Der Hauptvorteil dieser Methode ist, dass die Ergebnisse einfach zu reproduzieren und zu verifizieren sind.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kuration von Digitalen Daten&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Eine der Aufgaben der Datenkuration und der Datenkuratoren ist: »[to] intervene in the research process in order to translate or migrate data into new formats, to enhance it through additional layers of context or markup, to create connections between data sets, and to otherwise ensure that data is maintained in as highly-functional a form as possible.« (Flanders &amp; Muñoz, 2017).&lt;/p&gt;
&lt;p&gt;Praktisch können wir die Arbeit der Datenkuratoren auf folgende Art und Weise grob zusammenfassen:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p&gt;Die Daten werden von den Forschern zu den Kuratoren übertragen.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Die Kuratoren studieren die Daten, sowohl ihren Inhalt als auch ihr Format.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Die Kuratoren reichern die Daten mit den nötigen Metadaten an und sie sorgen dafür, dass eventuelle Inkohärenzen zwischen den originalen Formaten und die Zielformaten ausgeglichen werden.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Die so verarbeiteten Daten werden publiziert oder archiviert.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Datenkuration am CCeH: Das Cologne-Sanskrit-Lexicon-Projekt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ein praktisches Beispiel von Datenkuration sind die Wörterbücher des Cologne Sanskrit Lexikon, die von verschiedenen Wissenschaftlern der Universität zu Köln seit den 90er Jahren (pre XML und pre Unicode) erarbeitet wurden und die, innerhalb des Lazarus-Projektes, vom CCeH kuratiert (in TEI/Unicode umgewandelt) und 2015 zugänglich gemacht wurden.&lt;/p&gt;
&lt;p&gt;Die anfängliche Arbeitshypothese, im Verlauf jedoch fallen gelassen, sah einen eher klassischen Workflow vor: die originalen Dateien in XML umwandeln, danach eine XSLT-Transformation nutzen um diese in TEI zu umwandeln und schließlich, die durch die Transformation entstandenen Impferektionen per Hand zu verbessern.&lt;/p&gt;
&lt;p&gt;Wir haben es vorgezogen, diesen Weg aus zwei Gründe nicht einzuschlagen.&lt;/p&gt;
&lt;p&gt;Erstens, von beginn an sind verschiedene Versionen der zu kuratierenden Dateien aufgetaucht. Hätten wir neue Versionen der Dateien auf halber Strecke entdeckt, wäre wir die bis dahin geleistete Arbeit um sonst gewesen.&lt;/p&gt;
&lt;p&gt;Zweitens, die Arbeitsgruppe bestand aus drei Personen aus unterschiedlichen Fächern und mit unterschiedlichen Herangehensweisen an das Thema Kuration. Einen homogenen Stil zu beizubehalten wäre nicht einfach wahrscheinlich unmöglich gewesen.&lt;/p&gt;
&lt;p&gt;Die Arbeitsgruppe hat sich dann für eine andere Arbeitsweise entschieden: eine Methode, die auf der funktionellen Programmierung basiert, statt manueller Konversionen und Anpassungen.&lt;/p&gt;
&lt;p&gt;In dieser ist jeder Arbeitsschritt formell durch ein Programm beschreibt, das in einer funktionalen und deklarativen Programmiersprache implementiert ist. Diese Programme sind im Sinne einer Pipeline organisiert, d.h. der Output des einen ist der Input eines anderen. Die Kuratoren erklären nicht nur was sind die Schritte sondern auch was sind die Abhängigkeiten zwischen den einzelnen Schritten sind, z.B. dass die Konversionschritte den Fetchschritten folgen sollen.&lt;/p&gt;
&lt;p&gt;Alle diese Schritte sind reine idempotente Funktionen. D.h., dass ihr Ergebnis nur von den Input-Daten anhängig ist. Konkret bedeutet das, dass man die Kurationpipeline mehrmals durchlaufen kann, und immer das gleiche Ergebnis resultiert. Dies steht im Gegensatz zu den klassischen Skript-basierten Methoden.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lazarus-Kurationsworkflow: XML-Pipelines, Makefiles und Schematron&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In der Essenz bedeutet das konkret, dass der Kurationworkflow, der im Lazaruz-Projekt und in anderen folgenden CCeH-Projekte benutzt wurde, aus drei große Komponenten besteht:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;
&lt;p&gt;Die Wörterbücher-Makefiles. Ein Makefile ist eine Datei, die vom unix-Tool `make` benutzt wird. Der Makefile erklärt, wie man eine Datei X (genannt &lt;em&gt;target&lt;/em&gt;) durch die Dateien A, B und C (genannt &lt;em&gt;Anhängigkeiten von X&lt;/em&gt;) herstellen kann. Im Fall des Cologne-Sankrit-Lexicon-Projekts erklären die Makefiles, wo man die originalen Dateien finden kann, wie man sie herunterladen kann, wie man den Konversionprozess durchführen kann und wie man die Ergebnisse testen kann.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Die Konversionpipelines. Jede Pipeline ist für die Konversion bestimmte Dateien verantwortlich und besteht aus verschieden Schritten. Jeder Schritt ist implementiert durch einer XSLT-Transformation. Die Pipelines selbst sind XProc-basierte XML-Pipelines (Walsh, 2007).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Schematron-basierte Tests. Verschieden automatisierte Tests kontrollieren, dass die hergestellte Dateien valide sind, dass keine alte schon behobenen Fehler erneut eingepflegt werden sowie, dass keine Informationen verloren gehen.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Der Gebrauch dieser Methode hat viele Vorteile sowohl im Hinblick auf die methodologische Stringenz als auch der Technik an sich:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p&gt;Jede einzelne Handlung der Kuratoren ist formalisiert und dokumentiert (durch XSLT-Code und Code-Kommentare).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jeder Wissenschaftler kann unabhängig verifizieren, wie die Ergebnisse entstanden sind.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jeder Schritte kann einzeln getestet werden.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Man kann zurückverfolgen, welcher Schritt ein bestimmt Konstrukt in den Ergebnisse generiert hat.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Die Wiederverwendung von Schritten ist möglich und leicht nachzuvollziehen&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kann eine methodologische Kohärenz über Jahre hinweg beibehalten werden, auch wann neue Kuratoren diese Daten verwalten werden.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dank Versioning-Systeme wie &lt;em&gt;git&lt;/em&gt; kann man sehen, wie der Kurationprozess entwickelt worden ist.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Diese Methode ändert nicht die Rolle oder die Verantwortung der Kuratoren, aber sie verändert grundlegend ihre tägliche Arbeit. Die Arbeit der Kuratoren besteht nicht mehr in dem Modifizieren von Dateien in einem Editor, sondern in dem Schreiben von Arbeitschritten und in dem korrekten Verwaltung von den Abhängigkeiten zwischen Arbeitschritten.&lt;/p&gt;
&lt;p&gt;Die Kurationarbeit ist dann in zwei Teile aufgeteilt. Der erste Teil besteht im Schreiben und im schrittweise Präzisierung von Kurationsprogrammen, welches die Hauptaufgabe der Kuratoren darstellt. Hierin zeigt sich die Fähigkeit, die Erfahrung der Kuratoren sowie die von ihnen präferierten anwendbaren Richtlinien.&lt;/p&gt;
&lt;p&gt;Der zweite Teil ist die Schaffung von den kuratierten Daten, welche in sterilen Art und Wiese von ein Koordinationsprogramm vollzogen würde, welches die verschiedenen Schritte in der von Kuratoren bestimmten Reihenfolge in wenigen Minuten durchführt.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Verwandte Arbeiten&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Für die Kuration digitaler Daten wurden die Verwendung von &lt;em&gt;git&lt;/em&gt; vorgeschlagen, um die durchgeführte Änderungen zu dokumentieren. Das ist aber nicht ausreichend. &lt;em&gt;git&lt;/em&gt; speichert nur &lt;em&gt;was&lt;/em&gt; geändert wurde, nicht welche die Absicht mit welche eine Änderung durchgeführt wurde. Zusätzlich, die Änderungen mithilfe von git nachzuvollziehen, löst nicht die Probleme, welche entstehen, wenn die originalen Daten verändert werden: In diesem Fall muss die ganze Arbeit von vorne begonnen werden.&lt;/p&gt;
&lt;p&gt;Ähnliche Workflows, in welchen die Hauptaufgabe der Kuratoren ist, die Pipelines zu schreiben, finden sie sich häufig in der Informatik (Doltra &amp; Löh, 2008; Schoen &amp; Perry, 2014) und in der Physik (Peng, 2009).&lt;/p&gt;
&lt;p&gt;Diese sind auch im Bereich Digital Scholarly Edition vorgeschlagen worden, z.B. von van Zundert (2016) oder Barabucci und Fischer (2018).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bibliographie&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Barabucci, Gioele &amp; Fischer, Franz. (2018 accepted for publication). The formalization of textual criticism: bridging the gap between automated collation and edited critical texts. In Advances in Digital Scholarly Editing: Papers presented at the DiXiT conferences in The Hague, Cologne, and Antwerp.&lt;/p&gt;
&lt;p&gt;CCeH. (2015). sanskrit-dict-to-tei: TEI-fy existing Sanskrit dictionaries. https://github.com/cceh/sanskrit-dict-to-tei (Das Repository wird bis Ende 2017 veröffentlicht werden).&lt;/p&gt;
&lt;p&gt;Dolstra, E., &amp; Löh, A. (2008). NixOS: A purely functional Linux distribution. ACM Sigplan Notices, 43(9), 367-378.&lt;/p&gt;
&lt;p&gt;Flanders, Julia &amp; Muñoz, Trevor. (2017). An Introduction to Humanities Data Curation. http://guide.dhcuration.org/contents/intro/, aufgerufen 2017-09-25&lt;/p&gt;
&lt;p&gt;Peng, R. D. (2009). Reproducible research and biostatistics. Biostatistics, 10(3), 405-408.&lt;/p&gt;
&lt;p&gt;Schoen, S., &amp; Perry, M. (2014). Why and how of reproducible builds: Distrusting our own infrastructure for safer software releases. &lt;https://air.mozilla.org/why-and-how-of-reproducible-builds-distrusting-our-own-infrastructure-for-safer-software-releases/&gt;, aufgerufen 2017-09-25&lt;/p&gt;
&lt;p&gt;Walsh, N., Milowski, A., &amp; Thompson, H. S. (2007). XProc: An XML pipeline language. XML Prague 2007.&lt;/p&gt;
&lt;p&gt;van Zundert, Joris J. (2016). Close Reading and Slow Programming — Computer Code as Digital Scholarly Edition.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>73</session_ID>
  <session_title>Kaffeepause</session_title>
  <session_start>2018-03-02 10:30</session_start>
  <session_end>2018-03-02 11:00</session_end>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>157</session_ID>
  <session_short>VP_9a</session_short>
  <session_title>Theorie der Digitalen Geisteswissenschaften IV</session_title>
  <session_start>2018-03-02 11:00</session_start>
  <session_end>2018-03-02 12:30</session_end>
  <session_room_ID>2</session_room_ID>
  <session_room>Hörsaal A1, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Jannidis, Fotis</chair1>
  <attendee_count>6</attendee_count>
  <chair1_name>Fotis Jannidis</chair1_name>
  <chair1_organisation>Universität Würzburg</chair1_organisation>
  <chair1_email>fotis.jannidis@uni-wuerzburg.de</chair1_email>
  <chair1_ID>1069</chair1_ID>
  <sessionID>157</sessionID>
  <presentations>3</presentations>
  <p1_paperID>240</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Ernst, Thomas</p1_authors>
  <p1_organisations>University of Amsterdam, Niederlande</p1_organisations>
  <p1_emails>t.ernst@uva.nl</p1_emails>
  <p1_presenting_author>Ernst, Thomas</p1_presenting_author>
  <p1_title>Critical Digital Cultural Studies: Digitale Kulturwissenschaft und die Kritik des Mem-Begriffs </p1_title>
  <p1_abstract>&lt;p&gt;Die Digitalisierung hat weltweit nicht nur positive Effekte hervorgebracht, sondern auch vielfach kritisierte Probleme verursacht – man denke nur an die monopolistische Marktmacht intransparenter Firmen der digitalen Information (Google, Facebook), die neuen Formen der Überwachung (Snowden vs. NSA) oder die Popularisierung politischer Öffentlichkeiten und deren Beitrag zu unerwarteten Entwicklungen (Präsident Trump, Brexit, AfD). Während zwar einerseits die digitalen Medien und Kommunikationsverhältnisse den Alltag der meisten europäischen Bürgerinnen und Bürger bestimmen, hat sich andererseits noch keine digitale Kultur etabliert, in der die Bürger auf breiter Ebene kritisch und bewusst mit der Macht von Algorithmen oder den Inhalten Allgemeiner Geschäftsbedingungen umgingen.&lt;/p&gt;
&lt;p&gt;In einer solchen Periode eines umfassenden gesellschaftlichen Medienwandels ist es die Aufgabe einer Digitalen Kulturwissenschaft, die sich neben allgemeinen kulturellen Gegenständen auch mit digitalen Öffentlichkeiten, Medien und Methoden kulturtheoretisch und -analytisch beschäftigt, sowohl innerhalb der Geisteswissenschaften als auch in die Gesellschaft hinein den Stand der digitalen Kultur kritisch zu reflektieren. Dazu gehört die Frage des Ausgleichs zwischen dem Schutz und der Öffnung persönlicher und institutioneller Daten, die Frage nach klarer Zuweisung von Identitäten oder der Möglichkeit der anonymisierten Mediennutzung, nach Formen der kollaborativen digitalen Arbeit und ihrer Regulierung oder nach den Online-Marktverhältnissen und der (Nicht-)Offenlegung von Algorithmen, die öffentliche Massenmedien regulieren – sowie deren Auswirkungen auf unterschiedliche Kulturen. Mit solchen Themen beschäftigen sich intensiv die Medien- und Medienkulturwissenschaften (vgl. Engemann 2003, Lovink 2008, Reichert 2013, Schäfer 2011), aber auch Künstler und Publizisten (Lanier 2014, Morozov 2013) sowie Politiker und politisch Bewegte (Albrecht 2014, Wagner 2017).&lt;/p&gt;
&lt;p&gt;Für den Bereich der Kulturwissenschaft ist eine grundsätzlichere theoretische Begriffsarbeit notwendig, bevor eine Digitale Kulturwissenschaft, die sich explizit als (digital)kulturkritisch versteht, grundiert werden kann. Um solche – in einem internationalen, interdisziplinären und komparatistischen Kontext zu verortende und von mir hiermit eingeführte – &lt;em&gt;Critical Digital Cultural Studies&lt;/em&gt; zu fundieren, muss zunächst ein angemessener Begriff der Kritik bestimmt werden. Dieser lässt sich in einem dreifachen Verfahren konturieren: Zunächst kann ein kulturwissenschaftlicher Begriff der Kritik in einer kritischen Lektüre der Auseinandersetzungen von Theodor W. Adorno, Max Horkheimer und Michel Foucault mit den Begriffen der Kritik und der Aufklärung ermöglichen, einen kulturtheoretisch fundierten Begriff einer Kritik der digitalen Vernunft zu entwickeln.&lt;/p&gt;
&lt;p&gt;Max Horkheimer hat 1937 mit seiner Unterscheidung von traditioneller und kritischer Theorie „die mathematische Naturwissenschaft, die als ewiger Logos erscheint,“ einer geisteswissenschaftlich-begrifflich ausgerichteten „kritische[n] Theorie der bestehenden Gesellschaft“ (Horkheimer 1995: 215) gegenübergestellt. Als Folge des Nationalsozialismus und des Holocaust denken Horkheimer und Theodor W. Adorno in ihrer &lt;em&gt;Dialektik der Aufklärung&lt;/em&gt; über die Konsequenzen aus diesem Zivilisationsbruch nach: Fortan müsse das Denken von der „Selbstzerstörung der Aufklärung“ ausgehen, zugleich sei allerdings – so die dialektische Figur – „die Freiheit in der Gesellschaft vom aufklärenden Denken unabtrennbar“ (Horkheimer/Adorno 2003: 3). Eine starke Skepsis gegen die Massenmedien und die technische Entwicklung prägen diesen Ansatz, der die Selbstzerstörung der Aufklärung durch mehr (individuelle) Aufklärung überwinden will.&lt;/p&gt;
&lt;p&gt;Es mag überraschen, dass Michel Foucault in seiner Arbeit über einen diskursanalytisch fundierten Begriff der Kritik explizit an diese Vernunftkritik der Kritischen Theorie anschließt, wobei bei ihm die Technikkritik eine wesentlich geringere Rolle spielt. Foucault stellt den Begriff der Kritik jenem der Regierung, der „Regierbarmachung der Gesellschaft“ (Foucault 1992: 17), entgegen: Kritik als „die Kunst nicht dermaßen regiert zu werden.“ (Ebd.: 12) Innerhalb dieses Verständnisses der Kritik stehen die Begriffe des Wissens und der Macht zentral: Wissen bezeichnet bei Foucault „alle Erkenntnisverfahren und -wirkungen [...], die in einem bestimmten Moment und in einem bestimmten Gebiet akzeptabel sind“; Macht wiederum jene Mechanismen, „die in der Lage scheinen, Verhalten oder Diskurse zu induzieren“ (ebd.: 32). Foucault plädiert somit für die kritische Analyse jener gesellschaftlichen Vernetzungen, die der Wissensproduktion dienen und zugleich der Legitimierung dieses Wissens – und somit der Machtausübung – dienen (vgl. ebd.: 37). Mit Foucault ließe sich ein kulturtheoretisch und diskursanalytisch fundierter Begriff der Kritik formulieren, der gerade nicht – wie noch bei Adorno und Horkheimer – technik- und massenfeindlich ist, sondern vielmehr auch bei der Analyse digitaler Massenmedien genutzt werden und zugleich kritisch gegen die Wissensproduktion der traditionellen und der Digitalen Kulturwissenschaft selbst gewendet werden kann (vgl. auch Foucault 1995, Foucault 2003).&lt;/p&gt;
&lt;p&gt;Ein solchermaßen konturierter kritischer Begriff des Wissens kann zusätzlich bereichert werden durch bisherige Arbeiten aus verwandten Schulen und Teildisziplinen. Dazu zählen unter anderem die &lt;em&gt;Critical Code Studies&lt;/em&gt;, die ihre Analysen stärker auf Code und Software selbst konzentrieren, die &lt;em&gt;Critical Digital Studies&lt;/em&gt; (vgl. Kroker/Kroker 2013) und der &lt;em&gt;Critical Cultural Studies&lt;/em&gt;. Dies würde zugleich ermöglichen, Gegenstände, Felder und Methoden der Critical Digital Cultural Studies zu differenzieren, wobei hier insbesondere eine klare Bestimmung der Objekte der Kritik, eine klare Differenzierung der Kriterien der Kritik sowie eine selbstreflexive Kritik der Kritik (vgl. auch Ullmaier 2017: 71) unabdingbar erscheinen. Eine solche kritische Form der Geisteswissenschaft, die auf digitales Wissen und digitale Methoden fokussiert und zugleich auf kulturtheoretisches Wissen zurückgreifen kann, wird nicht nur eine kritische Funktion in der Gesellschaft einnehmen können, sondern auch innerhalb der Kulturwissenschaften bestehende Konzepte, Theorien und Methoden problematisieren können – und zugleich auch in die Digital Humanities selbst wirken.&lt;/p&gt;
&lt;p&gt;Die Produktivität eines solchen Engführens von kulturwissenschaftlichen Begriffen der Aufklärung und der kritischen Geisteswissenschaft mit Theoremen, Methoden und Themen der Digital Humanities kann schließlich an einem repräsentativen Beispiel vorgeführt werden. Der Biologe Richard Dawkins führte 1976 den ‚Mem’-Begriff ein, um analog zur Genetik auch die soziokulturelle Evolution begrifflich fassen zu können (vgl. Dawkins 1976). Dieses Konzept hat sich inzwischen auch in den Medien- und Kommunikationswissenschaften sowie teilweise auch in den Kulturwissenschaften durchgesetzt, insbesondere um virale Internetphänomene konzeptionell beschreiben zu können (vgl. u.a. Shifman 2014). Die breite Kritik an der Memtheorie, die unterkomplex sei und viele kultur- und sozialwissenschaftliche Erkenntnisse missachte, kann hier in einer direkten Konfrontation mit der machtkritischen Diskursanalyse Michel Foucaults geleistet werden, die ein wesentlich komplexeres Modell sozialer Entwicklung und Kommunikation zur Verfügung stellt, das über den Diskursbegriff die Produktion, Legitimation und Distribution von Wissen unter den jeweiligen Machtverhältnissen zu beschreiben versucht (Foucault 1995, Foucault 2003).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Literatur&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Zum Begriff der Kritik und zu Theorien und Methoden der Kulturwissenschaft&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Bal, Mieke (2006): &lt;em&gt;Kulturanalyse&lt;/em&gt;. Frankfurt am Main: Suhrkamp.&lt;/p&gt;
&lt;p&gt;Foucault, Michel (1992): &lt;em&gt;Was ist Kritik?&lt;/em&gt; Aus dem Französischen von Walter Seitter. Berlin: Merve.&lt;/p&gt;
&lt;p&gt;Horkheimer, Max/Adorno, Theodor W. (2003): &lt;em&gt;Dialektik der Aufklärung. Philosophische Fragmente&lt;/em&gt;. Frankfurt am Main: S. Fischer.&lt;/p&gt;
&lt;p&gt;Horkheimer, Max (1995): „Traditionelle und kritische Theorie (1937).“ In: Ders.: &lt;em&gt;Traditionelle und kritische Theorie. Fünf Aufsätze&lt;/em&gt;. Frankfurt am Main: S. Fischer, S. 205-259.&lt;/p&gt;
&lt;p&gt;Schäfer, Mirko Tobias/van Es, Karin (Hg., 2017): &lt;em&gt;The Datafied Society. Studying Culture Through Data&lt;/em&gt;. Amsterdam: Amsterdam University Press.&lt;/p&gt;
&lt;p&gt;Ullmaier, Johannes (2017): „Kategorien der Kritik. Detaillierte Inhaltsübersicht zu einer ungeschriebenen Studie.“ In: &lt;em&gt;Testcard. Beiträge zur Popgeschichte. #25: Kritik&lt;/em&gt;. Mainz: Ventil , S. 70-88.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kritische Analysen der digitalen Kultur&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Albrecht, Jan Philipp (2014): &lt;em&gt;Finger weg von unseren Daten! Wie wir entmündigt und ausgenommen werden&lt;/em&gt;. München.&lt;/p&gt;
&lt;p&gt;Engemann, Christoph (2003): &lt;em&gt;Electronic Government – vom User zum Bürger. Zur kritischen Theorie des Internet&lt;/em&gt;. Bielefeld: transcript.&lt;/p&gt;
&lt;p&gt;Lanier, Jaron (2014). &lt;em&gt;Wem gehört die Zukunft? Du bist nicht die Zukunft der Internet-Konzerne. Du bist ihr Produkt&lt;/em&gt;. Hamburg.&lt;/p&gt;
&lt;p&gt;Lovink, Geert (2008): &lt;em&gt;Zero Comments. Elemente einer kritischen Internetkultur&lt;/em&gt;. Bielefeld: transcript.&lt;/p&gt;
&lt;p&gt;Morozov, Evgeny (2013): &lt;em&gt;Smarte neue Welt. Digitale Technik und die Freiheit des Menschen&lt;/em&gt;. München: Blessing.&lt;/p&gt;
&lt;p&gt;Reichert, Ramón (2013): &lt;em&gt;Die Macht der Vielen. Über den neuen Kult der digitalen Vernetzung&lt;/em&gt;. Bielefeld: transcript.&lt;/p&gt;
&lt;p&gt;Schäfer, Mirko Tobias (2011): &lt;em&gt;Bastard Culture! How User Participation Transforms Cultural Production&lt;/em&gt;. Amsterdam: Amsterdam University Press.&lt;/p&gt;
&lt;p&gt;Wagner, Thomas (2017): &lt;em&gt;Das Netz in unsere Hand! Vom digitalen Kapitalismus zur Datendemokratie&lt;/em&gt;. Köln: PapyRossa.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Zum Meme-Begriff und zur Diskurstheorie&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Dawkins, Richard (1976): &lt;em&gt;The Selfish Gene&lt;/em&gt;. Oxford: Oxford University Press.&lt;/p&gt;
&lt;p&gt;Foucault, Michel (1995): &lt;em&gt;Archäologie des Wissens&lt;/em&gt;. Übersetzt von Ulrich Köppen. Frankfurt am Main: Suhrkamp (7. Aufl.).&lt;/p&gt;
&lt;p&gt;Foucault, Michel (2003): &lt;em&gt;Die Ordnung des Diskurses&lt;/em&gt;. Aus dem Französischen von Walter Seitter. Mit einem Essay von Ralf Konersmann. Frankfurt am Main: S. Fischer (9. Aufl.).&lt;/p&gt;
&lt;p&gt;Shifman, Limor (2014): &lt;em&gt;Meme. Kunst, Kultur und Politik im digitalen Zeitalter&lt;/em&gt;. Berlin: Suhrkamp.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>259</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Schaßan, Torsten</p2_authors>
  <p2_organisations>Herzog August Bibliothek Wolfenbüttel, Deutschland</p2_organisations>
  <p2_emails>schassan@hab.de</p2_emails>
  <p2_presenting_author>Schaßan, Torsten</p2_presenting_author>
  <p2_title> Im Netz der Möglichkeiten - Wechselwirkungen in der Entwicklung von Theorie, Methode und Tools in den Digital Humanities am Beispiel der TEI</p2_title>
  <p2_abstract>&lt;p align="center"&gt;&lt;strong&gt; Im Netz der Möglichkeiten&lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt;Wechselwirkungen in der Entwicklung von Theorie, Methode und Tools in den Digital Humanities am Beispiel der TEI&lt;/p&gt;
&lt;p&gt;Im CfP wird die These formuliert, dass die Digital Humanities "häufig als digital transformierte Bearbeitung von Fragestellungen aus den verschiedenen beteiligten Fächern beschrieben" werden und "in weiten Teilen eine daten-, algorithmen- und werkzeuggetriebene Wissenschaft sei[en], die von ihren unmittelbaren Möglichkeiten und ihren Praktiken dominiert" werden, welche den Prinzipien kritischer Wissenschaftlichkeit möglicherweise nicht genüge. Die Korrektheit dieser These soll am Beispiel der Entwicklung der Guidelines sowie der Analyse einzelner konkreter Anwendungsfälle der Text Encoding Initiative untersucht werden. Die Entwicklungsgeschichte wird dabei auf die Wechselwirkungen von Theorie, Methode und Tool-Entwicklung hin betrachtet.&lt;/p&gt;
&lt;p&gt;Der Gegenstand&lt;/p&gt;
&lt;p&gt;Der Beobachtungsgegenstand dieser Untersuchung sind die Guidelines der TEI selbst, die daraus resultierenden Schemata sowie deren Anwendung in ausgewählten Anwendungsfeldern der DH. Die TEI dokumentiert die Entwicklung der Guidelines seit der Einführung 1990 mit der Version P1 in unterschiedlicher Tiefe. Während für die älteren Versionen meist nur deren Endprodukt in Form der DTD bzw. des Textes der Guidelines vorliegt, kann man die Entwicklung des de-facto-Standards seit der Version P4 und dann ab 2007 in listenförmiger Dokumentation der vorgenommenen Änderungen nachvollziehen. Die TEI dokumentiert außerdem die Diskussionen, die in ihren Gremien sowie in der Community über Mailingliste geführt werden in je eigenen Archiven. Damit lässt sich ein nahezu lückenloses Bild der Entwicklungsschritte hin zu der geltenden Version nachvollziehen. Diese Materialien können vor dem Hintergrund der oben genannten Thesen untersucht und mit den traditionellen Wissenschaften in Kontext gesetzt werden, um Aufschlüsse über die Wechselwirkungen zu erhalten.&lt;/p&gt;
&lt;p&gt;Ein weiterer Gegenstand besteht in der konkreten Anwendung der Guidelines bzw. Schemata der TEI in Projekten. Hier sollen beispielhaft Editionsprojekte sowie die Verwendung der TEI zur Speicherung von Metadaten betrachtet werden, um die Wechselwirkungen zu analysieren. Es soll damit der Frage nachgegangen werden, inwieweit die Definition einer Markup-Sprache, die Anwendung bei der Erstellung konkreter Dokumente und die Validierung der Ergebnisse durch technische Rahmenbedingungen vorgegeben oder durch außerhalb der DH liegenden Theoriebildung beeinflusst wird oder selbst doch auch die Theoriebildung und Methodenentwicklung vorantreibt.&lt;/p&gt;
&lt;p&gt;Theoriebildung&lt;/p&gt;
&lt;p&gt;Bereits 1994 wies Sperberg-McQueen auf die theoretischen Implikationen der Textauszeichnung, analog dazu: des Gebrauchs einer Wissenschaftssprache, hin: „Like any notation, the TEI Guidelines inevitably make it easy to express certain kinds of ideas, and concomitantly harder to express other kinds of ideas, about the texts we encode in electronic form. Any notation carries with it the danger that it must favor certain habits of thought --- in the TEI's case, certain approaches to text --- at the expense of others. No one should use TEI markup without being aware of this danger --- any more than we should use the English language, or any other, without realizing that it favors the expression of certain kinds of ideas, and discourages the expression, and even the conception, of other ideas.“ Gleichzeitig wirkt die TEI-Community mit der offenen Diskussion über die semantischen Dimensionen des Markups, mit Best-Practice-Beispielen für die Anwendung diesen Gefahren entgegen. Und beweist immer wieder die Anpassungsfähigkeit, den etablierten Standard im Fall erweiterter Anforderungen an neue Aufgaben anzupassen. Im Sinn Thomas Kuhns können damit Paadigmenwechsel in der Editionsphilologie adaptiert und wissenschaftlich nutzbar gemacht werden, ohne deshalb vorhandene Dokumente oder die bisherige Anwendung des Standards unnütz zu machen. Beispiele hierfür sind die Inkorporation von Markup-Subsystemen für genetische Editionen, zur Beschreibung von Handschriften oder auch zur Dokumentation von Kommunikationsvorgängen, etwa in Briefen, mit der Einführung von &lt;correspDesc&gt;. Das System der TEI eher erweitert als grundlegend verändert. Die Falsifikation von Ergebnissen nach Popper ist daher seltener als der Proof of Concept, mit welchem zunächst die Tauglichkeit von Markup zur Lösung bestimmter Probleme oder die Repräsentation bestimmter Phänomene getestet wird, bevor die Aufnahme in den Standard geschieht und die Angebote durch die Community weiter genutzt werden können.&lt;/p&gt;
&lt;p&gt;Gleichzeitig bestimmen die Anforderungen der inhaltlichen bzw. theoretischen Weiterentwicklung eines Faches die technische und theoretische Weiterentwicklung der TEI mit, allerdings in einer Weise, in der Ursache und Wirkung kaum mehr auseinander zu halten sind. Ein rezentes Beispiel hierfür ist die theoretische Wendung in der Editionsphilologie und den historischen Wissenschaften hin zum Objekt, der in der TEI mit der Einführung von &lt;facsimile&gt; und dem Modul für genetische Editionen begegnet ist. Auch Überlegungen zur Verallgemeinerung des Beschreibungsstandards von Dokumenten und Objekten mittels der Strukturen von &lt;msDesc&gt; wären hier zu nennen.&lt;/p&gt;
&lt;p&gt;Methodenbildung&lt;/p&gt;
&lt;p&gt;Ein Beispiel für die Anwendung neuer Methoden im Rahmen der TEI ist die Anreicherung von Texten um bestimmte Kontexte. Wenn in einem Text benannte Entitäten wie Personen, Orte, Objekte oder Ereignisse ausgezeichnet werden, dann entspricht dies zunächst der klassischen Registerarbeit traditioneller Publikationsverfahren. Wenn diese Entitäten allerdings mit Normdaten angereichert und diese somit zu nachnutzbaren Bestandteilen im Sinne der Linked Open Data werden, darf man wohl von der Anwendung einer neuen Methode sprechen. Die entstandenen Dokumente verändern ihren Charakter von traditionell und händisch ausgewerteten Objekten zu konzeptionell gestalteten und automatisiert nutzbaren. Die Dokumente werden in die Lage versetzt, mit allgemeinen Methoden ausgewertet zu werden, ohne dass sie z.B. auch inhaltlich für einen vorgegebenen Auswertungskontext vorgesehen sind.&lt;/p&gt;
&lt;p&gt;Die Konformität zur TEI spielt hier eine wesentliche Rolle. Im System der TEI ist die &lt;em&gt;Customisation&lt;/em&gt; als Normalfall vorgesehen, also die kontextabhängige Auswahl von Modulen, Elementen und Attributen bzw. die Vorgabe von Wertemengen für Attribute. Diese Vorgaben können allgemein in der TEI in sogenannten ODD-Dateien definiert und dokumentiert werden, um dann anschließend daraus eigene Schemata zu generieren. Zunehmend werden hierbei neben DTDs oder RelaxNG bzw. W3C-Schemata auch Schemtron-Regelwerke definiert, welche eine noch stärkere Kontrolle über den Inhalt eines Dokumentes ermöglicht. ODD sind ein starkes Hilfsmittel, um die Verwissenschaftlichung von textuellem Markup und dem unterliegenden Textverständnis zu erreichen. Wie aber der Grad der TEI-Konformität zu messen oder zu validieren sei, ist in der TEI bereits seit Längerem Gegenstand von Diskussionen.&lt;/p&gt;
&lt;p&gt;Tool-Entwicklung&lt;/p&gt;
&lt;p&gt;Wo nun mehr Texte in TEI-konformer Auszeichnung vorliegen, umso leichter lassen sich Tools dafür entwickeln bzw. vor allem nach nutzen. Wo Texte zu Bestandteilen des LOD werden, wird dies die Entwicklung von Tools befördern. Wo Digitalisierung, egal ob Image- oder Textdigitalisierung, Quellen einfacher zugänglich macht, werden diese Materialien potentiell als Gegenstand der DH nützlich. Ähnlich wie bei der Theorie- oder Methodenbildung befruchten sich zur Verfügung stehende Materialien und daran anzulegende Fragestellungen gegenseitig. Die Kritik der digitalen Methoden wird hier ansetzen müssen, wo das Markup vom Ende er zu denken ist, weil bstimmte Funktionalitäten gewünscht werden: Dokumente sollen nach bestimmten Kriterien sortiert werden? Was bedeutet dies für die Aufbereitung der Daten? – Dokumente sollen im Rahmen des LOD genutzt werden? Welche Eigenschaften müssen sie hierfür haben?&lt;/p&gt;
&lt;p&gt;DH, eine Wissenschaft wie jede andere?&lt;/p&gt;
&lt;p&gt;Abschließend wäre zu fragen, inwieweit sich die DH im bisher dargestellten wirklich von "traditionellen" Wissenschaften unterscheiden? Theorien, Methoden, zur Verfügung stehende Quellen und Verarbeitungsmechanismen haben sich immer schon gegenseitig beeinflusst. An den Beispielen der Untersuchung soll gezeigt werden, dass sich dies in den DH nicht anders ausnimmt.&lt;/p&gt;
&lt;p&gt;Bibliographie&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p&gt;DFG: Förderkriterien für wissenschaftliche Editionen in der Literaturwissenschaft. 2015. http://www.dfg.de/download/pdf/foerderung/grundlagen_dfg_foerderung/informationen_fachwissenschaften/geisteswissenschaften/foerderkriterien_editionen_literaturwissenschaft.pdf&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Guidelines for Electronic Text Encoding and Interchange&lt;/em&gt;, edited by TEI Consortium. 1990-, P1--P5. http://www.tei-c.org/Guidelines/, http://www.tei-c.org/Vault/&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;C. M. Sperberg-McQueen: Textual Criticism and the Text Encoding Initiative. (First draft of a paper presented at MLA, San Diego, 1994) http://www.tei-c.org/Vault/XX/mla94.html&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Patrick Sahle: &lt;em&gt;Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. 3Bde. &lt;/em&gt;Norderstedt 2013.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</p2_abstract>
  <p3_paperID>275</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Pfeiffer, Jasmin</p3_authors>
  <p3_organisations>FAU Erlangen, Deutschland</p3_organisations>
  <p3_emails>jasmin_pfeiffer@gmx.de</p3_emails>
  <p3_presenting_author>Pfeiffer, Jasmin</p3_presenting_author>
  <p3_title>Realität programmieren? Zum Einfluss von Algorithmen auf die Wirklichkeit</p3_title>
  <p3_abstract>&lt;p&gt;Im Mai erregte ein Blog-Artikel von Lisa Ringen Aufsehen, der auf die Benachteiligung weiblicher Personen durch den Suchalgorithmus des Netzwerks Xing hinwies: Gibt ein potentieller Auftraggeber bei seiner Suche nach Freiberuflern nicht explizit die weibliche Form der entsprechenden Berufsbezeichnung ein, so werden ihm ausschließlich Profile von Männern in der Ergebnisliste angezeigt. Möchte man eine Freiberuflerin finden, so muss man etwa nach einer „Fotografin“ oder einer „Entwicklerin“ suchen – eine Idee, die vermutlich die wenigsten User haben werden. Wie Ringen richtig schreibt, werden Frauen hierdurch nicht nur benachteiligt, sondern es entsteht auch „der unterschwellige Eindruck, Männer seien die erfolgreicheren, kompetenteren Fotografie-, Beratungs- und Grafik-Spezialisten“ (Ringen 2017). Bezeichnend scheint das naiv anmutende Statement des Xing-Sprechers Kopka, der, wie die SZ berichtete, betonte, dass ihm das Problem nicht bewusst gewesen sei (Holzki 2017). &lt;/p&gt;
&lt;p&gt;Die Affäre um Xing legt ein Problem offen, das etwas verkürzt zum Titel für einen Artikel von Nosthoff und Maschewski avancierte: „Das Netz ist nie neutral“ (Maschewski / Nosthoff 2017). Dies liegt primär darin begründet, dass, entgegen des oft bedienten Topos der Objektivität von Computerprogrammen, Algorithmen im Allgemeinen nie neutral sind. Wie die Mathematikerin Cathy O’Neil in &lt;em&gt;Weapons of Math Destruction&lt;/em&gt; pointiert aufzeigt, stellen sie vielmehr in Code eingebettete Meinungen dar, die die Weltsicht desjenigen widerspiegeln, der sie verfasst hat (vgl. O’Neil 2016). Nosthoff und Maschewski haben in ihrem oben zitierten Artikel jedoch Statements diverser Größen der Tech-Welt zusammengetragen, die eindeutig zeigen, dass es aktuell noch in vieler Hinsicht an einem Bewusstsein für diese Tatsache mangelt und Algorithmen auch von Experten häufig als neutral angesehen werden. &lt;/p&gt;
&lt;p&gt;Hier, so die These meines Vortrags, besteht ein wichtiger Ansatzpunkt für die Digital Humanities. Die Frage, wie Algorithmen unsere Wirklichkeit verändern und beeinflussen, hat im Rahmen der Debatte um Fake News und den Einfluss von Facebook auf den Ausgang der Wahlen in den USA an Wichtigkeit gewonnen. Wie auch Daniel Lemire betont, lässt sich nicht mehr leugnen, dass Computerprogramme zu wirkungsmächtigen Agenten im sozialen Raum geworden sind: „In any case, we have to accept software as an active agent that helps shape our views and our consumption rather than a mere passive tool.“ (Lemire 2016) Gerade die Geisteswissenschaften besitzen, so meine These, die Kompetenzen und das methodische Handwerkszeug, diese Wirkmacht der Algorithmen zu analysieren und zu beschreiben.&lt;/p&gt;
&lt;p&gt;An dieser Stelle möchte ich mit meinem Vortrag ansetzen und erste theoretische Grundlagen für solche Analysen schaffen, indem ich die Funktionsweise von Programmiersprachen aus geisteswissenschaftlicher Perspektive und mit geisteswissenschaftlichem Vokabular beschreibe. Dabei verfolge ich in meinem Vortrag einen sprachwissenschaftlichen, insbesondere sprachpraktischen Ansatz und verorte mich grob im Feld der Software Studies und der Critical Code Studies. &lt;/p&gt;
&lt;p&gt;Mein Vortrag setzt sich aus vier Teilen zusammen. Im ersten Teil versuche ich, einen Überblick über Programmiersprachen und ihre grundlegenden Funktionsweisen zu geben. Zunächst gehe ich kurz auf die wichtigsten Programmierparadigmen ein, nämlich die imperative, deklarative und die objektorientierte Programmierung, wobei ich den Schwerpunkt auf die aktuell sehr verbreitete objektorientierte Programmierung lege. Im Anschluss stelle ich übersichtlich und verständlich anhand einfacher Beispiele die grundlegende Funktionsweise von objektorientierten Programmiersprachen, insbesondere von Java und C#, vor. Ein solches Basis-Wissen ist für die tiefergehende Untersuchung von Algorithmen und ihrem Einfluss auf die Wirklichkeit unabdingbar, denn wie auch David M. Berry konstatiert, kann die Wichtigkeit eines gewissen Verständnisses von Programmiersprachen kaum überschätzt werden: „Understanding digital humanities is in some sense then understanding code.“ (Berry 2011: 5)&lt;/p&gt;
&lt;p&gt;Im zweiten Teil meines Vortrags konzentriere ich mich weiterhin auf die objektorientierte Programmierung und zeige auf, dass diese immer auf die Herstellung einer Ähnlichkeit zwischen der Struktur des Programmcodes und der echten Welt abzielt: Reale Entitäten werden in Klassen und Objekte abgebildet, wobei zu zeigen sein wird, dass diese Abbildung keine eindeutige, objektive ist, sondern immer auch vom persönlichen Vorwissen und den Erfahrungen des Programmierers abhängt. Welche realen Eigenschaften der Entitäten er als Attribute der Klassen oder Objekte in seinen Code übernimmt, ist das Ergebnis einer gezielte Selektion, die einerseits durch die Erfordernisse des Programmierproblems und andererseits durch die Weltsicht des Programmierers beeinflusst wird. In diesem Teil soll insbesondere deutlich werden, dass der Programmcode keine von unserer Realität separierte, unabhängige Einheit darstellt, sondern in vieler Hinsicht eng mit ihr verwoben ist und zahlreiche Wechselbeziehungen bestehen. &lt;/p&gt;
&lt;p&gt;Im dritten Teil stelle ich einige Überlegungen zur Beziehung zwischen Programmiersprachen und natürlichen Sprachen an. Aufbauend auf ein sprechakttheoretisches Fundament werde ich die Funktionsweise von Programmiersprachen näher analysieren und fragen, inwiefern man das Programmieren als Sprechakt und Sprachhandlung beschreiben kann. Dabei möchte ich insbesondere aufzeigen, dass sich Programmcode am ehesten als Deklaration beschreiben lässt, d. h. als eine Art von Sprechakt, der eine Korrespondenz zwischen Wirklichkeit und Sprache herstellt (vgl. z.B. Searle 1975). Diese deklarativen Sprechakte verändern, so die These, die Wirklichkeit nicht nur auf der maschinellen Ebene der Einsen und Nullen, sondern können auch Einfluss auf Diskurse nehmen und somit, wie Lemire konstatiert, zu aktiven, die Wirklichkeit verändernden Agenten werden. Programmieren soll also ähnlich wie natürliche Sprechakte als eine Art Einschreibung in den Diskurs und als wirklichkeitsschaffend aufgefasst werden. &lt;/p&gt;
&lt;p&gt;In einem dritten Teil werde ich die gewonnen Erkenntnisse an einem Beispiel aus dem Bereich des Machine Learning illustrieren. Durch die zunehmende Verbreitung von verschiedenen Formen künstlicher Intelligenz, die in den kommenden Jahrzehnten zweifelsohne weiterhin an Wichtigkeit gewinnen werden, bekommt die Frage nach dem Wirklichkeitsverständnis des Programmierers eine besondere Relevanz, denn was die Maschine lernt und welche Konzeption der Realität sie hat, ist vollkommen abhängig von dem, was die vom Programmierer erschaffenen Algorithmen ihr beibringen. Dies wird näher erläutert am Beispiel der Entscheidungsbäume: Diese stellen eine einfache Form des induktiven maschinellen Lernens dar. Sie liefern zu Objekten, die durch Mengen von Attribut-Wert-Paaren beschrieben sind, jeweils eine Entscheidung, welcher Klasse das betreffende Objekt zuzuordnen ist. Hier werden die oben umrissenen Fragen besonders virulent: Welche Eigenschaften eine Klasse hat, wird vom Programmierer festgelegt und somit unmittelbar von dessen Wirklichkeitsverständnis beeinflusst. &lt;/p&gt;
&lt;p&gt;In Hinblick auf das Tagungs-Thema, „Kritik der digitalen Vernunft“, verfolgt der Vortrag zweierlei Ziele: Einerseits soll ein möglicher methodischer Ansatzpunkt zur Analyse von Programmiersprachen und Algorithmen entwickelt werden und aufgezeigt werden, dass die genaue Analyse von Code und seiner Funktionsweise unabdingbar für das Verständnis digitaler Phänomene ist. Andererseits soll zu einer gewissen Vorsicht gegenüber digitalen Methoden in den Geisteswissenschaften aufgerufen werden: Die Benutzung digitaler Mittel ist eine große Bereicherung für die Geisteswissenschaft, erfordert aber auch eine eingehende Reflexion der Beziehung zwischen Realität und Programmiersprache und des Einflusses der Programmierung auf den betrachteten Gegenstand. &lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>158</session_ID>
  <session_short>VP_9b</session_short>
  <session_title>Semantische Analyse</session_title>
  <session_start>2018-03-02 11:00</session_start>
  <session_end>2018-03-02 12:30</session_end>
  <session_room_ID>3</session_room_ID>
  <session_room>Hörsaal A2, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Vogeler, Georg</chair1>
  <attendee_count>4</attendee_count>
  <chair1_name>Georg Vogeler</chair1_name>
  <chair1_organisation>ZIM, Universität Graz</chair1_organisation>
  <chair1_email>georg.vogeler@uni-graz.at</chair1_email>
  <chair1_ID>1453</chair1_ID>
  <sessionID>158</sessionID>
  <presentations>3</presentations>
  <p1_paperID>174</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Hamisch, Juliane
Große, Peggy</p1_authors>
  <p1_organisations>Germanisches Nationalmuseum, Nürnberg
Germanisches Nationalmuseum, Nürnberg</p1_organisations>
  <p1_emails>j.hamisch@gnm.de
p.grosse@gnm.de</p1_emails>
  <p1_presenting_author>Hamisch, Juliane
Große, Peggy</p1_presenting_author>
  <p1_title>Interpretation und Unschärfe bei der semantischen Erschließung von historischen Quellen</p1_title>
  <p1_abstract>&lt;p&gt;Im Rahmen von Forschungsprojekten werden die anfallenden Daten in einer virtuellen Forschungsumgebung semantisch erschlossen und unter Berücksichtigung der Fragestellung sowie der gewählten Bearbeitungsmethode für die formalisierte Erfassung und Darstellung eine Modellierung erarbeitet. Bei der Erschließung historischer Quellen werden Bearbeiter häufig mit inhaltlichen Unschärfen konfrontiert, die in der Regel eine Interpretation der Quellen in Hinblick auf die eigene Fragestellung unter Einbeziehung schon vorhandener Forschungsergebnisse erfordern. Es stellt sich folglich das Problem, wie die gewonnenen Forschungsdaten einerseits semantisch erschlossen und formalisiert werden können, um eine möglichst große Vergleichbarkeit der Daten zu gewährleisten, andererseits aber der Vorgang der Interpretation so transparent gehalten werden kann, dass er schließlich auch für Dritte nachvollziehbar bleibt. Dieses Problem soll anhand zweier Projekte erläutert werden, die mit der virtuellen Forschungsumgebung ‚Wissenschaftliche KommunikationsInfrastruktur‘ (WissKI) arbeiten und CIDOC CRM als ‚Semantisches Backend‘ nutzen.&lt;/p&gt;
</p1_abstract>
  <p2_paperID>250</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Nantke, Julia
Schlupkothen, Frederik</p2_authors>
  <p2_organisations>Bergische Universität Wuppertal, Deutschland
Bergische Universität Wuppertal, Deutschland</p2_organisations>
  <p2_emails>nantke@uni-wuppertal.de
schlupko@uni-wuppertal.de</p2_emails>
  <p2_presenting_author>Nantke, Julia
Schlupkothen, Frederik</p2_presenting_author>
  <p2_title>Zwischen Polysemie und Formalisierung: Mehrstufige Modellierung komplexer intertextueller Relationen als Annäherung an ein ‚literarisches‘ Semantic Web</p2_title>
  <p2_abstract>&lt;p&gt;&lt;strong&gt;Kontext und Zielsetzung&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Die Modellierung textueller und transbiblionomer Relationen mithilfe von Semantic Web-Technologien bildet mittlerweile eines der zentralen Forschungsfelder der Digital Humanities. Im Vergleich mit anderen geisteswissenschaftlichen Bereichen wie der Geschichte oder der Philosophie stellt die Literatur ein Feld dar, auf dem die mit der computergestützten Modellierung verbundene Formalisierung zu einer besonderen Herausforderung wird. Dies ist in der spezifischen Struktur und Funktionsweise literarischer Texte begründet, deren interne und externe textuelle Beziehungen in Form komplexer Zeichenrelationen bestehen, die plurale, sich auf verschiedenen Ebenen überlagernde Bedeutungsangebote stiften. Diese können nicht – wie etwa im Falle historischer Quellen – auf verortbare Ereignisse oder ein konkretes argumentatives Ziel bezogen werden. Dieser Herausforderung begegnet das Projekt, welches in dem vorgeschlagenen Beitrag vorgestellt werden soll, indem auf der Basis eines situationstheoretischen Formalismus (Barwise/Perry 1983, Devlin 1990) ein mehrstufiges Modell zur Abbildung und Beschreibung komplexer intertextueller Relationen zwischen literarischen Texten entwickelt wird.&lt;/p&gt;
&lt;p&gt;Das Projekt möchte damit in zweierlei Hinsicht einen Beitrag zur methodologischen Reflexion leisten: Zum einen streben wir mit dieser zunächst auf das genauere Verständnis intertextueller Phänomene gerichteten stufenweisen Modellierung[1] einen literaturtheoretisch reflektierten Einsatz digitaler Methoden an. Zum anderen trägt das Vorgehen bei der Modellierung ebenso zu einer Schärfung der literaturwissenschaftlichen Perspektive auf Intertextualität und zur fundierten Beschreibung hierbei wirksamer Faktoren bei. Ziel des Projekts ist eine maschinenlesbare Systematisierung intertextueller „Schreibweisen“ (Verweyen/Wittig, S. 38)[2] sowie der Kriterien zur Isolierung und Charakterisierung der Schreibweisen. Diese soll eine computergestützte Erschließung literarischer Intertextualität ermöglichen.&lt;/p&gt;
&lt;p&gt;Der Beitrag möchte zum einen konkrete Probleme diskutieren, welche sich im Spannungsfeld zwischen literarischer Polysemie, der Literaturwissenschaft inhärenter Perspektivenvielfalt und technischer Normierung ergeben, denn das Projekt dient nicht zuletzt auch der Reflexion der Möglichkeiten zur Formalisierung literaturwissenschaftlicher Erkenntnisse sowie dem Ausloten der Grenzen für den Einsatz formaler Beschreibungssprachen im Hinblick auf literaturwissenschaftliche Forschungsfragen.&lt;/p&gt;
&lt;p&gt;Zum anderen soll dargestellt werden, wie durch die spezifische Anlage der Modellierung auf verschiedenen Ebenen Desideraten bisheriger Ansätze begegnet und gleichzeitig ein Beitrag zum literaturtheoretisch fundierten Einsatz von DH-Methoden geleistet werden kann.&lt;/p&gt;
&lt;p&gt;Erste Ergebnisse sollen in dem vorgeschlagenen Beitrag anhand konkreter Beispiele wie etwa des intertextuellen Netzes um Matthias Claudius’ &lt;em&gt;Rheinweinlied&lt;/em&gt; präsentiert werden, welches aufgrund der dem Netz inhärenten Vielzahl intertextueller Phänomene bei gleichzeitig relativ kurzen Texten hierfür besonders geeignet erscheint.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stand der Forschung und Abgrenzung&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In vielen bisherigen Projekten zur Erfassung literarischer Beziehungen schränkt die Konzentration auf automatisierbare Analysevorgänge das heuristische Potential digitaler Modellierung für die literaturwissenschaftliche Forschung in verschiedener Hinsicht ein:&lt;/p&gt;
&lt;p&gt;Erstens werden Modelle zur Beschreibung (innertextueller) literarischer Strukturen an stark Plot-lastigen Texten entwickelt,[3] was die Herausforderung der vielfältigen Bedeutungsebenen komplexerer literarischer Texte deutlich reduziert. Die Modelle erscheinen deshalb kaum auf den Großteil der literaturwissenschaftlich relevanten Beispiele übertragbar.&lt;/p&gt;
&lt;p&gt;Zweitens erfolgt eine Modellierung intertextueller Beziehungen anhand einer „historische[n] Positivtät von Kontext-Dokumenten“ (Wagner/Mehler/Biber 2016, S. 90 mit Bezug auf das Projekt Wikidition), deren Verknüpfungen auf linguistischer Ebene modelliert werden. Auf diese Weise werden zwar viele Probleme im Hinblick auf die Intersubjektivierbarkeit der Modellierung und die Differenzen zwischen verschiedenen Intertextualitätskonzepten vermieden, gleichzeitig wird aber aus literaturwissenschaftlicher Sicht die Aussagekraft der Ergebnisse stark eingeschränkt, indem der literaturwissenschaftlich relevante Fokus auf die Kategorisierung, Funktion und Wirkung von Intertextualität und die hierbei produktiven Schreibweisen und Markierungen (vgl. Kocher 2007, 179) zugunsten einer eher enzyklopädischen Perspektive verloren geht.[4]&lt;/p&gt;
&lt;p&gt;Unser Projekt richtet sich hingegen weder auf die automatisierte Textanalyse noch beschränkt es sich auf die linguistische Ebene konkreter Wortäquivalenz. Vielmehr steht die Entwicklung eines formalisierten Vokabulars zur semantischen Repräsentation literarischer Intertextualität im Zentrum des Forschungsinteresses. Die Isolierung und formale Beschreibung intertextueller Phänomene dient der Beobachtung und Darstellung des Zusammenwirkens jener Schreibweisen und Markierungen bei der Erzeugung von Intertextualität. Die intertextuellen Beziehungen werden dabei also nicht deduktiv im Sinne einer Qualifizierung als Parodie, Kontrafaktur, Nachahmung, Hommage etc. modelliert, da derartige ‚Gattungszuschreibungen‘ in systematischen literaturwissenschaftlichen Untersuchungen zur Typisierung von Intertextualität oftmals den Blick für die spezifischen, bei der Erzeugung von Intertextualität wirksamen Faktoren verstellen. Die angeführten literarischen Beispiele dienen dann eher der selektiven Untermauerung der jeweils präfigurierten Typologie (vgl. einschlägig Broich/Pfister 1985; Genette 1993). Im Gegensatz dazu bildet die Modellierung von Beziehungen zwischen konkreten Schreibweisen den Ausgangspunkt unseres Projekts, welcher im Anschluss Schlussfolgerungen über die Relationen der verschiedenen Ebenen, auf denen intertextuelle Verknüpfungen stattfinden, sowie über die jeweils erzeugten Wirkungen ermöglichen soll.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Methodik&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Für das angestrebte Forschungsziel stellt die mehrstufige Modellierung einen expliziten heuristischen Gewinn gegenüber bisherigen Ansätzen der Systematisierung dar, indem die umfassende induktive Erfassung und Beschreibung sowie die anschließend abgeleiteten strukturellen Konstanten nicht unverbunden nebeneinander stehen, sondern Mikro- und Makrostrukturen durch das Modell in ihrem Zusammenhang beobachtbar gemacht werden. Indem zunächst Einzeltextphänomene modelliert, darauf aufbauend deren Funktionen im Sinne ihres Beitrags zur „Bedeutungskonstitution“ (Hempfer 1991, S. 19) erfasst und daraus übergreifende Kategorien abgeleitet werden, können zwei bislang getrennt voneinander verhandelte Bereiche der Untersuchung von Intertextualität in einer ganzheitlichen Modellierung verbunden werden: die ‚Entwirrung‘ des intertextuellen Gefüges eines einzelnen Textes (vgl. hierfür exemplarisch Bauer Lucca 2001; Dudzik 2017) sowie die übergeordnete Suche nach gemeinsamen Strukturen und Funktionsweisen intertextueller Verweise. Das vorgestellte Modell verknüpft also die in der Literaturwissenschaft seit den 1980er Jahren unternommenen Bestrebungen zur Typologisierung intertextueller Strukturen und Funktionsweisen mit einer umfassenden Detailuntersuchung literarischer Texte. Die Differenzierung in Phänomenbeschreibung und ‑bewertung, welche der eingesetzte Formalismus unterstützt (s. u.), sieht explizit die Modellierung funktionaler Überlagerungen und alternativer Forschungsmeinungen vor, sodass im Rahmen der Formalisierung sowohl der Multifunktionalität intertextueller Schreibweisen (vgl. Kocher 2010, S. 179) als auch der maßgeblich auf produktivem Dissens basierenden Dynamik des literaturwissenschaftlichen Diskurses Rechnung getragen wird.&lt;/p&gt;
&lt;p&gt;Als Ausgangspunkt zur formalen Beschreibung intertextueller Phänomene dient ein situationstheoretischer Ansatz, welcher die Brücke zwischen literaturwissenschaftlicher Analyse und technischer Modellierung darstellt. Die Situationstheorie bietet sich an, da sie im Sinne der angestrebten Beschreibung der Intertextualität einen mehrstufigen Formalismus zur Verfügung stellt, welcher Informationen und Informationsflüsse in Kontextabhängigkeit beschreibt: Basale Phänomene werden durch basale Informationseinheiten (sog. „Infone“) beschrieben, welche wiederum „Situationen“ als Phänomene einer höheren Ordnung aus der Perspektive eines oder mehrerer „Agenten“ zu bilden erlauben.&lt;/p&gt;
&lt;p&gt;Somit kann formal unterschieden werden zwischen der Modellierung konkreter (sprachlicher, inhaltlicher, stilistischer) Texteigenschaften (beschrieben als Infone) und der Klassifizierung der modellierten Informationseinheiten im Sinne ihrer Funktion  sowie einer durch sie indizierten, Kontext-abhängigen Wirkung (beschrieben als Situationen). Im Gegensatz zu technischen Beschreibungssprachen (wie etwa RDF oder OWL) liefert die Situationstheorie einen Formalismus, welcher zunächst frei von umsetzungsspezifischen Einschränkungen ist, die sich ungewollt perspektivierend auf die Modellierung auswirken können.[5] Für die Beschreibung literarischer Texte erweist sich der situationstheoretische Formalismus also als besonders geeignet, da er Modellierungsfreiheit mit der für die technische Umsetzung notwendigen formalen Strenge vereint. Das Modell wird sukzessive unter Einbezug einer wachsenden Anzahl literarischer Texte konkretisiert und weiterentwickelt.&lt;/p&gt;
&lt;p&gt;An diese sukzessive formale Strukturierung anknüpfend wird geprüft, inwieweit etablierte Beschreibungssprachen bei einer technischen Umsetzung des Modells Anwendung finden können. Insbesondere etablierte Sprachen aus dem Umfeld elektronischer Publikation sollen auf ihre Anwendbarkeit bzw. Möglichkeiten der Erweiterung hin betrachtet werden. Dies sind im Rahmen der durch das W3C beschriebene Standards für Verweisstrukturen Sprachen wie XPath, XLink oder XPointer (vgl. einschlägig Wilde/Lowe 2003), für semantische Auszeichnungen die Sprachen des Semantic Web wie RDF oder OWL. Dies schließt – unabhängig von der konkreten Sprache – die Berücksichtigung unterschiedlicher Auszeichnungskonzepte wie bspw. Standoff- in Abgrenzung zu Inline-Markup ein (vgl. Banski 2010).[6]&lt;/p&gt;
&lt;br clear="all" /&gt;
&lt;p&gt;[1] Vgl. zur Unterscheidung zwischen „modeling for understanding“ und „modeling for production“ Eide 2014.&lt;/p&gt;

&lt;p&gt;[2]„Schreibweisen“ ist hierbei nicht intentionalistisch, sondern im Sinne von Textstrukturen mit Verweisfunktion zu verstehen.&lt;/p&gt;

&lt;p&gt;[3] Vgl. hierzu u. a. die Ausführungen zur visuellen Analyse in John u. a. 2016. Einen komplexeren, narratologisch ausgerichteten Ansatz verfolgt das Projekt heureCLÉA (vgl. hierzu Gius/Jacke 2015). Das Projekt ist daher in seiner Orientierung an konkreten literaturwissenschaftlichen Methoden beispielhaft, verfolgt aber mit seiner Ausrichtung auf innertextuelle Strukturen ein grundsätzlich anderes Ziel als unser Projekt.&lt;/p&gt;

&lt;p&gt;[4] Wagner/Mehler/Biber 2016, S. 90 verstehen ihr Projekt daher auch eher im Sinne einer Vorstufe für die „Erschließung des intertextuellen Potentials eines je gegebenen literarischen Texts“. Ihr methodischer Ansatz „zielt nicht auf die &lt;em&gt;Implementierung&lt;/em&gt; literarischer Intertextualität“.&lt;/p&gt;

&lt;p&gt;[5] Im Gegensatz dazu ist bspw. der in Heßbrüggen-Walter 2015 dargestellte Ansatz unmittelbar RDF-basiert gedacht.&lt;/p&gt;

&lt;p&gt;[6] Zahlreiche rechnergestützte, aber proprietäre Anwendungen wurden im Verlauf der Entwicklung der Situationstheorie vorgestellt (vgl. einschlägig etwa Tin/Akman 1994). Neuere Ansätze schlagen die Verwendung etablierter Sprachen, wie insb. durch das Semantic Web gegeben, vor (Kokar/Matheus/Baclawski 2009).&lt;/p&gt;

</p2_abstract>
  <p3_paperID>254</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Knoth, Alexander
Stede, Manfred
Hägert, Erik</p3_authors>
  <p3_organisations>Universität Potsdam, Deutschland
Universität Potsdam, Deutschland
Universität Potsdam, Deutschland</p3_organisations>
  <p3_emails>alexander.knoth@uni-potsdam.de
stede@uni-potsdam.de
haegert@uni-potsdam.de</p3_emails>
  <p3_presenting_author>Knoth, Alexander
Hägert, Erik</p3_presenting_author>
  <p3_title>Dokumentenarbeit mit hierarchisch strukturierten Texten: Eine historisch vergleichende Analyse von Verfassungen</p3_title>
  <p3_abstract>&lt;p&gt;Der Beitrag widmet sich der historisch vergleichenden Untersuchung von Verfassungen als einer besonderen Art von strukturierten Dokumenten. In der Perspektivenverschränkung von historischer Soziologie und Computerlingistik wird der Frage nachgegangen, inwiefern sich Verfassungen anhand ihres (formal) strukturierten Aufbaus computergestützt in historisch-fallvergleichender Perspektive analysieren lassen. Mit anderen Worten: Wie können hierarchische Textstrukturen und Zeit angemessen für die Dokumentenarbeit modelliert werden? Ausgehend von einer Reflexion bestehender dokumentenanalytischer Verfahren wird zunächst nach den methodologischen Voraussetzungen des Dokumentenvergleichs gefragt, bevor dann auf die Entwicklung unserer Untersuchungssoftware und die konkreten Verfahrensschritte, d.h. auf die Extraktion der Verfassungen als HTML-Dokumente, deren Überführung und Auszeichnung als XML-Dokumente und das Softwaretool sowie dessen Impulse für neue methodische Ansätze genauer eingegagen wird. Den Abschluss bilden bisherige Entwicklungsergebnisse sowie ein Ausblick.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>159</session_ID>
  <session_short>VP_9c</session_short>
  <session_title>Annotation</session_title>
  <session_start>2018-03-02 11:00</session_start>
  <session_end>2018-03-02 12:30</session_end>
  <session_room_ID>4</session_room_ID>
  <session_room>Hörsaal C, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Fischer, Frank</chair1>
  <attendee_count>5</attendee_count>
  <chair1_name>Frank Fischer</chair1_name>
  <chair1_organisation>Higher School of Economics, Moskau</chair1_organisation>
  <chair1_email>frafis@gmail.com</chair1_email>
  <chair1_ID>1040</chair1_ID>
  <sessionID>159</sessionID>
  <presentations>3</presentations>
  <p1_paperID>185</p1_paperID>
  <p1_contribution_type>Vortrag</p1_contribution_type>
  <p1_acceptance>Akzeptiert</p1_acceptance>
  <p1_authors>Zirker, Angelika</p1_authors>
  <p1_organisations>Eberhard Karls Universität Tübingen, Deutschland; Humboldt Universität zu Berlin</p1_organisations>
  <p1_emails>angelika.zirker@uni-tuebingen.de</p1_emails>
  <p1_presenting_author>Zirker, Angelika</p1_presenting_author>
  <p1_title>Ambiguität und Annotation: Herausforderungen von Automatisierung und Digitalität </p1_title>
  <p1_abstract>&lt;p&gt;Der Vortrag beruht auf Überlegungen zur Theorie und Praxis der erklärenden Annotation literarischer Texte. Im Vordergrund steht also weniger die digitale Aufbereitung (im Sinne von Markup) als die Anreicherung von Texten durch Annotationen, wie sie in TEASys (Tübingen Explanatory Annotations System; s. Bauer / Zirker 2015) entwickelt wurden. TEASys bietet erklärende Annotationen auf drei verschiedenen Komplexitätsebenen und ist strukturiert nach Kategorien wie sprachliche Erklärungen, Kontextinformationen, Intertextualität, intratextuelle Verweise, formale Aspekte, textphilologische Anmerkungen und Interpretationen. Das Aufkommen digitaler Annotationen eröffnet neue Möglichkeiten für die Gestaltung der erklärenden Annotationen von Texten, die es noch zu entdecken und aufzubereiten gilt. Ein wesentlicher Faktor liegt dabei im Informationsmanagement, insbesondere im Unterschied zu Annotationen im gedruckten Buch: das digitale Medium erlaubt eine schier unbegrenzte Menge an Informationen sowohl hinsichtlich der dargebotenen Inhalte wie auch durch Hyperlinks. TEASys wurde für die Annotation literarischer Texte entwickelt, soll langfristig aber auch für die Informationsanreicherung und Erläuterung nicht-literarischer Texte und anderer Disziplinen herangezogen werden (vgl. Bauer / Zirker 2015). Es wurde bei der DHd 2016 und 2017 bereits vorgestellt. Seither hat sich TEASys vor allem technisch weiterentwickelt: Die online bereitgestellten Annotationen werden in einer Datenbank gespeichert, die bei der Neuanlage von Annotationen in einem Text auch Vorschläge automatisiert anbietet. Der Vortrag für die DHd 2018 ergibt sich aus dieser technischen Weiterentwicklung des Projekts hinsichtlich eines theoretischen Problems, nämlich der Frage, wie in erklärenden Annotationen mit Ambiguität, d.h. sprachlicher aber auch textueller Mehrdeutigkeit, umzugehen ist, und widmet sich vor allem den Herausforderungen, die sich aus der Automatisierung von Annotationen im Zusammenhang mit Ambiguität ergeben.&lt;/p&gt;
&lt;p&gt;Obwohl es sich bei Praxis der erklärenden Annotation um eine der ältesten Kulturtechniken bei der Aufbereitung von (literarischen) Texten handelt, wurden die theoretischen Probleme und Herausforderungen der erklärenden Annotation bisher nicht systematisch behandelt (vgl. Assmann 1995; Eggert 2009; van Peursen 2010; Drucker 2012; Parry 2012; Zirker / Bauer 2017). Dazu gehört insbesondere die Frage nach dem Verhältnis von Textteilen und Textganzem, Text und Kontext, Erklärung und Interpretation. Ambiguität ist für all diese Aspekte hoch relevant: die Annotation etwa eines Ausdrucks oder eines Textauszugs, z.B. in einem Roman, kann dazu beitragen, die Verbindung zwischen lokaler und globaler Textbedeutung zu zeigen. Ein Fallbeispiel dafür ist etwa die Geistergeschichte „To Be Taken with a Grain of Salt“ von Charles Dickens: diese Erzählung ist in die Sammlung &lt;em&gt;Doctor Marigold’s Prescriptions&lt;/em&gt; eingebettet, woraus die Ambiguität des Titels resultiert, d.h. er kann wörtlich wie auch metaphorisch gelesen werden (Zirker 2014). Dies muss von einer Annotation entsprechend erläutert werden. Die Datenbank kann beispielsweise auf weitere (Kon)Texte der Verwendung und damit auf ein Spektrum möglicher Bedeutungen hinweisen, die wiederum auf den Text (zurück)bezogen werden können.[1]  &lt;/p&gt;
&lt;p&gt;Die Ambiguität – oder Mehrdeutigkeit von Wörtern, Ausdrücken, Sätzen, ganzen Texten – stellt somit eine besondere Herausforderung bei der digitalen erläuternden Annotation dar, vor allem wenn Annotationen automatisiert werden (s. dazu auch Gius / Jacke 2017). Ein Negativbeispiel für die automatisierte Annotation literarischer Texte findet sich bei Amazon x-ray (vgl. Bauer / Zirker 2017): dort werden häufig falsche Annotationen angeboten oder es wird bei der Weiterleitung auf die Disambiguierungsseiten von Wikipedia Wissen vorausgesetzt, das dem Textverstehen bereits zugrunde liegt. In TEASys tritt ein anderes Problem hervor: bei der Anlage neuer Annotationen werden dem Annotator aus der Datenbank Vorschläge zu dem Item aus der zugrunde liegenden Datenbank unterbreitet. Im Falle von Ambiguität tritt hier nun die Schwierigkeit auf, dass Textverstehen vorausgesetzt wird, um die ‚richtige‘ Annotation im jeweiligen Kontext zu wählen. Nimmt man etwa die Phrase „Let me not“, die Shakespeares 116. Sonett einleitet, so kann sie sowohl von Sprecher an sich selbst gerichtet sein (analog zu einem Soliloquium) oder aber an einen Adressaten (im Sinne eines Imperativs). Die hier vorliegende Ambiguität hinsichtlich der Kommunikationssituation ist jedoch nicht automatisch auf andere (Kon)Texte übertragbar: „Let me not“ wird auch von Hamlet in einem Soliloquium verwendet (Akt 1, Sz. 2) bzw. von Brutus in &lt;em&gt;Julius Caesar&lt;/em&gt; in einem Dialog mit Cassius (Akt 1, Sz. 2). Eine Automatisierung wie auch Rekontextualisierung der Annotation zu „Let me not“ ist deshalb schwierig gerade &lt;em&gt;aufgrund&lt;/em&gt; ihres Potentials, mehrdeutig zu sein. Dies bedeutet aber auch, dass im Fall der Eindeutigkeit des Ausdrucks die Metadaten der jeweiligen Annotationen auf die Bedingungen für eine solche Disambiguierung verweisen sollten, damit dieser Hinweis beim Erstellen weiterer Annotationen erhältlich und nützlich bleibt.&lt;/p&gt;
&lt;p&gt;Doch was passiert, wenn Annotationen Ambiguität berücksichtigen? Hierzu gibt es drei Szenarien: (1) Die erklärende Annotation disambiguiert die Textstelle / den Text. (2) Die erklärende Annotation weist den Nutzer auf die Ambiguität hin, insbesondere in einem literarischen Text (Bode 1988), und bietet distinkte Denotationen an. (3) Die erklärende Annotation führt (in strategischer Weise; s. dazu Bauer / Zirker, in Vorb.) die Wahrnehmung einer Ambiguität ein, die tatsächlich vorliegt, oder eben auch nicht. Im Fall von (1) kann dies in einer Vereindeutigung des Textes resultieren, die besondere Qualitäten literarischer Werke außer Acht lässt; möglicherweise ermöglicht dies aber auch eine klare Interpretation. Die Disambiguierung mag sogar erforderlich sein, etwa im Zuge von Sprachwandel (im Englischen denke man hier z.B. an die heute weniger geläufigen Bedeutungen von „gay“ und „nice“, die im 18. Jahrhundert völlig andere Denotationen besaßen). Im Fall von (2) kann die Annotation einer Ambiguität die ästhetischen Merkmale eines Textes in besonderer Weise hervortreten lassen, die sonst in den Hintergrund gerückt werden, bspw. die Ambiguität zwischen interner und externer Kommunikationsebene (man denke hier an dramatische Ironie). Im Fall von (3) kann der Annotator eine Erklärung eines potentiell ambigen Items anbieten, die das globale Textverstehen beeinflusst. Dies trifft etwa auf biographische Lesarten der Sonette Shakespeares zu, wo lokale Ambiguitäten strategisch in die Texte hineingelesen werden. Im 145. Sonett ist z.B. von „hate away“ die Rede, was in einer Annotation als Verweis auf Shakespeares Frau, Ann Hathaway, interpretiert wird (Booth 1977: 501).&lt;/p&gt;
&lt;p&gt;Die theoretischen Probleme, die an diese Überlegungen und kurzen Fallbeispiele anknüpfen, wurden bisher nicht systematisch reflektiert. Dies gilt auch hinsichtlich der Frage, inwieweit erklärende Annotationen der Komplexität und Ambiguität literarischer Texte gerecht werden können. Und während das digitale Medium und der damit (scheinbar) unbegrenzte Raum für Annotationen neue Möglichkeiten für die Lösung dieser Fragen und Probleme bietet, resultieren daraus aber auch wiederum neue Herausforderungen, nämlich die Reflexion über den Mehrwert und den Verlust, der sich aufgrund der geschilderten hermeneutischen Schwierigkeiten ergibt (s. auch Gius / Jacke 2015 und 2017). Wendet man sich nun der digitalen erklärenden Annotation im Verhältnis zu Ambiguität zu, so resultiert daraus – wie in den Fällen (1) bis (3) dargelegt – ein &lt;em&gt;trade-off&lt;/em&gt; zwischen Klärung und Verdunkelung sowie die Notwendigkeit eines Mittelwegs zwischen der Überforderung bzw. Überfrachtung des Nutzers und der Gefahr, zu wenig Informationen anzubieten (s. dazu Berry 2012).&lt;/p&gt;
&lt;p&gt;Der Vortrag widmet sich diesen Fragen und Herausforderungen anhand einiger ausgewählter Beispiele und versucht, folgende Aspekte zu präsentieren bzw. anzureißen: 1. die theoretischen Grundlagen der erklärenden Annotation und ihrer hermeneutischen Grundlagen im Hinblick auf die Ambiguität näher zu beleuchten; 2. das Verhältnis von digitalem Medium und literarischer Annotation weiter zu konzeptualisieren, und zwar vor dem Hintergrund der erschwerten Bedingungen durch Ambiguität; 3. die Automatisierung von Annotationen ambiger Items im digitalen Medium in theoretischer Hinsicht  voranzutreiben.&lt;/p&gt;
&lt;br clear="all" /&gt;
&lt;p&gt;[1] Die Phrase „grain of salt“ wird z.B. von Henry James in The American metaphorisch verwendet (Kap. 9), während sie von Sir Alfred Tennyson in der letzten Zeile seines Gedichts „Will“ zwar literal gebraucht, durch den Kontext ihrer Verwendung die Metaphorik durch die Rätselhaftigkeit des Ausdrucks „The city sparkles like a grain of salt“ aber aufgerufen wird.&lt;/p&gt;

</p1_abstract>
  <p2_paperID>191</p2_paperID>
  <p2_contribution_type>Vortrag</p2_contribution_type>
  <p2_acceptance>Akzeptiert</p2_acceptance>
  <p2_authors>Gius, Evelyn
Reiter, Nils
Strötgen, Jannik
Willand, Marcus</p2_authors>
  <p2_organisations>Universität Hamburg, Deutschland
Universität Stuttgart, Deutschland
Max-Planck-Institut für Informatik, Saarbrücken, Deutschland
Universität Stuttgart, Deutschland</p2_organisations>
  <p2_emails>evelyn.gius@uni-hamburg.de
nils.reiter@ims.uni-stuttgart.de
jannik.stroetgen@mpi-inf.mpg.de
marcus.willand@ilw.uni-stuttgart.de</p2_emails>
  <p2_presenting_author>Gius, Evelyn
Reiter, Nils</p2_presenting_author>
  <p2_title>SANTA: Systematische Analyse Narrativer Texte durch Annotation</p2_title>
  <p2_abstract>&lt;p dir="ltr"&gt;In diesem Beitrag wollen wir ein Vorhaben zur Diskussion stellen, das an zwei zentralen Herausforderungen in den Digital Humanities ansetzt: Der Erstellung adäquater Annotationsrichtlinien für geisteswissenschaftlich relevante textuelle Konzepte und der Schnittstelle in der Kooperation zwischen beteiligten Wissenschaftlerinnen und Wissenschaftlern aus Geisteswissenschaft und Informatik. Für DH-Projekte sind Kooperationen unerlässlich, wenn fortgeschrittene Techniken zur Textanalyse eingesetzt werden und/oder es um eine Zusammenführung von Konzepten oder Zugangsweisen geht, die bereits intradisziplinär als komplex gelten. Dabei wird ein signifikanter Anteil der Projektlaufzeit auf die Entwicklung einer “gemeinsamen Sprache” und die Identifikation der exakten, gemeinsamen wissenschaftlichen Fragestellung verwendet. Dies ist zweifellos ein produktiver Prozess, dessen erfolgreiche Durchführung allerdings voraussetzt, dass auf beiden Seiten Forscherinnen und Forscher beteiligt sind, die sich auf das interdisziplinäre Vorgehen voll einlassen und auch den nötigen Zeitaufwand tragen.&lt;/p&gt;
&lt;p dir="ltr"&gt;Methodisch-technisch ist ein substanzielles Nadelöhr bei der Entwicklung automatischer Werkzeuge das Fehlen von annotierten Goldstandards, an/auf denen Werkzeuge trainiert, verglichen und feinjustiert werden können. Das Fehlen der Goldstandards ist jedoch eigentlich ein nachgelagertes Problem, wie sich z.B. in narratologisch orientierten Projekten zeigt (heureCLÉA: Bögel et al., 2015; Propp annotation: Fisseni et al., 2014): Die Umsetzung narratologischer Theorien als Annotationen ist alles andere als trivial, da narratologische Konzepte nicht im Hinblick auf Annotation entwickelt wurden. Leerstellen in den Definitionen müssen gefüllt, Voraussetzungen geklärt und Unterkategorien geklärt werden. Die Annotation solcher Kategorien ist also kein reiner Umsetzungs- oder Implementierungsprozess, sondern einer bei dem sich tiefe, konzeptionelle Fragen stellen. Als Ergebnis solcher Prozesse stehen dann Annotationsrichtlinien, die die Brücke zwischen Theorie und Praxis schlagen. Erst wenn Annotationsrichtlinien für ein Phänomen (oder eine Gruppe von Phänomenen) etabliert sind, können größere Annotationsprojekte mit Aussicht auf Erfolg durchgeführt werden.&lt;/p&gt;
&lt;p dir="ltr"&gt;Das von uns vorgeschlagene Vorgehen erlaubt den Beteiligten Forscherinnen und Forschern ihre Expertise einzubringen, ohne in einem gemeinsamen Projektkontext zu arbeiten. Die Schnittstelle zwischen D und H wird hierbei von annotierten Daten und Annotationsrichtlinien gebildet, wobei die Richtlinien ohne Kompromisse bezüglich möglicher Automatisierungen erstellt werden. Das Vorhaben gibt somit auch narratologisch/literaturwissenschaftlich anspruchsvoller Konzeptentwicklung und damit Theoriebildung einen Rahmen. Verfügbare annotierte Daten wiederum erlauben Informatikerinnen und Informatikern ohne Expertise in narratologischen Fragen die Entwicklung von Werkzeugen für komplexe technische Probleme.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</p2_abstract>
  <p3_paperID>195</p3_paperID>
  <p3_contribution_type>Vortrag</p3_contribution_type>
  <p3_acceptance>Akzeptiert</p3_acceptance>
  <p3_authors>Roeder, Torsten</p3_authors>
  <p3_organisations>Universität Würzburg, Deutschland</p3_organisations>
  <p3_emails>mail@torstenroeder.de</p3_emails>
  <p3_presenting_author>Roeder, Torsten</p3_presenting_author>
  <p3_title>Horizontales Lesen: Das "Verdi-Requiem" und die deutsche Kritik</p3_title>
  <p3_abstract>&lt;p&gt;Dieser Beitrag stellt eine Methode vor, die hier »horizontales Lesen« genannt wird. Darin wird ein heterogenes, semantisch tief erschlossenes Textkorpus zur Analyse verwendet. Das Anwendungsbeispiel ist die &lt;em&gt;Messa da Requiem&lt;/em&gt; des italienischen Komponisten Giuseppe Verdi (1813–1901) und die deutsche Kritik in den 1870er Jahren. Durch einen flächendeckenden Vergleich von Kontexten auf der Basis eines TEI-Korpus war es möglich, allgemeine Tendenzen der Rezeption zu identifizieren und zu verorten, auch wenn die individuellen Hintergründe der Autoren nicht immer bekannt sind. Das Verfahren entstand im Rahmen einer kürzlich eingereichten Dissertation im Fach Musikwissenschaft.&lt;/p&gt;
</p3_abstract>
 </session>

 <session>
  <session_ID>78</session_ID>
  <session_title>Mittagspause</session_title>
  <session_start>2018-03-02 12:30</session_start>
  <session_end>2018-03-02 14:00</session_end>
  <attendee_count>3</attendee_count>
 </session>

 <session>
  <session_ID>177</session_ID>
  <session_title>Führung durch das VR-Labor des Regionalen Rechenzentrums Köln</session_title>
  <session_start>2018-03-02 13:00</session_start>
  <session_end>2018-03-02 13:30</session_end>
  <session_info>maximale Teilnehmerzahl: 10 &lt;br&gt;
Anmeldung nur vor Ort im Konferenzsekretariat. </session_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>178</session_ID>
  <session_title>Führung durch das VR-Labor des Regionalen Rechenzentrums Köln</session_title>
  <session_start>2018-03-02 13:30</session_start>
  <session_end>2018-03-02 14:00</session_end>
  <session_info>maximale Teilnehmerzahl: 10 &lt;br&gt;
Anmeldung nur vor Ort im Konferenzsekretariat. </session_info>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>92</session_ID>
  <session_title>Abschluss von DHd 2018 und Abschluss-Keynote - C. M. Sperberg-McQueen (PhD)</session_title>
  <session_start>2018-03-02 14:00</session_start>
  <session_end>2018-03-02 15:30</session_end>
  <session_room_ID>1</session_room_ID>
  <session_room>Hörsaal B, Hörsaalgebäude</session_room>
  <session_room_info>Hörsaalgebäude</session_room_info>
  <chair1>Witt, Andreas</chair1>
  <chair2>Moulin, Claudine</chair2>
  <session_info>Kritik der digitalen Vernunft</session_info>
  <attendee_count>9</attendee_count>
  <chair1_name>Andreas Witt</chair1_name>
  <chair1_organisation>Universität zu Köln</chair1_organisation>
  <chair1_email>andreas.witt@uni-koeln.de</chair1_email>
  <chair1_ID>1936</chair1_ID>
  <chair2_name>Claudine Moulin</chair2_name>
  <chair2_organisation>Universität Trier</chair2_organisation>
  <chair2_email>moulin@uni-trier.de</chair2_email>
  <chair2_ID>1015</chair2_ID>
 </session>

 <session>
  <session_ID>90</session_ID>
  <session_short>Altstadtführung 1</session_short>
  <session_title>Gruppe 1</session_title>
  <session_start>2018-03-02 16:00</session_start>
  <session_end>2018-03-02 17:30</session_end>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>91</session_ID>
  <session_short>Altstadtführung 2</session_short>
  <session_title>Gruppe 2</session_title>
  <session_start>2018-03-02 16:00</session_start>
  <session_end>2018-03-02 17:30</session_end>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>101</session_ID>
  <session_short>Domführung 1</session_short>
  <session_title>Gruppe 1</session_title>
  <session_start>2018-03-02 16:30</session_start>
  <session_end>2018-03-02 17:30</session_end>
  <attendee_count>2</attendee_count>
 </session>

 <session>
  <session_ID>102</session_ID>
  <session_short>Domführung 2</session_short>
  <session_title>Gruppe 2</session_title>
  <session_start>2018-03-02 16:30</session_start>
  <session_end>2018-03-02 17:30</session_end>
  <attendee_count>1</attendee_count>
 </session>

 <session>
  <session_ID>103</session_ID>
  <session_short>Brauhaustour 1</session_short>
  <session_title>mit anschließendem Abendessen (Selbstzahlerbasis)</session_title>
  <session_start>2018-03-02 17:45</session_start>
  <session_end>2018-03-02 19:15</session_end>
  <attendee_count>0</attendee_count>
 </session>

 <session>
  <session_ID>131</session_ID>
  <session_short>Brauhaustour 2</session_short>
  <session_title>mit anschließendem Abendessen (Selbstzahlerbasis)</session_title>
  <session_start>2018-03-02 17:45</session_start>
  <session_end>2018-03-02 19:15</session_end>
  <attendee_count>0</attendee_count>
 </session>
</sessions>
</c4me>